{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi8HBJq9Dp_I",
        "outputId": "435dfbba-5a39-47ed-ded5-39213b7c5922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat May 20 16:02:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A95e3uiEDxY",
        "outputId": "1e7ed140-2108-4f75-b146-4350e49f73db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HXfr90rNEF80"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install keras_applications\n",
        "!pip install keras_vggface\n",
        "filename = \"/usr/local/lib/python3.10/dist-packages/keras_vggface/models.py\"\n",
        "text = open(filename).read()\n",
        "open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))\n",
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My3L3wTuEP8K",
        "outputId": "02722ed0-47e1-4bc0-e557-4b79babe1df9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from keras.utils import load_img, img_to_array\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FavQBL-3ESY3",
        "outputId": "01abfa39-a1be-4956-95ca-3aa5c9196588"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3edd36ce-7f41-4b64-bb7a-caa79ceafbe2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>bmi</th>\n",
              "      <th>gender</th>\n",
              "      <th>is_training</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>34.207396</td>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>img_0.bmp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>26.453720</td>\n",
              "      <td>Male</td>\n",
              "      <td>1</td>\n",
              "      <td>img_1.bmp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>34.967561</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>img_2.bmp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>22.044766</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>img_3.bmp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>37.758789</td>\n",
              "      <td>Female</td>\n",
              "      <td>1</td>\n",
              "      <td>img_4.bmp</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3edd36ce-7f41-4b64-bb7a-caa79ceafbe2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3edd36ce-7f41-4b64-bb7a-caa79ceafbe2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3edd36ce-7f41-4b64-bb7a-caa79ceafbe2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0        bmi  gender  is_training       name\n",
              "0           0  34.207396    Male            1  img_0.bmp\n",
              "1           1  26.453720    Male            1  img_1.bmp\n",
              "2           2  34.967561  Female            1  img_2.bmp\n",
              "3           3  22.044766  Female            1  img_3.bmp\n",
              "4           4  37.758789  Female            1  img_4.bmp"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/data.csv')\n",
        "data.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MTCNN to crop the images and scale them equally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhDO3fRLEUQp",
        "outputId": "0452b9d2-c7ba-4327-88f3-2083155766cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3660.bmp\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3680.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3681.bmp\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3694.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3695.bmp\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3726.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3727.bmp\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3731.bmp\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3764.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3765.bmp\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3790.bmp\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3792.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3793.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3794.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3795.bmp\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3802.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3803.bmp\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3812.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3813.bmp\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3816.bmp\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3826.bmp\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3840.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3841.bmp\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3851.bmp\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3854.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3855.bmp\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3878.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3879.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3880.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3881.bmp\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3900.bmp\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3906.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3907.bmp\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3921.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3922.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3923.bmp\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3931.bmp\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3958.bmp\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3966.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3967.bmp\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3970.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3971.bmp\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3973.bmp\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3990.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3991.bmp\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3994.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_3995.bmp\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4018.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4019.bmp\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4104.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4105.bmp\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4146.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4147.bmp\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4168.bmp\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4174.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4175.bmp\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4182.bmp\n",
            "Invalid image: /content/drive/MyDrive/BMI/Images/img_4183.bmp\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3/3 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from mtcnn import MTCNN\n",
        "from keras.layers.pooling import GlobalAveragePooling2D\n",
        "import pandas as pd\n",
        "from keras.utils.layer_utils import get_source_inputs\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Flatten, Conv2D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the CSV data\n",
        "data = pd.read_csv('/content/drive/MyDrive/data.csv')\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Set up the training data generator\n",
        "directory = '/content/drive/MyDrive/BMI/Images'\n",
        "\n",
        "# Create a new folder to save the cropped face images\n",
        "cropped_directory = '/content/drive/MyDrive/BMI_Cropped/CroppedImages'\n",
        "os.makedirs(cropped_directory, exist_ok=True)\n",
        "\n",
        "# Create an instance of MTCNN\n",
        "mtcnn = MTCNN()\n",
        "\n",
        "# Function to crop faces from images and save them to a new folder\n",
        "def save_cropped_faces(image_path, save_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            # resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            try:\n",
        "                padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "                padded_image[:face.shape[0], :face.shape[1]] = face\n",
        "            except:\n",
        "                height = face.shape[0]  # Get the height of the image\n",
        "                width = face.shape[1]   # Get the width of the image\n",
        "                scale_factor = min(224 / width, 224 / height)\n",
        "\n",
        "                # Resize the image while preserving aspect ratio\n",
        "                new_width = int(width * scale_factor)\n",
        "                new_height = int(height * scale_factor)\n",
        "                face = cv2.resize(face, (new_width, new_height))\n",
        "                padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "                padded_image[:face.shape[0], :face.shape[1]] = face\n",
        "            cv2.imwrite(save_path, padded_image)\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "\n",
        "\n",
        "# Iterate through the data and crop faces from images\n",
        "for index, row in data.iterrows():\n",
        "    image_path = os.path.join(directory, row['name'])\n",
        "    save_path = os.path.join(cropped_directory, row['name'])\n",
        "    save_cropped_faces(image_path, save_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trying out diferent models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meV4xyjjgVhf",
        "outputId": "73f27305-5d9f-4ffe-c9e0-d26bb99ac740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_88\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_89 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_349 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 3, 3, 32)         0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " dense_205 (Dense)           (None, 3, 3, 128)         4224      \n",
            "                                                                 \n",
            " dense_206 (Dense)           (None, 3, 3, 1)           129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,784,609\n",
            "Trainable params: 69,921\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 591.0190 - rmse: 24.2897\n",
            "Epoch 1: val_rmse improved from inf to 16.22020, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 9s 80ms/step - loss: 591.0190 - rmse: 24.2897 - val_loss: 263.1055 - val_rmse: 16.2202\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 178.1900 - rmse: 13.3503\n",
            "Epoch 2: val_rmse improved from 16.22020 to 12.60514, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 178.1900 - rmse: 13.3503 - val_loss: 158.9155 - val_rmse: 12.6051\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 136.4486 - rmse: 11.6861\n",
            "Epoch 3: val_rmse improved from 12.60514 to 11.49912, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 136.4486 - rmse: 11.6861 - val_loss: 132.1521 - val_rmse: 11.4991\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 119.7831 - rmse: 10.9480\n",
            "Epoch 4: val_rmse improved from 11.49912 to 10.72449, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 119.7831 - rmse: 10.9480 - val_loss: 114.9890 - val_rmse: 10.7245\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 108.5669 - rmse: 10.4182\n",
            "Epoch 5: val_rmse improved from 10.72449 to 10.12431, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 108.5669 - rmse: 10.4182 - val_loss: 102.4155 - val_rmse: 10.1243\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 99.7547 - rmse: 9.9874\n",
            "Epoch 6: val_rmse improved from 10.12431 to 9.59287, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 99.7547 - rmse: 9.9874 - val_loss: 92.0555 - val_rmse: 9.5929\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 92.0236 - rmse: 9.5970\n",
            "Epoch 7: val_rmse improved from 9.59287 to 9.18416, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 92.0236 - rmse: 9.5970 - val_loss: 84.5289 - val_rmse: 9.1842\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 84.9318 - rmse: 9.2174\n",
            "Epoch 8: val_rmse improved from 9.18416 to 8.72110, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 84.9318 - rmse: 9.2174 - val_loss: 75.9953 - val_rmse: 8.7211\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 80.2102 - rmse: 8.9520\n",
            "Epoch 9: val_rmse improved from 8.72110 to 8.55034, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 80.2102 - rmse: 8.9520 - val_loss: 73.1042 - val_rmse: 8.5503\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 75.5688 - rmse: 8.6892\n",
            "Epoch 10: val_rmse improved from 8.55034 to 8.09525, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 75.5688 - rmse: 8.6892 - val_loss: 65.6627 - val_rmse: 8.0952\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.7339 - rmse: 8.5203\n",
            "Epoch 11: val_rmse improved from 8.09525 to 7.95975, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 72.7339 - rmse: 8.5203 - val_loss: 63.3230 - val_rmse: 7.9597\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.1379 - rmse: 8.4245\n",
            "Epoch 12: val_rmse did not improve from 7.95975\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 71.1379 - rmse: 8.4245 - val_loss: 64.2874 - val_rmse: 8.0084\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.8127 - rmse: 8.4710\n",
            "Epoch 13: val_rmse improved from 7.95975 to 7.89555, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 71.8127 - rmse: 8.4710 - val_loss: 62.3261 - val_rmse: 7.8955\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.4432 - rmse: 8.3923\n",
            "Epoch 14: val_rmse did not improve from 7.89555\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 70.4432 - rmse: 8.3923 - val_loss: 63.4738 - val_rmse: 7.9685\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.1222 - rmse: 8.3692\n",
            "Epoch 15: val_rmse improved from 7.89555 to 7.84243, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 70.1222 - rmse: 8.3692 - val_loss: 61.4441 - val_rmse: 7.8424\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.5800 - rmse: 8.4598\n",
            "Epoch 16: val_rmse did not improve from 7.84243\n",
            "88/88 [==============================] - 6s 72ms/step - loss: 71.5800 - rmse: 8.4598 - val_loss: 61.6559 - val_rmse: 7.8489\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.6195 - rmse: 8.4598\n",
            "Epoch 17: val_rmse did not improve from 7.84243\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 71.6195 - rmse: 8.4598 - val_loss: 62.6650 - val_rmse: 7.9095\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.2485 - rmse: 8.3845\n",
            "Epoch 18: val_rmse did not improve from 7.84243\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 70.2485 - rmse: 8.3845 - val_loss: 62.0191 - val_rmse: 7.8723\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.3426 - rmse: 8.3854\n",
            "Epoch 19: val_rmse improved from 7.84243 to 7.80494, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 70.3426 - rmse: 8.3854 - val_loss: 61.0370 - val_rmse: 7.8049\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.6349 - rmse: 8.4091\n",
            "Epoch 20: val_rmse did not improve from 7.80494\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 70.6349 - rmse: 8.4091 - val_loss: 62.1552 - val_rmse: 7.8825\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.8374 - rmse: 8.3583\n",
            "Epoch 21: val_rmse improved from 7.80494 to 7.79997, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.8374 - rmse: 8.3583 - val_loss: 60.9282 - val_rmse: 7.8000\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.4628 - rmse: 8.3377\n",
            "Epoch 22: val_rmse did not improve from 7.79997\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.4628 - rmse: 8.3377 - val_loss: 61.0922 - val_rmse: 7.8129\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.3536 - rmse: 8.3847\n",
            "Epoch 23: val_rmse did not improve from 7.79997\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 70.3536 - rmse: 8.3847 - val_loss: 60.8549 - val_rmse: 7.8035\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2008 - rmse: 8.3206\n",
            "Epoch 24: val_rmse did not improve from 7.79997\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.2008 - rmse: 8.3206 - val_loss: 62.3824 - val_rmse: 7.8874\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2697 - rmse: 8.3215\n",
            "Epoch 25: val_rmse did not improve from 7.79997\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 69.2697 - rmse: 8.3215 - val_loss: 61.2733 - val_rmse: 7.8310\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.5810 - rmse: 8.3435\n",
            "Epoch 26: val_rmse improved from 7.79997 to 7.78649, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.5810 - rmse: 8.3435 - val_loss: 60.6217 - val_rmse: 7.7865\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2979 - rmse: 8.3244\n",
            "Epoch 27: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.2979 - rmse: 8.3244 - val_loss: 62.1172 - val_rmse: 7.8863\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.7249 - rmse: 8.3456\n",
            "Epoch 28: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.7249 - rmse: 8.3456 - val_loss: 63.1887 - val_rmse: 7.9494\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.7624 - rmse: 8.3414\n",
            "Epoch 29: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 6s 73ms/step - loss: 69.7624 - rmse: 8.3414 - val_loss: 64.2646 - val_rmse: 8.0156\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.6318 - rmse: 8.4096\n",
            "Epoch 30: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 70.6318 - rmse: 8.4096 - val_loss: 60.6475 - val_rmse: 7.7921\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.7414 - rmse: 8.4141\n",
            "Epoch 31: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 70.7414 - rmse: 8.4141 - val_loss: 60.8864 - val_rmse: 7.8019\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.8631 - rmse: 8.3577\n",
            "Epoch 32: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.8631 - rmse: 8.3577 - val_loss: 65.9785 - val_rmse: 8.1175\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.9115 - rmse: 8.3660\n",
            "Epoch 33: val_rmse did not improve from 7.78649\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.9115 - rmse: 8.3660 - val_loss: 61.2124 - val_rmse: 7.8235\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.4926 - rmse: 8.3365\n",
            "Epoch 34: val_rmse improved from 7.78649 to 7.78237, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.4926 - rmse: 8.3365 - val_loss: 60.5743 - val_rmse: 7.7824\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3330 - rmse: 8.3278\n",
            "Epoch 35: val_rmse did not improve from 7.78237\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 69.3330 - rmse: 8.3278 - val_loss: 66.2833 - val_rmse: 8.1364\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.5231 - rmse: 8.3404\n",
            "Epoch 36: val_rmse improved from 7.78237 to 7.78010, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.5231 - rmse: 8.3404 - val_loss: 60.4803 - val_rmse: 7.7801\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.8044 - rmse: 8.3584\n",
            "Epoch 37: val_rmse did not improve from 7.78010\n",
            "88/88 [==============================] - 6s 74ms/step - loss: 69.8044 - rmse: 8.3584 - val_loss: 60.5575 - val_rmse: 7.7823\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.6693 - rmse: 8.3448\n",
            "Epoch 38: val_rmse improved from 7.78010 to 7.77439, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.6693 - rmse: 8.3448 - val_loss: 60.5053 - val_rmse: 7.7744\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.4838 - rmse: 8.3408\n",
            "Epoch 39: val_rmse did not improve from 7.77439\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 69.4838 - rmse: 8.3408 - val_loss: 60.6865 - val_rmse: 7.7883\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.6098 - rmse: 8.3466\n",
            "Epoch 40: val_rmse did not improve from 7.77439\n",
            "88/88 [==============================] - 6s 73ms/step - loss: 69.6098 - rmse: 8.3466 - val_loss: 61.5438 - val_rmse: 7.8443\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.6212 - rmse: 8.3309\n",
            "Epoch 41: val_rmse did not improve from 7.77439\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.6212 - rmse: 8.3309 - val_loss: 62.5376 - val_rmse: 7.9070\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1527 - rmse: 8.3181\n",
            "Epoch 42: val_rmse did not improve from 7.77439\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 69.1527 - rmse: 8.3181 - val_loss: 60.4418 - val_rmse: 7.7779\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.9460 - rmse: 8.3663\n",
            "Epoch 43: val_rmse improved from 7.77439 to 7.77065, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.9460 - rmse: 8.3663 - val_loss: 60.5128 - val_rmse: 7.7706\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.8243 - rmse: 8.3511\n",
            "Epoch 44: val_rmse did not improve from 7.77065\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.8243 - rmse: 8.3511 - val_loss: 60.6111 - val_rmse: 7.7915\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1462 - rmse: 8.3182\n",
            "Epoch 45: val_rmse improved from 7.77065 to 7.75940, saving model to best_bmi_model_v2.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.1462 - rmse: 8.3182 - val_loss: 60.3827 - val_rmse: 7.7594\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3413 - rmse: 8.3303\n",
            "Epoch 46: val_rmse did not improve from 7.75940\n",
            "88/88 [==============================] - 6s 73ms/step - loss: 69.3413 - rmse: 8.3303 - val_loss: 60.5979 - val_rmse: 7.7903\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3175 - rmse: 8.3219\n",
            "Epoch 47: val_rmse did not improve from 7.75940\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.3175 - rmse: 8.3219 - val_loss: 60.8926 - val_rmse: 7.8101\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2573 - rmse: 8.3265\n",
            "Epoch 48: val_rmse did not improve from 7.75940\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 69.2573 - rmse: 8.3265 - val_loss: 60.8594 - val_rmse: 7.8057\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.1813 - rmse: 8.3744\n",
            "Epoch 49: val_rmse did not improve from 7.75940\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 70.1813 - rmse: 8.3744 - val_loss: 61.7415 - val_rmse: 7.8636\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.6080 - rmse: 8.4062\n",
            "Epoch 50: val_rmse did not improve from 7.75940\n",
            "88/88 [==============================] - 6s 73ms/step - loss: 70.6080 - rmse: 8.4062 - val_loss: 60.7033 - val_rmse: 7.7891\n",
            "88/88 [==============================] - 5s 59ms/step\n"
          ]
        }
      ],
      "source": [
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "\n",
        "# Load the VGGFace model without the top layer\n",
        "base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.layers[-1].output\n",
        "x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "x = keras.layers.AveragePooling2D()(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='relu')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "# Set up callbacks to save the best model and enable early stopping\n",
        "checkpoint_bmi = ModelCheckpoint('best_bmi_model_v2.h5', monitor='val_rmse', save_best_only=True, verbose=1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_bmi], validation_data=validation_set\n",
        ")\n",
        "\n",
        "train_predictions = model.predict(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD2X8auFdqWv",
        "outputId": "7af0570d-af76-4968-8c74-0a41ffa0205d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI pi: [[32.671383]\n",
            " [32.61521 ]\n",
            " [32.95267 ]]\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted BMI pi: [[31.611036]\n",
            " [31.86374 ]\n",
            " [33.43963 ]]\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: [[33.532097]\n",
            " [31.198223]\n",
            " [31.872921]]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: [[32.53389 ]\n",
            " [31.567865]\n",
            " [32.25808 ]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI: [[32.521862]\n",
            " [32.72001 ]\n",
            " [32.67032 ]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI: [[32.345627]\n",
            " [32.72348 ]\n",
            " [32.94284 ]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted BMI: [[31.94447 ]\n",
            " [31.384174]\n",
            " [32.62609 ]]\n"
          ]
        }
      ],
      "source": [
        "def cropped_faces(image_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return None\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            padded_image[:resized_face.shape[0], :resized_face.shape[1]] = resized_face\n",
        "\n",
        "            # Save the padded image\n",
        "            cropped_image_path = '/content/drive/MyDrive/Cropped_Image.jpg'\n",
        "            cv2.imwrite(cropped_image_path, padded_image)\n",
        "            \n",
        "            return cropped_image_path\n",
        "        else:\n",
        "            print(\"No face detected in the image:\", image_path)\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PassportPhoto.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z18K9NSejsH4",
        "outputId": "77503d6b-329d-42e3-fdb6-c9f5f5061dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_89\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_90 (InputLayer)       [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_350 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 3, 3, 32)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dense_207 (Dense)           (None, 3, 3, 128)         4224      \n",
            "                                                                 \n",
            " dense_208 (Dense)           (None, 3, 3, 1)           129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,784,609\n",
            "Trainable params: 7,149,345\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 318.3992 - rmse: 17.8597\n",
            "Epoch 1: val_rmse improved from inf to 12.84056, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 10s 80ms/step - loss: 318.3992 - rmse: 17.8597 - val_loss: 164.8079 - val_rmse: 12.8406\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 121.1839 - rmse: 11.0148\n",
            "Epoch 2: val_rmse improved from 12.84056 to 10.76012, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 121.1839 - rmse: 11.0148 - val_loss: 115.9329 - val_rmse: 10.7601\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 102.0371 - rmse: 10.1080\n",
            "Epoch 3: val_rmse improved from 10.76012 to 9.17975, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 102.0371 - rmse: 10.1080 - val_loss: 84.1573 - val_rmse: 9.1798\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 97.6920 - rmse: 9.8905\n",
            "Epoch 4: val_rmse did not improve from 9.17975\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 97.6920 - rmse: 9.8905 - val_loss: 102.7140 - val_rmse: 10.1336\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 95.8821 - rmse: 9.7857\n",
            "Epoch 5: val_rmse did not improve from 9.17975\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 95.8821 - rmse: 9.7857 - val_loss: 88.5199 - val_rmse: 9.4107\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 95.1403 - rmse: 9.7551\n",
            "Epoch 6: val_rmse did not improve from 9.17975\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 95.1403 - rmse: 9.7551 - val_loss: 106.6591 - val_rmse: 10.3262\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 89.4304 - rmse: 9.4614\n",
            "Epoch 7: val_rmse improved from 9.17975 to 8.88762, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 89.4304 - rmse: 9.4614 - val_loss: 78.9341 - val_rmse: 8.8876\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 84.8873 - rmse: 9.2132\n",
            "Epoch 8: val_rmse improved from 8.88762 to 8.73813, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 84.8873 - rmse: 9.2132 - val_loss: 76.5437 - val_rmse: 8.7381\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 85.9406 - rmse: 9.2751\n",
            "Epoch 9: val_rmse did not improve from 8.73813\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 85.9406 - rmse: 9.2751 - val_loss: 99.8711 - val_rmse: 9.9936\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 82.2100 - rmse: 9.0675\n",
            "Epoch 10: val_rmse did not improve from 8.73813\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 82.2100 - rmse: 9.0675 - val_loss: 82.4167 - val_rmse: 9.0861\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 82.9833 - rmse: 9.1085\n",
            "Epoch 11: val_rmse did not improve from 8.73813\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 82.9833 - rmse: 9.1085 - val_loss: 81.9938 - val_rmse: 9.0567\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 80.4306 - rmse: 8.9683\n",
            "Epoch 12: val_rmse improved from 8.73813 to 8.72742, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 80.4306 - rmse: 8.9683 - val_loss: 76.1068 - val_rmse: 8.7274\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 78.3324 - rmse: 8.8482\n",
            "Epoch 13: val_rmse improved from 8.72742 to 8.69507, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 78.3324 - rmse: 8.8482 - val_loss: 75.5421 - val_rmse: 8.6951\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 79.3048 - rmse: 8.9064\n",
            "Epoch 14: val_rmse did not improve from 8.69507\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 79.3048 - rmse: 8.9064 - val_loss: 77.8253 - val_rmse: 8.8244\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 78.1259 - rmse: 8.8350\n",
            "Epoch 15: val_rmse improved from 8.69507 to 8.52140, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 78.1259 - rmse: 8.8350 - val_loss: 72.7458 - val_rmse: 8.5214\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 79.7735 - rmse: 8.9341\n",
            "Epoch 16: val_rmse did not improve from 8.52140\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 79.7735 - rmse: 8.9341 - val_loss: 87.9883 - val_rmse: 9.3819\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 80.5514 - rmse: 8.9797\n",
            "Epoch 17: val_rmse improved from 8.52140 to 8.42457, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 80.5514 - rmse: 8.9797 - val_loss: 70.9513 - val_rmse: 8.4246\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 78.0763 - rmse: 8.8360\n",
            "Epoch 18: val_rmse did not improve from 8.42457\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 78.0763 - rmse: 8.8360 - val_loss: 75.6924 - val_rmse: 8.7045\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 75.3860 - rmse: 8.6864\n",
            "Epoch 19: val_rmse did not improve from 8.42457\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 75.3860 - rmse: 8.6864 - val_loss: 71.4531 - val_rmse: 8.4567\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.3918 - rmse: 8.5671\n",
            "Epoch 20: val_rmse did not improve from 8.42457\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 73.3918 - rmse: 8.5671 - val_loss: 75.7940 - val_rmse: 8.7013\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.7458 - rmse: 8.5923\n",
            "Epoch 21: val_rmse did not improve from 8.42457\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 73.7458 - rmse: 8.5923 - val_loss: 77.9371 - val_rmse: 8.8347\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 74.5614 - rmse: 8.6397\n",
            "Epoch 22: val_rmse did not improve from 8.42457\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 74.5614 - rmse: 8.6397 - val_loss: 72.8051 - val_rmse: 8.5328\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.4835 - rmse: 8.5789\n",
            "Epoch 23: val_rmse improved from 8.42457 to 8.38096, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 73.4835 - rmse: 8.5789 - val_loss: 70.2515 - val_rmse: 8.3810\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.3078 - rmse: 8.5085\n",
            "Epoch 24: val_rmse improved from 8.38096 to 8.29591, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 72.3078 - rmse: 8.5085 - val_loss: 68.7497 - val_rmse: 8.2959\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.9152 - rmse: 8.5434\n",
            "Epoch 25: val_rmse improved from 8.29591 to 8.25781, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 72.9152 - rmse: 8.5434 - val_loss: 68.1466 - val_rmse: 8.2578\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.7409 - rmse: 8.5286\n",
            "Epoch 26: val_rmse did not improve from 8.25781\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 72.7409 - rmse: 8.5286 - val_loss: 73.3682 - val_rmse: 8.5733\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 74.6528 - rmse: 8.6405\n",
            "Epoch 27: val_rmse did not improve from 8.25781\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 74.6528 - rmse: 8.6405 - val_loss: 79.1073 - val_rmse: 8.8992\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.3076 - rmse: 8.5610\n",
            "Epoch 28: val_rmse did not improve from 8.25781\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 73.3076 - rmse: 8.5610 - val_loss: 78.7528 - val_rmse: 8.8740\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.4367 - rmse: 8.5070\n",
            "Epoch 29: val_rmse did not improve from 8.25781\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 72.4367 - rmse: 8.5070 - val_loss: 68.9671 - val_rmse: 8.3091\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.4038 - rmse: 8.4570\n",
            "Epoch 30: val_rmse improved from 8.25781 to 8.12635, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 71.4038 - rmse: 8.4570 - val_loss: 66.1941 - val_rmse: 8.1264\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 75.0174 - rmse: 8.6638\n",
            "Epoch 31: val_rmse did not improve from 8.12635\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 75.0174 - rmse: 8.6638 - val_loss: 66.1374 - val_rmse: 8.1385\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.5077 - rmse: 8.4616\n",
            "Epoch 32: val_rmse improved from 8.12635 to 8.10906, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 71.5077 - rmse: 8.4616 - val_loss: 65.7790 - val_rmse: 8.1091\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.5056 - rmse: 8.5684\n",
            "Epoch 33: val_rmse did not improve from 8.10906\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 73.5056 - rmse: 8.5684 - val_loss: 69.6206 - val_rmse: 8.3489\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.2866 - rmse: 8.4392\n",
            "Epoch 34: val_rmse improved from 8.10906 to 8.07717, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 71.2866 - rmse: 8.4392 - val_loss: 65.2939 - val_rmse: 8.0772\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.9175 - rmse: 8.4865\n",
            "Epoch 35: val_rmse did not improve from 8.07717\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 71.9175 - rmse: 8.4865 - val_loss: 68.7092 - val_rmse: 8.2871\n",
            "Epoch 36/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 70.9349 - rmse: 8.4223\n",
            "Epoch 36: val_rmse did not improve from 8.07717\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 70.8528 - rmse: 8.4191 - val_loss: 72.7436 - val_rmse: 8.5279\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.8828 - rmse: 8.5408\n",
            "Epoch 37: val_rmse improved from 8.07717 to 7.90380, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 72.8828 - rmse: 8.5408 - val_loss: 62.5300 - val_rmse: 7.9038\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.3151 - rmse: 8.4446\n",
            "Epoch 38: val_rmse did not improve from 7.90380\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 71.3151 - rmse: 8.4446 - val_loss: 63.4905 - val_rmse: 7.9738\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.7227 - rmse: 8.4726\n",
            "Epoch 39: val_rmse did not improve from 7.90380\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 71.7227 - rmse: 8.4726 - val_loss: 68.9258 - val_rmse: 8.3111\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.4451 - rmse: 8.5703\n",
            "Epoch 40: val_rmse improved from 7.90380 to 7.77562, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 73.4451 - rmse: 8.5703 - val_loss: 60.4895 - val_rmse: 7.7756\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.8553 - rmse: 8.3598\n",
            "Epoch 41: val_rmse did not improve from 7.77562\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.8553 - rmse: 8.3598 - val_loss: 64.8298 - val_rmse: 8.0585\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.2912 - rmse: 8.3795\n",
            "Epoch 42: val_rmse did not improve from 7.77562\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 70.2912 - rmse: 8.3795 - val_loss: 61.5094 - val_rmse: 7.8448\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.1233 - rmse: 8.4327\n",
            "Epoch 43: val_rmse did not improve from 7.77562\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 71.1233 - rmse: 8.4327 - val_loss: 64.2137 - val_rmse: 8.0081\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.5629 - rmse: 8.3988\n",
            "Epoch 44: val_rmse improved from 7.77562 to 7.76112, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 70.5629 - rmse: 8.3988 - val_loss: 60.2430 - val_rmse: 7.7611\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.5264 - rmse: 8.3952\n",
            "Epoch 45: val_rmse did not improve from 7.76112\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 70.5264 - rmse: 8.3952 - val_loss: 60.7604 - val_rmse: 7.7993\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.3156 - rmse: 8.4497\n",
            "Epoch 46: val_rmse did not improve from 7.76112\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 71.3156 - rmse: 8.4497 - val_loss: 60.2602 - val_rmse: 7.7623\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.5083 - rmse: 8.3918\n",
            "Epoch 47: val_rmse did not improve from 7.76112\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 70.5083 - rmse: 8.3918 - val_loss: 61.8278 - val_rmse: 7.8621\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.5790 - rmse: 8.3925\n",
            "Epoch 48: val_rmse did not improve from 7.76112\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 70.5790 - rmse: 8.3925 - val_loss: 62.6701 - val_rmse: 7.9192\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 71.1023 - rmse: 8.4170\n",
            "Epoch 49: val_rmse improved from 7.76112 to 7.75371, saving model to best_bmi_model_v2_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 71.1023 - rmse: 8.4170 - val_loss: 60.3189 - val_rmse: 7.7537\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.1864 - rmse: 8.4938\n",
            "Epoch 50: val_rmse did not improve from 7.75371\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 72.1864 - rmse: 8.4938 - val_loss: 60.1961 - val_rmse: 7.7573\n",
            "88/88 [==============================] - 5s 58ms/step\n"
          ]
        }
      ],
      "source": [
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "\n",
        "# Load the VGGFace model without the top layer\n",
        "base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.layers[-1].output\n",
        "x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "x = keras.layers.AveragePooling2D()(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='relu')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "# Set up callbacks to save the best model and enable early stopping\n",
        "checkpoint_bmi = ModelCheckpoint('best_bmi_model_v2_1.h5', monitor='val_rmse', save_best_only=True, verbose=1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_bmi], validation_data=validation_set\n",
        ")\n",
        "\n",
        "train_predictions = model.predict(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxsKAYiAkQkG",
        "outputId": "4a3f6ca9-c5cc-4624-903f-01a6f3e4c86e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "Predicted BMI pi: [[32.553722]\n",
            " [32.78946 ]\n",
            " [32.862858]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI pi: [[32.68963 ]\n",
            " [32.761345]\n",
            " [32.71654 ]]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI pi: [[32.53496]\n",
            " [32.45109]\n",
            " [32.72851]]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: [[32.522846]\n",
            " [32.50012 ]\n",
            " [32.68018 ]]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI: [[32.495083]\n",
            " [32.49697 ]\n",
            " [32.81796 ]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI: [[32.647182]\n",
            " [32.65306 ]\n",
            " [33.015553]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI: [[32.697502]\n",
            " [32.691547]\n",
            " [32.89199 ]]\n"
          ]
        }
      ],
      "source": [
        "def cropped_faces(image_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return None\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            padded_image[:resized_face.shape[0], :resized_face.shape[1]] = resized_face\n",
        "\n",
        "            # Save the padded image\n",
        "            cropped_image_path = '/content/drive/MyDrive/Cropped_Image.jpg'\n",
        "            cv2.imwrite(cropped_image_path, padded_image)\n",
        "            \n",
        "            return cropped_image_path\n",
        "        else:\n",
        "            print(\"No face detected in the image:\", image_path)\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PassportPhoto.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga7R0nvwkkhI",
        "outputId": "ada47f45-3a78-4409-b168-caab2e6f9d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_90\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_94 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1/7x7_s2 (Conv2D)          (None, 112, 112, 64  9408        ['input_94[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1/7x7_s2/bn (BatchNormaliz  (None, 112, 112, 64  256        ['conv1/7x7_s2[0][0]']           \n",
            " ation)                         )                                                                 \n",
            "                                                                                                  \n",
            " activation_147 (Activation)    (None, 112, 112, 64  0           ['conv1/7x7_s2/bn[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_177 (MaxPooling2  (None, 55, 55, 64)  0           ['activation_147[0][0]']         \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_1_1x1_reduce (Conv2D)    (None, 55, 55, 64)   4096        ['max_pooling2d_177[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_1_1x1_reduce/bn (BatchNo  (None, 55, 55, 64)  256         ['conv2_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_148 (Activation)    (None, 55, 55, 64)   0           ['conv2_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv2_1_3x3 (Conv2D)           (None, 55, 55, 64)   36864       ['activation_148[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_1_3x3/bn (BatchNormaliza  (None, 55, 55, 64)  256         ['conv2_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_149 (Activation)    (None, 55, 55, 64)   0           ['conv2_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_1_1x1_increase (Conv2D)  (None, 55, 55, 256)  16384       ['activation_149[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_1_1x1_proj (Conv2D)      (None, 55, 55, 256)  16384       ['max_pooling2d_177[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_1_1x1_increase/bn (Batch  (None, 55, 55, 256)  1024       ['conv2_1_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2_1_1x1_proj/bn (BatchNorm  (None, 55, 55, 256)  1024       ['conv2_1_1x1_proj[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_48 (Add)                   (None, 55, 55, 256)  0           ['conv2_1_1x1_increase/bn[0][0]',\n",
            "                                                                  'conv2_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_150 (Activation)    (None, 55, 55, 256)  0           ['add_48[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2_2_1x1_reduce (Conv2D)    (None, 55, 55, 64)   16384       ['activation_150[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_1x1_reduce/bn (BatchNo  (None, 55, 55, 64)  256         ['conv2_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_151 (Activation)    (None, 55, 55, 64)   0           ['conv2_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv2_2_3x3 (Conv2D)           (None, 55, 55, 64)   36864       ['activation_151[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_3x3/bn (BatchNormaliza  (None, 55, 55, 64)  256         ['conv2_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_152 (Activation)    (None, 55, 55, 64)   0           ['conv2_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_1x1_increase (Conv2D)  (None, 55, 55, 256)  16384       ['activation_152[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_1x1_increase/bn (Batch  (None, 55, 55, 256)  1024       ['conv2_2_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_49 (Add)                   (None, 55, 55, 256)  0           ['conv2_2_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_150[0][0]']         \n",
            "                                                                                                  \n",
            " activation_153 (Activation)    (None, 55, 55, 256)  0           ['add_49[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2_3_1x1_reduce (Conv2D)    (None, 55, 55, 64)   16384       ['activation_153[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_1x1_reduce/bn (BatchNo  (None, 55, 55, 64)  256         ['conv2_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_154 (Activation)    (None, 55, 55, 64)   0           ['conv2_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv2_3_3x3 (Conv2D)           (None, 55, 55, 64)   36864       ['activation_154[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_3x3/bn (BatchNormaliza  (None, 55, 55, 64)  256         ['conv2_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_155 (Activation)    (None, 55, 55, 64)   0           ['conv2_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_1x1_increase (Conv2D)  (None, 55, 55, 256)  16384       ['activation_155[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_1x1_increase/bn (Batch  (None, 55, 55, 256)  1024       ['conv2_3_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_50 (Add)                   (None, 55, 55, 256)  0           ['conv2_3_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_153[0][0]']         \n",
            "                                                                                                  \n",
            " activation_156 (Activation)    (None, 55, 55, 256)  0           ['add_50[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_1_1x1_reduce (Conv2D)    (None, 28, 28, 128)  32768       ['activation_156[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_157 (Activation)    (None, 28, 28, 128)  0           ['conv3_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_1_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_157[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_158 (Activation)    (None, 28, 28, 128)  0           ['conv3_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_158[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_proj (Conv2D)      (None, 28, 28, 512)  131072      ['activation_156[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_1_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv3_1_1x1_proj/bn (BatchNorm  (None, 28, 28, 512)  2048       ['conv3_1_1x1_proj[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 28, 28, 512)  0           ['conv3_1_1x1_increase/bn[0][0]',\n",
            "                                                                  'conv3_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_159 (Activation)    (None, 28, 28, 512)  0           ['add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_2_1x1_reduce (Conv2D)    (None, 28, 28, 128)  65536       ['activation_159[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_160 (Activation)    (None, 28, 28, 128)  0           ['conv3_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_2_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_160[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_161 (Activation)    (None, 28, 28, 128)  0           ['conv3_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_161[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_2_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_52 (Add)                   (None, 28, 28, 512)  0           ['conv3_2_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_159[0][0]']         \n",
            "                                                                                                  \n",
            " activation_162 (Activation)    (None, 28, 28, 512)  0           ['add_52[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_3_1x1_reduce (Conv2D)    (None, 28, 28, 128)  65536       ['activation_162[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_163 (Activation)    (None, 28, 28, 128)  0           ['conv3_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_3_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_163[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_164 (Activation)    (None, 28, 28, 128)  0           ['conv3_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_164[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_3_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_53 (Add)                   (None, 28, 28, 512)  0           ['conv3_3_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_162[0][0]']         \n",
            "                                                                                                  \n",
            " activation_165 (Activation)    (None, 28, 28, 512)  0           ['add_53[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_4_1x1_reduce (Conv2D)    (None, 28, 28, 128)  65536       ['activation_165[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_4_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_166 (Activation)    (None, 28, 28, 128)  0           ['conv3_4_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_4_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_166[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_4_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_167 (Activation)    (None, 28, 28, 128)  0           ['conv3_4_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_167[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_4_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_54 (Add)                   (None, 28, 28, 512)  0           ['conv3_4_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_165[0][0]']         \n",
            "                                                                                                  \n",
            " activation_168 (Activation)    (None, 28, 28, 512)  0           ['add_54[0][0]']                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_reduce (Conv2D)    (None, 14, 14, 256)  131072      ['activation_168[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_1_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_169 (Activation)    (None, 14, 14, 256)  0           ['conv4_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_1_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_169[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_1_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_170 (Activation)    (None, 14, 14, 256)  0           ['conv4_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_1_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_170[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_proj (Conv2D)      (None, 14, 14, 1024  524288      ['activation_168[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_1_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_proj/bn (BatchNorm  (None, 14, 14, 1024  4096       ['conv4_1_1x1_proj[0][0]']       \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " add_55 (Add)                   (None, 14, 14, 1024  0           ['conv4_1_1x1_increase/bn[0][0]',\n",
            "                                )                                 'conv4_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_171 (Activation)    (None, 14, 14, 1024  0           ['add_55[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_2_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_171[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_2_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_172 (Activation)    (None, 14, 14, 256)  0           ['conv4_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_2_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_172[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_2_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_173 (Activation)    (None, 14, 14, 256)  0           ['conv4_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_2_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_173[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_2_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_2_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_56 (Add)                   (None, 14, 14, 1024  0           ['conv4_2_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_171[0][0]']         \n",
            "                                                                                                  \n",
            " activation_174 (Activation)    (None, 14, 14, 1024  0           ['add_56[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_3_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_174[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_3_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_175 (Activation)    (None, 14, 14, 256)  0           ['conv4_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_3_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_175[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_3_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_176 (Activation)    (None, 14, 14, 256)  0           ['conv4_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_3_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_176[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_3_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_3_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_57 (Add)                   (None, 14, 14, 1024  0           ['conv4_3_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_174[0][0]']         \n",
            "                                                                                                  \n",
            " activation_177 (Activation)    (None, 14, 14, 1024  0           ['add_57[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_4_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_177[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_4_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_4_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_178 (Activation)    (None, 14, 14, 256)  0           ['conv4_4_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_4_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_178[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_4_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_4_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_179 (Activation)    (None, 14, 14, 256)  0           ['conv4_4_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_4_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_179[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_4_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_4_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_58 (Add)                   (None, 14, 14, 1024  0           ['conv4_4_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_177[0][0]']         \n",
            "                                                                                                  \n",
            " activation_180 (Activation)    (None, 14, 14, 1024  0           ['add_58[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_5_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_180[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_5_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_5_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_181 (Activation)    (None, 14, 14, 256)  0           ['conv4_5_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_5_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_181[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_5_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_5_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_182 (Activation)    (None, 14, 14, 256)  0           ['conv4_5_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_5_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_182[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_5_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_5_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_59 (Add)                   (None, 14, 14, 1024  0           ['conv4_5_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_180[0][0]']         \n",
            "                                                                                                  \n",
            " activation_183 (Activation)    (None, 14, 14, 1024  0           ['add_59[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_6_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_183[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_6_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_6_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_184 (Activation)    (None, 14, 14, 256)  0           ['conv4_6_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_6_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_184[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_6_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_6_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_185 (Activation)    (None, 14, 14, 256)  0           ['conv4_6_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_6_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_185[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_6_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_6_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_60 (Add)                   (None, 14, 14, 1024  0           ['conv4_6_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_183[0][0]']         \n",
            "                                                                                                  \n",
            " activation_186 (Activation)    (None, 14, 14, 1024  0           ['add_60[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_1_1x1_reduce (Conv2D)    (None, 7, 7, 512)    524288      ['activation_186[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_reduce/bn (BatchNo  (None, 7, 7, 512)   2048        ['conv5_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_187 (Activation)    (None, 7, 7, 512)    0           ['conv5_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv5_1_3x3 (Conv2D)           (None, 7, 7, 512)    2359296     ['activation_187[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_3x3/bn (BatchNormaliza  (None, 7, 7, 512)   2048        ['conv5_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_188 (Activation)    (None, 7, 7, 512)    0           ['conv5_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_increase (Conv2D)  (None, 7, 7, 2048)   1048576     ['activation_188[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_proj (Conv2D)      (None, 7, 7, 2048)   2097152     ['activation_186[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_increase/bn (Batch  (None, 7, 7, 2048)  8192        ['conv5_1_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv5_1_1x1_proj/bn (BatchNorm  (None, 7, 7, 2048)  8192        ['conv5_1_1x1_proj[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_61 (Add)                   (None, 7, 7, 2048)   0           ['conv5_1_1x1_increase/bn[0][0]',\n",
            "                                                                  'conv5_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_189 (Activation)    (None, 7, 7, 2048)   0           ['add_61[0][0]']                 \n",
            "                                                                                                  \n",
            " conv5_2_1x1_reduce (Conv2D)    (None, 7, 7, 512)    1048576     ['activation_189[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_1x1_reduce/bn (BatchNo  (None, 7, 7, 512)   2048        ['conv5_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_190 (Activation)    (None, 7, 7, 512)    0           ['conv5_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv5_2_3x3 (Conv2D)           (None, 7, 7, 512)    2359296     ['activation_190[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_3x3/bn (BatchNormaliza  (None, 7, 7, 512)   2048        ['conv5_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_191 (Activation)    (None, 7, 7, 512)    0           ['conv5_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_1x1_increase (Conv2D)  (None, 7, 7, 2048)   1048576     ['activation_191[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_1x1_increase/bn (Batch  (None, 7, 7, 2048)  8192        ['conv5_2_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_62 (Add)                   (None, 7, 7, 2048)   0           ['conv5_2_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_189[0][0]']         \n",
            "                                                                                                  \n",
            " activation_192 (Activation)    (None, 7, 7, 2048)   0           ['add_62[0][0]']                 \n",
            "                                                                                                  \n",
            " conv5_3_1x1_reduce (Conv2D)    (None, 7, 7, 512)    1048576     ['activation_192[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_1x1_reduce/bn (BatchNo  (None, 7, 7, 512)   2048        ['conv5_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_193 (Activation)    (None, 7, 7, 512)    0           ['conv5_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv5_3_3x3 (Conv2D)           (None, 7, 7, 512)    2359296     ['activation_193[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_3x3/bn (BatchNormaliza  (None, 7, 7, 512)   2048        ['conv5_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_194 (Activation)    (None, 7, 7, 512)    0           ['conv5_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_1x1_increase (Conv2D)  (None, 7, 7, 2048)   1048576     ['activation_194[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_1x1_increase/bn (Batch  (None, 7, 7, 2048)  8192        ['conv5_3_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_63 (Add)                   (None, 7, 7, 2048)   0           ['conv5_3_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_192[0][0]']         \n",
            "                                                                                                  \n",
            " activation_195 (Activation)    (None, 7, 7, 2048)   0           ['add_63[0][0]']                 \n",
            "                                                                                                  \n",
            " avg_pool (AveragePooling2D)    (None, 1, 1, 2048)   0           ['activation_195[0][0]']         \n",
            "                                                                                                  \n",
            " dense_209 (Dense)              (None, 1, 1, 128)    262272      ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            " dense_210 (Dense)              (None, 1, 1, 1)      129         ['dense_209[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,823,553\n",
            "Trainable params: 262,401\n",
            "Non-trainable params: 23,561,152\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 259.0680 - rmse: 16.1063\n",
            "Epoch 1: val_rmse improved from inf to 7.76904, saving model to best_bmi_model_v2_2.h5\n",
            "88/88 [==============================] - 13s 101ms/step - loss: 259.0680 - rmse: 16.1063 - val_loss: 60.3434 - val_rmse: 7.7690\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8197 - rmse: 8.2915\n",
            "Epoch 2: val_rmse did not improve from 7.76904\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.8197 - rmse: 8.2915 - val_loss: 60.6099 - val_rmse: 7.7904\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.6677 - rmse: 8.2860\n",
            "Epoch 3: val_rmse improved from 7.76904 to 7.74239, saving model to best_bmi_model_v2_2.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 68.6677 - rmse: 8.2860 - val_loss: 60.1955 - val_rmse: 7.7424\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8211 - rmse: 8.2936\n",
            "Epoch 4: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 68.8211 - rmse: 8.2936 - val_loss: 60.2132 - val_rmse: 7.7534\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.6500 - rmse: 8.2843\n",
            "Epoch 5: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.6500 - rmse: 8.2843 - val_loss: 60.2064 - val_rmse: 7.7609\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8782 - rmse: 8.2993\n",
            "Epoch 6: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 68.8782 - rmse: 8.2993 - val_loss: 60.1681 - val_rmse: 7.7619\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7106 - rmse: 8.2953\n",
            "Epoch 7: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.7106 - rmse: 8.2953 - val_loss: 60.1894 - val_rmse: 7.7640\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8991 - rmse: 8.2994\n",
            "Epoch 8: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.8991 - rmse: 8.2994 - val_loss: 60.3690 - val_rmse: 7.7703\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8217 - rmse: 8.2832\n",
            "Epoch 9: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.8217 - rmse: 8.2832 - val_loss: 60.2920 - val_rmse: 7.7639\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8356 - rmse: 8.3020\n",
            "Epoch 10: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.8356 - rmse: 8.3020 - val_loss: 60.3479 - val_rmse: 7.7696\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9614 - rmse: 8.3052\n",
            "Epoch 11: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.9614 - rmse: 8.3052 - val_loss: 60.9449 - val_rmse: 7.8086\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1578 - rmse: 8.3190\n",
            "Epoch 12: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.1578 - rmse: 8.3190 - val_loss: 60.5250 - val_rmse: 7.7847\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9744 - rmse: 8.2976\n",
            "Epoch 13: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.9744 - rmse: 8.2976 - val_loss: 60.2653 - val_rmse: 7.7693\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7385 - rmse: 8.2944\n",
            "Epoch 14: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.7385 - rmse: 8.2944 - val_loss: 60.1804 - val_rmse: 7.7553\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8800 - rmse: 8.2982\n",
            "Epoch 15: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.8800 - rmse: 8.2982 - val_loss: 60.9956 - val_rmse: 7.8061\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.6831 - rmse: 8.2913\n",
            "Epoch 16: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.6831 - rmse: 8.2913 - val_loss: 61.3664 - val_rmse: 7.8388\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2943 - rmse: 8.3257\n",
            "Epoch 17: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.2943 - rmse: 8.3257 - val_loss: 60.1864 - val_rmse: 7.7524\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8434 - rmse: 8.2942\n",
            "Epoch 18: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.8434 - rmse: 8.2942 - val_loss: 61.0097 - val_rmse: 7.8043\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7898 - rmse: 8.2981\n",
            "Epoch 19: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.7898 - rmse: 8.2981 - val_loss: 60.4121 - val_rmse: 7.7680\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.5985 - rmse: 8.2848\n",
            "Epoch 20: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 68.5985 - rmse: 8.2848 - val_loss: 62.8275 - val_rmse: 7.9257\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0181 - rmse: 8.3132\n",
            "Epoch 21: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.0181 - rmse: 8.3132 - val_loss: 60.2366 - val_rmse: 7.7645\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2213 - rmse: 8.3229\n",
            "Epoch 22: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.2213 - rmse: 8.3229 - val_loss: 63.6580 - val_rmse: 7.9770\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9391 - rmse: 8.3063\n",
            "Epoch 23: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.9391 - rmse: 8.3063 - val_loss: 61.8756 - val_rmse: 7.8706\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3661 - rmse: 8.3266\n",
            "Epoch 24: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.3661 - rmse: 8.3266 - val_loss: 60.1754 - val_rmse: 7.7602\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8473 - rmse: 8.3036\n",
            "Epoch 25: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.8473 - rmse: 8.3036 - val_loss: 60.5226 - val_rmse: 7.7835\n",
            "Epoch 26/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 68.4961 - rmse: 8.2762\n",
            "Epoch 26: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.6571 - rmse: 8.2826 - val_loss: 61.2622 - val_rmse: 7.8286\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0173 - rmse: 8.3104\n",
            "Epoch 27: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 84ms/step - loss: 69.0173 - rmse: 8.3104 - val_loss: 60.1896 - val_rmse: 7.7600\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8993 - rmse: 8.3006\n",
            "Epoch 28: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 68.8993 - rmse: 8.3006 - val_loss: 60.4912 - val_rmse: 7.7724\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0300 - rmse: 8.3099\n",
            "Epoch 29: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 8s 85ms/step - loss: 69.0300 - rmse: 8.3099 - val_loss: 61.4095 - val_rmse: 7.8307\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1988 - rmse: 8.3213\n",
            "Epoch 30: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 85ms/step - loss: 69.1988 - rmse: 8.3213 - val_loss: 63.4252 - val_rmse: 7.9642\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1410 - rmse: 8.3136\n",
            "Epoch 31: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.1410 - rmse: 8.3136 - val_loss: 60.1652 - val_rmse: 7.7534\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0554 - rmse: 8.3070\n",
            "Epoch 32: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.0554 - rmse: 8.3070 - val_loss: 62.5498 - val_rmse: 7.9162\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7390 - rmse: 8.2871\n",
            "Epoch 33: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.7390 - rmse: 8.2871 - val_loss: 60.9322 - val_rmse: 7.8034\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.1118 - rmse: 8.3785\n",
            "Epoch 34: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 70.1118 - rmse: 8.3785 - val_loss: 61.0931 - val_rmse: 7.8159\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2134 - rmse: 8.3258\n",
            "Epoch 35: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.2134 - rmse: 8.3258 - val_loss: 60.5773 - val_rmse: 7.7863\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0246 - rmse: 8.3079\n",
            "Epoch 36: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.0246 - rmse: 8.3079 - val_loss: 62.4050 - val_rmse: 7.8988\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0783 - rmse: 8.3140\n",
            "Epoch 37: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 6s 73ms/step - loss: 69.0783 - rmse: 8.3140 - val_loss: 60.1992 - val_rmse: 7.7586\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7671 - rmse: 8.2905\n",
            "Epoch 38: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 68.7671 - rmse: 8.2905 - val_loss: 61.0651 - val_rmse: 7.8139\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3994 - rmse: 8.3345\n",
            "Epoch 39: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.3994 - rmse: 8.3345 - val_loss: 61.6687 - val_rmse: 7.8546\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3334 - rmse: 8.3257\n",
            "Epoch 40: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.3334 - rmse: 8.3257 - val_loss: 60.1843 - val_rmse: 7.7586\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1736 - rmse: 8.3240\n",
            "Epoch 41: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.1736 - rmse: 8.3240 - val_loss: 60.7641 - val_rmse: 7.7928\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2878 - rmse: 8.3252\n",
            "Epoch 42: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 74ms/step - loss: 69.2878 - rmse: 8.3252 - val_loss: 60.2192 - val_rmse: 7.7605\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9211 - rmse: 8.3070\n",
            "Epoch 43: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.9211 - rmse: 8.3070 - val_loss: 63.5150 - val_rmse: 7.9726\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1162 - rmse: 8.3178\n",
            "Epoch 44: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.1162 - rmse: 8.3178 - val_loss: 60.3116 - val_rmse: 7.7603\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0795 - rmse: 8.3137\n",
            "Epoch 45: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.0795 - rmse: 8.3137 - val_loss: 61.5814 - val_rmse: 7.8374\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1275 - rmse: 8.3162\n",
            "Epoch 46: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.1275 - rmse: 8.3162 - val_loss: 60.4571 - val_rmse: 7.7784\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9625 - rmse: 8.3075\n",
            "Epoch 47: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.9625 - rmse: 8.3075 - val_loss: 60.2452 - val_rmse: 7.7609\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2078 - rmse: 8.3128\n",
            "Epoch 48: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.2078 - rmse: 8.3128 - val_loss: 60.1478 - val_rmse: 7.7531\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2810 - rmse: 8.3246\n",
            "Epoch 49: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.2810 - rmse: 8.3246 - val_loss: 62.4812 - val_rmse: 7.9072\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2625 - rmse: 8.3207\n",
            "Epoch 50: val_rmse did not improve from 7.74239\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.2625 - rmse: 8.3207 - val_loss: 62.3319 - val_rmse: 7.8967\n",
            "88/88 [==============================] - 6s 60ms/step\n"
          ]
        }
      ],
      "source": [
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "\n",
        "# Load the VGGFace model without the top layer\n",
        "base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.layers[-1].output\n",
        "# x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "# x = keras.layers.AveragePooling2D()(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='relu')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "# Set up callbacks to save the best model and enable early stopping\n",
        "checkpoint_bmi = ModelCheckpoint('best_bmi_model_v2_2.h5', monitor='val_rmse', save_best_only=True, verbose=1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_bmi], validation_data=validation_set\n",
        ")\n",
        "\n",
        "train_predictions = model.predict(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz04i4-KkqH8",
        "outputId": "e9696a73-3aef-4cef-e128-e47aa839d1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted BMI pi: [[34.134846]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted BMI pi: [[33.83492]]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted BMI pi: [[33.8629]]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted BMI pi: [[33.84612]]\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted BMI: [[33.396328]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI: [[33.75414]]\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted BMI: [[33.79772]]\n"
          ]
        }
      ],
      "source": [
        "def cropped_faces(image_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return None\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            padded_image[:resized_face.shape[0], :resized_face.shape[1]] = resized_face\n",
        "\n",
        "            # Save the padded image\n",
        "            cropped_image_path = '/content/drive/MyDrive/Cropped_Image.jpg'\n",
        "            cv2.imwrite(cropped_image_path, padded_image)\n",
        "            \n",
        "            return cropped_image_path\n",
        "        else:\n",
        "            print(\"No face detected in the image:\", image_path)\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PassportPhoto.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "# Load the new image and perform face cropping\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg' \n",
        "cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "if cropped_image_path is not None:\n",
        "    # Load the cropped image and preprocess\n",
        "    img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0  # Rescale the pixel values\n",
        "    print(\"Preprocessed image shape:\", img_array.shape)\n",
        "else:\n",
        "    print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)\n",
        "\n",
        "# Load the new image\n",
        "image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg'  \n",
        "img = load_img(image_path, target_size=(224, 224))\n",
        "img_array = img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = img_array / 255.0  # Rescale the pixel values\n",
        "\n",
        "# Predict the BMI for the new image\n",
        "prediction = model.predict(img_array)\n",
        "predicted_bmi = prediction[0][0]\n",
        "print(\"Predicted BMI:\", predicted_bmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj5Djrn_nc-v",
        "outputId": "4afab38d-62c6-434f-fb46-84968f3c5d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_91\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_95 (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1/7x7_s2 (Conv2D)          (None, 112, 112, 64  9408        ['input_95[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv1/7x7_s2/bn (BatchNormaliz  (None, 112, 112, 64  256        ['conv1/7x7_s2[0][0]']           \n",
            " ation)                         )                                                                 \n",
            "                                                                                                  \n",
            " activation_196 (Activation)    (None, 112, 112, 64  0           ['conv1/7x7_s2/bn[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_178 (MaxPooling2  (None, 55, 55, 64)  0           ['activation_196[0][0]']         \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_1_1x1_reduce (Conv2D)    (None, 55, 55, 64)   4096        ['max_pooling2d_178[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_1_1x1_reduce/bn (BatchNo  (None, 55, 55, 64)  256         ['conv2_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_197 (Activation)    (None, 55, 55, 64)   0           ['conv2_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv2_1_3x3 (Conv2D)           (None, 55, 55, 64)   36864       ['activation_197[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_1_3x3/bn (BatchNormaliza  (None, 55, 55, 64)  256         ['conv2_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_198 (Activation)    (None, 55, 55, 64)   0           ['conv2_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_1_1x1_increase (Conv2D)  (None, 55, 55, 256)  16384       ['activation_198[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_1_1x1_proj (Conv2D)      (None, 55, 55, 256)  16384       ['max_pooling2d_178[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_1_1x1_increase/bn (Batch  (None, 55, 55, 256)  1024       ['conv2_1_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2_1_1x1_proj/bn (BatchNorm  (None, 55, 55, 256)  1024       ['conv2_1_1x1_proj[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 55, 55, 256)  0           ['conv2_1_1x1_increase/bn[0][0]',\n",
            "                                                                  'conv2_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_199 (Activation)    (None, 55, 55, 256)  0           ['add_64[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2_2_1x1_reduce (Conv2D)    (None, 55, 55, 64)   16384       ['activation_199[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_1x1_reduce/bn (BatchNo  (None, 55, 55, 64)  256         ['conv2_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_200 (Activation)    (None, 55, 55, 64)   0           ['conv2_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv2_2_3x3 (Conv2D)           (None, 55, 55, 64)   36864       ['activation_200[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_3x3/bn (BatchNormaliza  (None, 55, 55, 64)  256         ['conv2_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_201 (Activation)    (None, 55, 55, 64)   0           ['conv2_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_1x1_increase (Conv2D)  (None, 55, 55, 256)  16384       ['activation_201[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_2_1x1_increase/bn (Batch  (None, 55, 55, 256)  1024       ['conv2_2_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 55, 55, 256)  0           ['conv2_2_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_199[0][0]']         \n",
            "                                                                                                  \n",
            " activation_202 (Activation)    (None, 55, 55, 256)  0           ['add_65[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2_3_1x1_reduce (Conv2D)    (None, 55, 55, 64)   16384       ['activation_202[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_1x1_reduce/bn (BatchNo  (None, 55, 55, 64)  256         ['conv2_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_203 (Activation)    (None, 55, 55, 64)   0           ['conv2_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv2_3_3x3 (Conv2D)           (None, 55, 55, 64)   36864       ['activation_203[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_3x3/bn (BatchNormaliza  (None, 55, 55, 64)  256         ['conv2_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_204 (Activation)    (None, 55, 55, 64)   0           ['conv2_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_1x1_increase (Conv2D)  (None, 55, 55, 256)  16384       ['activation_204[0][0]']         \n",
            "                                                                                                  \n",
            " conv2_3_1x1_increase/bn (Batch  (None, 55, 55, 256)  1024       ['conv2_3_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 55, 55, 256)  0           ['conv2_3_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_202[0][0]']         \n",
            "                                                                                                  \n",
            " activation_205 (Activation)    (None, 55, 55, 256)  0           ['add_66[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_1_1x1_reduce (Conv2D)    (None, 28, 28, 128)  32768       ['activation_205[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_206 (Activation)    (None, 28, 28, 128)  0           ['conv3_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_1_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_206[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_207 (Activation)    (None, 28, 28, 128)  0           ['conv3_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_207[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_proj (Conv2D)      (None, 28, 28, 512)  131072      ['activation_205[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_1_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_1_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv3_1_1x1_proj/bn (BatchNorm  (None, 28, 28, 512)  2048       ['conv3_1_1x1_proj[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 28, 28, 512)  0           ['conv3_1_1x1_increase/bn[0][0]',\n",
            "                                                                  'conv3_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_208 (Activation)    (None, 28, 28, 512)  0           ['add_67[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_2_1x1_reduce (Conv2D)    (None, 28, 28, 128)  65536       ['activation_208[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_209 (Activation)    (None, 28, 28, 128)  0           ['conv3_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_2_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_209[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_210 (Activation)    (None, 28, 28, 128)  0           ['conv3_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_210[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_2_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_2_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 28, 28, 512)  0           ['conv3_2_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_208[0][0]']         \n",
            "                                                                                                  \n",
            " activation_211 (Activation)    (None, 28, 28, 512)  0           ['add_68[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_3_1x1_reduce (Conv2D)    (None, 28, 28, 128)  65536       ['activation_211[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_212 (Activation)    (None, 28, 28, 128)  0           ['conv3_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_3_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_212[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_213 (Activation)    (None, 28, 28, 128)  0           ['conv3_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_213[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_3_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_3_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 28, 28, 512)  0           ['conv3_3_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_211[0][0]']         \n",
            "                                                                                                  \n",
            " activation_214 (Activation)    (None, 28, 28, 512)  0           ['add_69[0][0]']                 \n",
            "                                                                                                  \n",
            " conv3_4_1x1_reduce (Conv2D)    (None, 28, 28, 128)  65536       ['activation_214[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_1x1_reduce/bn (BatchNo  (None, 28, 28, 128)  512        ['conv3_4_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_215 (Activation)    (None, 28, 28, 128)  0           ['conv3_4_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv3_4_3x3 (Conv2D)           (None, 28, 28, 128)  147456      ['activation_215[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_3x3/bn (BatchNormaliza  (None, 28, 28, 128)  512        ['conv3_4_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_216 (Activation)    (None, 28, 28, 128)  0           ['conv3_4_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_1x1_increase (Conv2D)  (None, 28, 28, 512)  65536       ['activation_216[0][0]']         \n",
            "                                                                                                  \n",
            " conv3_4_1x1_increase/bn (Batch  (None, 28, 28, 512)  2048       ['conv3_4_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_70 (Add)                   (None, 28, 28, 512)  0           ['conv3_4_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_214[0][0]']         \n",
            "                                                                                                  \n",
            " activation_217 (Activation)    (None, 28, 28, 512)  0           ['add_70[0][0]']                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_reduce (Conv2D)    (None, 14, 14, 256)  131072      ['activation_217[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_1_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_218 (Activation)    (None, 14, 14, 256)  0           ['conv4_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_1_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_218[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_1_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_219 (Activation)    (None, 14, 14, 256)  0           ['conv4_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_1_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_219[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_proj (Conv2D)      (None, 14, 14, 1024  524288      ['activation_217[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_1_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv4_1_1x1_proj/bn (BatchNorm  (None, 14, 14, 1024  4096       ['conv4_1_1x1_proj[0][0]']       \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 14, 14, 1024  0           ['conv4_1_1x1_increase/bn[0][0]',\n",
            "                                )                                 'conv4_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_220 (Activation)    (None, 14, 14, 1024  0           ['add_71[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_2_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_220[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_2_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_221 (Activation)    (None, 14, 14, 256)  0           ['conv4_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_2_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_221[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_2_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_222 (Activation)    (None, 14, 14, 256)  0           ['conv4_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_2_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_222[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_2_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_2_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_72 (Add)                   (None, 14, 14, 1024  0           ['conv4_2_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_220[0][0]']         \n",
            "                                                                                                  \n",
            " activation_223 (Activation)    (None, 14, 14, 1024  0           ['add_72[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_3_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_223[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_3_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_224 (Activation)    (None, 14, 14, 256)  0           ['conv4_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_3_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_224[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_3_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_225 (Activation)    (None, 14, 14, 256)  0           ['conv4_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_3_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_225[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_3_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_3_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_73 (Add)                   (None, 14, 14, 1024  0           ['conv4_3_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_223[0][0]']         \n",
            "                                                                                                  \n",
            " activation_226 (Activation)    (None, 14, 14, 1024  0           ['add_73[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_4_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_226[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_4_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_4_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_227 (Activation)    (None, 14, 14, 256)  0           ['conv4_4_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_4_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_227[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_4_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_4_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_228 (Activation)    (None, 14, 14, 256)  0           ['conv4_4_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_4_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_228[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_4_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_4_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_74 (Add)                   (None, 14, 14, 1024  0           ['conv4_4_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_226[0][0]']         \n",
            "                                                                                                  \n",
            " activation_229 (Activation)    (None, 14, 14, 1024  0           ['add_74[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_5_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_229[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_5_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_5_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_230 (Activation)    (None, 14, 14, 256)  0           ['conv4_5_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_5_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_230[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_5_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_5_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_231 (Activation)    (None, 14, 14, 256)  0           ['conv4_5_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_5_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_231[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_5_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_5_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_75 (Add)                   (None, 14, 14, 1024  0           ['conv4_5_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_229[0][0]']         \n",
            "                                                                                                  \n",
            " activation_232 (Activation)    (None, 14, 14, 1024  0           ['add_75[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_6_1x1_reduce (Conv2D)    (None, 14, 14, 256)  262144      ['activation_232[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_6_1x1_reduce/bn (BatchNo  (None, 14, 14, 256)  1024       ['conv4_6_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_233 (Activation)    (None, 14, 14, 256)  0           ['conv4_6_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv4_6_3x3 (Conv2D)           (None, 14, 14, 256)  589824      ['activation_233[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_6_3x3/bn (BatchNormaliza  (None, 14, 14, 256)  1024       ['conv4_6_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_234 (Activation)    (None, 14, 14, 256)  0           ['conv4_6_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv4_6_1x1_increase (Conv2D)  (None, 14, 14, 1024  262144      ['activation_234[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_6_1x1_increase/bn (Batch  (None, 14, 14, 1024  4096       ['conv4_6_1x1_increase[0][0]']   \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_76 (Add)                   (None, 14, 14, 1024  0           ['conv4_6_1x1_increase/bn[0][0]',\n",
            "                                )                                 'activation_232[0][0]']         \n",
            "                                                                                                  \n",
            " activation_235 (Activation)    (None, 14, 14, 1024  0           ['add_76[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_1_1x1_reduce (Conv2D)    (None, 7, 7, 512)    524288      ['activation_235[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_reduce/bn (BatchNo  (None, 7, 7, 512)   2048        ['conv5_1_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_236 (Activation)    (None, 7, 7, 512)    0           ['conv5_1_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv5_1_3x3 (Conv2D)           (None, 7, 7, 512)    2359296     ['activation_236[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_3x3/bn (BatchNormaliza  (None, 7, 7, 512)   2048        ['conv5_1_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_237 (Activation)    (None, 7, 7, 512)    0           ['conv5_1_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_increase (Conv2D)  (None, 7, 7, 2048)   1048576     ['activation_237[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_proj (Conv2D)      (None, 7, 7, 2048)   2097152     ['activation_235[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_1_1x1_increase/bn (Batch  (None, 7, 7, 2048)  8192        ['conv5_1_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv5_1_1x1_proj/bn (BatchNorm  (None, 7, 7, 2048)  8192        ['conv5_1_1x1_proj[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_77 (Add)                   (None, 7, 7, 2048)   0           ['conv5_1_1x1_increase/bn[0][0]',\n",
            "                                                                  'conv5_1_1x1_proj/bn[0][0]']    \n",
            "                                                                                                  \n",
            " activation_238 (Activation)    (None, 7, 7, 2048)   0           ['add_77[0][0]']                 \n",
            "                                                                                                  \n",
            " conv5_2_1x1_reduce (Conv2D)    (None, 7, 7, 512)    1048576     ['activation_238[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_1x1_reduce/bn (BatchNo  (None, 7, 7, 512)   2048        ['conv5_2_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_239 (Activation)    (None, 7, 7, 512)    0           ['conv5_2_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv5_2_3x3 (Conv2D)           (None, 7, 7, 512)    2359296     ['activation_239[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_3x3/bn (BatchNormaliza  (None, 7, 7, 512)   2048        ['conv5_2_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_240 (Activation)    (None, 7, 7, 512)    0           ['conv5_2_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_1x1_increase (Conv2D)  (None, 7, 7, 2048)   1048576     ['activation_240[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_2_1x1_increase/bn (Batch  (None, 7, 7, 2048)  8192        ['conv5_2_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_78 (Add)                   (None, 7, 7, 2048)   0           ['conv5_2_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_238[0][0]']         \n",
            "                                                                                                  \n",
            " activation_241 (Activation)    (None, 7, 7, 2048)   0           ['add_78[0][0]']                 \n",
            "                                                                                                  \n",
            " conv5_3_1x1_reduce (Conv2D)    (None, 7, 7, 512)    1048576     ['activation_241[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_1x1_reduce/bn (BatchNo  (None, 7, 7, 512)   2048        ['conv5_3_1x1_reduce[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_242 (Activation)    (None, 7, 7, 512)    0           ['conv5_3_1x1_reduce/bn[0][0]']  \n",
            "                                                                                                  \n",
            " conv5_3_3x3 (Conv2D)           (None, 7, 7, 512)    2359296     ['activation_242[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_3x3/bn (BatchNormaliza  (None, 7, 7, 512)   2048        ['conv5_3_3x3[0][0]']            \n",
            " tion)                                                                                            \n",
            "                                                                                                  \n",
            " activation_243 (Activation)    (None, 7, 7, 512)    0           ['conv5_3_3x3/bn[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_1x1_increase (Conv2D)  (None, 7, 7, 2048)   1048576     ['activation_243[0][0]']         \n",
            "                                                                                                  \n",
            " conv5_3_1x1_increase/bn (Batch  (None, 7, 7, 2048)  8192        ['conv5_3_1x1_increase[0][0]']   \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_79 (Add)                   (None, 7, 7, 2048)   0           ['conv5_3_1x1_increase/bn[0][0]',\n",
            "                                                                  'activation_241[0][0]']         \n",
            "                                                                                                  \n",
            " activation_244 (Activation)    (None, 7, 7, 2048)   0           ['add_79[0][0]']                 \n",
            "                                                                                                  \n",
            " avg_pool (AveragePooling2D)    (None, 1, 1, 2048)   0           ['activation_244[0][0]']         \n",
            "                                                                                                  \n",
            " dense_211 (Dense)              (None, 1, 1, 128)    262272      ['avg_pool[0][0]']               \n",
            "                                                                                                  \n",
            " dense_212 (Dense)              (None, 1, 1, 1)      129         ['dense_211[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,823,553\n",
            "Trainable params: 266,497\n",
            "Non-trainable params: 23,557,056\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 192.9137 - rmse: 13.9022\n",
            "Epoch 1: val_rmse improved from inf to 8.11186, saving model to best_bmi_model_v2_2.h5\n",
            "88/88 [==============================] - 12s 95ms/step - loss: 192.9137 - rmse: 13.9022 - val_loss: 65.6994 - val_rmse: 8.1119\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8932 - rmse: 8.3035\n",
            "Epoch 2: val_rmse improved from 8.11186 to 7.89413, saving model to best_bmi_model_v2_2.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 68.8932 - rmse: 8.3035 - val_loss: 62.2739 - val_rmse: 7.8941\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7004 - rmse: 8.2915\n",
            "Epoch 3: val_rmse improved from 7.89413 to 7.75028, saving model to best_bmi_model_v2_2.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 68.7004 - rmse: 8.2915 - val_loss: 60.1748 - val_rmse: 7.7503\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0004 - rmse: 8.3111\n",
            "Epoch 4: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.0004 - rmse: 8.3111 - val_loss: 60.6083 - val_rmse: 7.7765\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8898 - rmse: 8.3014\n",
            "Epoch 5: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.8898 - rmse: 8.3014 - val_loss: 61.3261 - val_rmse: 7.8352\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2858 - rmse: 8.3249\n",
            "Epoch 6: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.2858 - rmse: 8.3249 - val_loss: 60.5086 - val_rmse: 7.7845\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7145 - rmse: 8.2923\n",
            "Epoch 7: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 68.7145 - rmse: 8.2923 - val_loss: 61.8529 - val_rmse: 7.8674\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2130 - rmse: 8.3196\n",
            "Epoch 8: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.2130 - rmse: 8.3196 - val_loss: 60.9839 - val_rmse: 7.8174\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8176 - rmse: 8.2945\n",
            "Epoch 9: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.8176 - rmse: 8.2945 - val_loss: 61.4547 - val_rmse: 7.8458\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8715 - rmse: 8.2920\n",
            "Epoch 10: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.8715 - rmse: 8.2920 - val_loss: 60.8013 - val_rmse: 7.7998\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9376 - rmse: 8.3076\n",
            "Epoch 11: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 68.9376 - rmse: 8.3076 - val_loss: 60.2138 - val_rmse: 7.7505\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0893 - rmse: 8.3121\n",
            "Epoch 12: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.0893 - rmse: 8.3121 - val_loss: 60.2742 - val_rmse: 7.7568\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8983 - rmse: 8.3030\n",
            "Epoch 13: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.8983 - rmse: 8.3030 - val_loss: 60.2989 - val_rmse: 7.7682\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9986 - rmse: 8.3103\n",
            "Epoch 14: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.9986 - rmse: 8.3103 - val_loss: 60.4108 - val_rmse: 7.7694\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0876 - rmse: 8.3067\n",
            "Epoch 15: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.0876 - rmse: 8.3067 - val_loss: 60.3396 - val_rmse: 7.7651\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0129 - rmse: 8.3091\n",
            "Epoch 16: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 69.0129 - rmse: 8.3091 - val_loss: 60.4805 - val_rmse: 7.7789\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3378 - rmse: 8.3058\n",
            "Epoch 17: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.3378 - rmse: 8.3058 - val_loss: 61.5927 - val_rmse: 7.8510\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1735 - rmse: 8.3148\n",
            "Epoch 18: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.1735 - rmse: 8.3148 - val_loss: 61.9876 - val_rmse: 7.8760\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8943 - rmse: 8.3030\n",
            "Epoch 19: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.8943 - rmse: 8.3030 - val_loss: 60.5009 - val_rmse: 7.7758\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.6614 - rmse: 8.3484\n",
            "Epoch 20: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.6614 - rmse: 8.3484 - val_loss: 60.3612 - val_rmse: 7.7741\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.9241 - rmse: 8.3033\n",
            "Epoch 21: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 68.9241 - rmse: 8.3033 - val_loss: 60.1641 - val_rmse: 7.7619\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2088 - rmse: 8.3197\n",
            "Epoch 22: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.2088 - rmse: 8.3197 - val_loss: 62.3874 - val_rmse: 7.9018\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0468 - rmse: 8.3135\n",
            "Epoch 23: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.0468 - rmse: 8.3135 - val_loss: 60.1939 - val_rmse: 7.7516\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3116 - rmse: 8.3170\n",
            "Epoch 24: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.3116 - rmse: 8.3170 - val_loss: 60.4350 - val_rmse: 7.7728\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7462 - rmse: 8.2918\n",
            "Epoch 25: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.7462 - rmse: 8.2918 - val_loss: 60.2254 - val_rmse: 7.7529\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.6213 - rmse: 8.2818\n",
            "Epoch 26: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.6213 - rmse: 8.2818 - val_loss: 61.5701 - val_rmse: 7.8493\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0769 - rmse: 8.3158\n",
            "Epoch 27: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 69.0769 - rmse: 8.3158 - val_loss: 62.2548 - val_rmse: 7.8906\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.4725 - rmse: 8.3339\n",
            "Epoch 28: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 69.4725 - rmse: 8.3339 - val_loss: 60.2082 - val_rmse: 7.7633\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.8956 - rmse: 8.3656\n",
            "Epoch 29: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.8956 - rmse: 8.3656 - val_loss: 60.7890 - val_rmse: 7.7962\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0379 - rmse: 8.3141\n",
            "Epoch 30: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 69.0379 - rmse: 8.3141 - val_loss: 60.2195 - val_rmse: 7.7590\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1965 - rmse: 8.3240\n",
            "Epoch 31: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.1965 - rmse: 8.3240 - val_loss: 60.1504 - val_rmse: 7.7562\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8688 - rmse: 8.3034\n",
            "Epoch 32: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 68.8688 - rmse: 8.3034 - val_loss: 60.4742 - val_rmse: 7.7786\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2384 - rmse: 8.3248\n",
            "Epoch 33: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.2384 - rmse: 8.3248 - val_loss: 60.2251 - val_rmse: 7.7611\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0964 - rmse: 8.3166\n",
            "Epoch 34: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.0964 - rmse: 8.3166 - val_loss: 60.9746 - val_rmse: 7.8125\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3528 - rmse: 8.3298\n",
            "Epoch 35: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 69.3528 - rmse: 8.3298 - val_loss: 61.1867 - val_rmse: 7.8240\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3978 - rmse: 8.3333\n",
            "Epoch 36: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 69.3978 - rmse: 8.3333 - val_loss: 60.7720 - val_rmse: 7.7992\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.5404 - rmse: 8.3322\n",
            "Epoch 37: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 69.5404 - rmse: 8.3322 - val_loss: 60.9364 - val_rmse: 7.8076\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1397 - rmse: 8.3177\n",
            "Epoch 38: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.1397 - rmse: 8.3177 - val_loss: 60.1935 - val_rmse: 7.7625\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0560 - rmse: 8.3143\n",
            "Epoch 39: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.0560 - rmse: 8.3143 - val_loss: 64.1236 - val_rmse: 8.0076\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0935 - rmse: 8.3101\n",
            "Epoch 40: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.0935 - rmse: 8.3101 - val_loss: 60.5375 - val_rmse: 7.7759\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.1620 - rmse: 8.3188\n",
            "Epoch 41: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.1620 - rmse: 8.3188 - val_loss: 60.1708 - val_rmse: 7.7610\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0234 - rmse: 8.3121\n",
            "Epoch 42: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 69.0234 - rmse: 8.3121 - val_loss: 60.1926 - val_rmse: 7.7633\n",
            "Epoch 43/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 69.0630 - rmse: 8.3104\n",
            "Epoch 43: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.1001 - rmse: 8.3119 - val_loss: 60.1568 - val_rmse: 7.7548\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.7312 - rmse: 8.2879\n",
            "Epoch 44: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 68.7312 - rmse: 8.2879 - val_loss: 60.2144 - val_rmse: 7.7544\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.6298 - rmse: 8.2817\n",
            "Epoch 45: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 68.6298 - rmse: 8.2817 - val_loss: 61.4833 - val_rmse: 7.8348\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.4704 - rmse: 8.3385\n",
            "Epoch 46: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.4704 - rmse: 8.3385 - val_loss: 60.2700 - val_rmse: 7.7573\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.0250 - rmse: 8.3096\n",
            "Epoch 47: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 69.0250 - rmse: 8.3096 - val_loss: 60.2480 - val_rmse: 7.7648\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3420 - rmse: 8.3308\n",
            "Epoch 48: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 69.3420 - rmse: 8.3308 - val_loss: 60.1826 - val_rmse: 7.7527\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.3695 - rmse: 8.3289\n",
            "Epoch 49: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 69.3695 - rmse: 8.3289 - val_loss: 60.5292 - val_rmse: 7.7790\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.2391 - rmse: 8.3194\n",
            "Epoch 50: val_rmse did not improve from 7.75028\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.2391 - rmse: 8.3194 - val_loss: 60.2636 - val_rmse: 7.7636\n",
            "88/88 [==============================] - 6s 61ms/step\n"
          ]
        }
      ],
      "source": [
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "\n",
        "# Load the VGGFace model without the top layer\n",
        "base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.layers[-1].output\n",
        "# x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "# x = keras.layers.AveragePooling2D()(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='relu')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "# Set up callbacks to save the best model and enable early stopping\n",
        "checkpoint_bmi = ModelCheckpoint('best_bmi_model_v2_2.h5', monitor='val_rmse', save_best_only=True, verbose=1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_bmi], validation_data=validation_set\n",
        ")\n",
        "\n",
        "train_predictions = model.predict(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwRvpg5qvpGS",
        "outputId": "09b73590-4594-4cc4-a04a-fa7c562bb9a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_106\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_110 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_367 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_17 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_59 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_241 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_242 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 102,689\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 301.7998 - rmse: 16.7301\n",
            "Epoch 1: val_rmse improved from inf to 11.13308, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 9s 84ms/step - loss: 301.7998 - rmse: 16.7301 - val_loss: 140.4159 - val_rmse: 11.1331\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 111.2576 - rmse: 9.5482\n",
            "Epoch 2: val_rmse improved from 11.13308 to 8.78076, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 111.2576 - rmse: 9.5482 - val_loss: 96.5554 - val_rmse: 8.7808\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 90.9873 - rmse: 8.5488\n",
            "Epoch 3: val_rmse improved from 8.78076 to 8.13986, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 90.9873 - rmse: 8.5488 - val_loss: 85.9350 - val_rmse: 8.1399\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 81.8160 - rmse: 8.0543\n",
            "Epoch 4: val_rmse improved from 8.13986 to 7.66315, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 81.8160 - rmse: 8.0543 - val_loss: 76.9548 - val_rmse: 7.6631\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 75.1782 - rmse: 7.6796\n",
            "Epoch 5: val_rmse improved from 7.66315 to 7.28911, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 75.1782 - rmse: 7.6796 - val_loss: 72.1002 - val_rmse: 7.2891\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.5750 - rmse: 7.3586\n",
            "Epoch 6: val_rmse did not improve from 7.28911\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 69.5750 - rmse: 7.3586 - val_loss: 68.2080 - val_rmse: 7.4643\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 64.6873 - rmse: 7.0795\n",
            "Epoch 7: val_rmse improved from 7.28911 to 7.20602, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 64.6873 - rmse: 7.0795 - val_loss: 65.5085 - val_rmse: 7.2060\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 60.3330 - rmse: 6.8146\n",
            "Epoch 8: val_rmse improved from 7.20602 to 6.86871, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 60.3330 - rmse: 6.8146 - val_loss: 63.2822 - val_rmse: 6.8687\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 57.9784 - rmse: 6.6559\n",
            "Epoch 9: val_rmse improved from 6.86871 to 6.69193, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 57.9784 - rmse: 6.6559 - val_loss: 69.2519 - val_rmse: 6.6919\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 57.1418 - rmse: 6.5865\n",
            "Epoch 10: val_rmse did not improve from 6.69193\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 57.1418 - rmse: 6.5865 - val_loss: 62.8031 - val_rmse: 6.7565\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 55.1374 - rmse: 6.4571\n",
            "Epoch 11: val_rmse did not improve from 6.69193\n",
            "88/88 [==============================] - 7s 84ms/step - loss: 55.1374 - rmse: 6.4571 - val_loss: 63.0600 - val_rmse: 7.1615\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 52.7079 - rmse: 6.3336\n",
            "Epoch 12: val_rmse did not improve from 6.69193\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 52.7079 - rmse: 6.3336 - val_loss: 62.4087 - val_rmse: 7.1347\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 51.2162 - rmse: 6.2216\n",
            "Epoch 13: val_rmse did not improve from 6.69193\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 51.2162 - rmse: 6.2216 - val_loss: 62.5409 - val_rmse: 6.8291\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 48.3758 - rmse: 6.0640\n",
            "Epoch 14: val_rmse did not improve from 6.69193\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 48.3758 - rmse: 6.0640 - val_loss: 62.7378 - val_rmse: 6.7308\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 46.9026 - rmse: 5.9298\n",
            "Epoch 15: val_rmse did not improve from 6.69193\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 46.9026 - rmse: 5.9298 - val_loss: 62.9921 - val_rmse: 6.8093\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 45.0382 - rmse: 5.8157\n",
            "Epoch 16: val_rmse improved from 6.69193 to 6.64886, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 45.0382 - rmse: 5.8157 - val_loss: 67.5800 - val_rmse: 6.6489\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 43.0411 - rmse: 5.6770\n",
            "Epoch 17: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 43.0411 - rmse: 5.6770 - val_loss: 66.5253 - val_rmse: 6.6729\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 42.3790 - rmse: 5.6105\n",
            "Epoch 18: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 42.3790 - rmse: 5.6105 - val_loss: 65.1274 - val_rmse: 6.8847\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 39.3198 - rmse: 5.4355\n",
            "Epoch 19: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 39.3198 - rmse: 5.4355 - val_loss: 64.8405 - val_rmse: 6.8669\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 37.8524 - rmse: 5.3062\n",
            "Epoch 20: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 37.8524 - rmse: 5.3062 - val_loss: 65.8097 - val_rmse: 7.3128\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 37.5735 - rmse: 5.2867\n",
            "Epoch 21: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 37.5735 - rmse: 5.2867 - val_loss: 65.5163 - val_rmse: 6.8628\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 36.7182 - rmse: 5.2010\n",
            "Epoch 22: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 36.7182 - rmse: 5.2010 - val_loss: 66.2988 - val_rmse: 7.0358\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 34.5667 - rmse: 5.0740\n",
            "Epoch 23: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 34.5667 - rmse: 5.0740 - val_loss: 71.4689 - val_rmse: 6.8718\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 31.3630 - rmse: 4.8076\n",
            "Epoch 24: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 31.3630 - rmse: 4.8076 - val_loss: 70.0728 - val_rmse: 6.8339\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 29.8344 - rmse: 4.6979\n",
            "Epoch 25: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 29.8344 - rmse: 4.6979 - val_loss: 67.3551 - val_rmse: 6.9660\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 27.6564 - rmse: 4.4980\n",
            "Epoch 26: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 27.6564 - rmse: 4.4980 - val_loss: 69.0498 - val_rmse: 7.0477\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 26.5882 - rmse: 4.4210\n",
            "Epoch 27: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 26.5882 - rmse: 4.4210 - val_loss: 67.3000 - val_rmse: 7.0956\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 26.1240 - rmse: 4.3662\n",
            "Epoch 28: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 26.1240 - rmse: 4.3662 - val_loss: 69.3553 - val_rmse: 7.0946\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 24.4518 - rmse: 4.2417\n",
            "Epoch 29: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 24.4518 - rmse: 4.2417 - val_loss: 70.4870 - val_rmse: 7.1287\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 24.0694 - rmse: 4.1805\n",
            "Epoch 30: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 24.0694 - rmse: 4.1805 - val_loss: 70.5205 - val_rmse: 7.2379\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 21.6614 - rmse: 3.9877\n",
            "Epoch 31: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 21.6614 - rmse: 3.9877 - val_loss: 71.7803 - val_rmse: 6.9974\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 20.5027 - rmse: 3.8597\n",
            "Epoch 32: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 20.5027 - rmse: 3.8597 - val_loss: 72.9011 - val_rmse: 7.2964\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.1161 - rmse: 3.7504\n",
            "Epoch 33: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 19.1161 - rmse: 3.7504 - val_loss: 70.7385 - val_rmse: 7.1786\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.7849 - rmse: 3.6834\n",
            "Epoch 34: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 18.7849 - rmse: 3.6834 - val_loss: 77.2266 - val_rmse: 7.3019\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.6996 - rmse: 3.5866\n",
            "Epoch 35: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 17.6996 - rmse: 3.5866 - val_loss: 70.9830 - val_rmse: 7.3472\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.6730 - rmse: 3.4843\n",
            "Epoch 36: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 16.6730 - rmse: 3.4843 - val_loss: 75.4920 - val_rmse: 7.1480\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.8100 - rmse: 3.3929\n",
            "Epoch 37: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 15.8100 - rmse: 3.3929 - val_loss: 72.7555 - val_rmse: 7.2831\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.8499 - rmse: 3.2907\n",
            "Epoch 38: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 14.8499 - rmse: 3.2907 - val_loss: 73.2295 - val_rmse: 7.3989\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.7176 - rmse: 3.2742\n",
            "Epoch 39: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 14.7176 - rmse: 3.2742 - val_loss: 74.9469 - val_rmse: 7.4235\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.1364 - rmse: 3.1076\n",
            "Epoch 40: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 13.1364 - rmse: 3.1076 - val_loss: 77.6714 - val_rmse: 7.3485\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.2040 - rmse: 3.0733\n",
            "Epoch 41: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 13.2040 - rmse: 3.0733 - val_loss: 79.0881 - val_rmse: 7.4916\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.8734 - rmse: 3.0777\n",
            "Epoch 42: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 12.8734 - rmse: 3.0777 - val_loss: 80.0135 - val_rmse: 7.4175\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.4974 - rmse: 2.9187\n",
            "Epoch 43: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 11.4974 - rmse: 2.9187 - val_loss: 79.0254 - val_rmse: 7.3039\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.1065 - rmse: 2.8315\n",
            "Epoch 44: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 11.1065 - rmse: 2.8315 - val_loss: 77.9088 - val_rmse: 7.4347\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.7402 - rmse: 2.8120\n",
            "Epoch 45: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 10.7402 - rmse: 2.8120 - val_loss: 79.3495 - val_rmse: 7.4447\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.3332 - rmse: 2.7236\n",
            "Epoch 46: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 10.3332 - rmse: 2.7236 - val_loss: 77.7924 - val_rmse: 7.5568\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.7226 - rmse: 2.6851\n",
            "Epoch 47: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 9.7226 - rmse: 2.6851 - val_loss: 79.1695 - val_rmse: 7.4388\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.4093 - rmse: 2.7305\n",
            "Epoch 48: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 10.4093 - rmse: 2.7305 - val_loss: 80.2009 - val_rmse: 7.4594\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.1289 - rmse: 2.7108\n",
            "Epoch 49: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 10.1289 - rmse: 2.7108 - val_loss: 80.6726 - val_rmse: 7.5322\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.9391 - rmse: 2.5491\n",
            "Epoch 50: val_rmse did not improve from 6.64886\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.9391 - rmse: 2.5491 - val_loss: 78.4394 - val_rmse: 7.6436\n",
            "88/88 [==============================] - 5s 60ms/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='bmi',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='other',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "\n",
        "# Load the VGGFace model without the top layer\n",
        "base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.layers[-1].output\n",
        "x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "x = keras.layers.AveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='linear')(x)  # Use linear activation for BMI prediction\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    penalty = 10.0  # Penalty factor for overestimation\n",
        "    squared_difference = tf.square(y_true - y_pred)\n",
        "    overestimation_penalty = tf.maximum(0.0, y_pred - y_true) * penalty\n",
        "    return tf.reduce_mean(squared_difference + overestimation_penalty, axis=-1)\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer='adam', loss=custom_loss, metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "# Set up callbacks to save the best model and enable early stopping\n",
        "checkpoint_bmi = ModelCheckpoint('best_bmi_model_v3_0.h5', monitor='val_rmse', save_best_only=True, verbose=1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_bmi],\n",
        "    validation_data=validation_set\n",
        ")\n",
        "\n",
        "train_predictions = model.predict(train_set)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PkiaGj96DaT_"
      },
      "source": [
        "## Final comparison of VGGFace models for predicting BMI below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrmyHfNIDany",
        "outputId": "c678beec-f2e2-4fc3-b87f-8f4c4a2d2dd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_112\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_116 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_373 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_23 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_65 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_253 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_254 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 102,689\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 431.5259 - rmse: 17.3316\n",
            "Epoch 1: val_rmse improved from inf to 11.62350, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 9s 79ms/step - loss: 431.5259 - rmse: 17.3316 - val_loss: 206.3716 - val_rmse: 11.6235\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 161.6897 - rmse: 9.9633\n",
            "Epoch 2: val_rmse improved from 11.62350 to 9.08881, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 161.6897 - rmse: 9.9633 - val_loss: 145.0878 - val_rmse: 9.0888\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 128.9474 - rmse: 8.7793\n",
            "Epoch 3: val_rmse improved from 9.08881 to 8.28719, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 128.9474 - rmse: 8.7793 - val_loss: 119.6608 - val_rmse: 8.2872\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 114.3373 - rmse: 8.2384\n",
            "Epoch 4: val_rmse improved from 8.28719 to 7.86967, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 114.3373 - rmse: 8.2384 - val_loss: 109.7645 - val_rmse: 7.8697\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 104.8961 - rmse: 7.8403\n",
            "Epoch 5: val_rmse improved from 7.86967 to 7.73908, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 104.8961 - rmse: 7.8403 - val_loss: 99.3395 - val_rmse: 7.7391\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 99.5080 - rmse: 7.6176\n",
            "Epoch 6: val_rmse improved from 7.73908 to 7.66735, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 99.5080 - rmse: 7.6176 - val_loss: 94.4808 - val_rmse: 7.6673\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 91.5033 - rmse: 7.3055\n",
            "Epoch 7: val_rmse improved from 7.66735 to 7.29415, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 91.5033 - rmse: 7.3055 - val_loss: 88.9709 - val_rmse: 7.2941\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 85.5369 - rmse: 7.0259\n",
            "Epoch 8: val_rmse did not improve from 7.29415\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 85.5369 - rmse: 7.0259 - val_loss: 87.5972 - val_rmse: 7.4037\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 80.2751 - rmse: 6.7970\n",
            "Epoch 9: val_rmse improved from 7.29415 to 7.20601, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 80.2751 - rmse: 6.7970 - val_loss: 85.0535 - val_rmse: 7.2060\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 79.2343 - rmse: 6.7677\n",
            "Epoch 10: val_rmse improved from 7.20601 to 6.63013, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 79.2343 - rmse: 6.7677 - val_loss: 86.7699 - val_rmse: 6.6301\n",
            "Epoch 11/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 76.7463 - rmse: 6.6188\n",
            "Epoch 11: val_rmse did not improve from 6.63013\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 76.4670 - rmse: 6.6076 - val_loss: 84.2501 - val_rmse: 7.2193\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.2323 - rmse: 6.4822\n",
            "Epoch 12: val_rmse did not improve from 6.63013\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 73.2323 - rmse: 6.4822 - val_loss: 82.6396 - val_rmse: 7.0517\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.9645 - rmse: 6.4790\n",
            "Epoch 13: val_rmse improved from 6.63013 to 6.62113, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 73.9645 - rmse: 6.4790 - val_loss: 86.6064 - val_rmse: 6.6211\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.8555 - rmse: 6.2708\n",
            "Epoch 14: val_rmse did not improve from 6.62113\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 68.8555 - rmse: 6.2708 - val_loss: 82.2599 - val_rmse: 7.0373\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.2694 - rmse: 6.2562\n",
            "Epoch 15: val_rmse did not improve from 6.62113\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.2694 - rmse: 6.2562 - val_loss: 95.7526 - val_rmse: 6.6948\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.1413 - rmse: 6.1991\n",
            "Epoch 16: val_rmse did not improve from 6.62113\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 68.1413 - rmse: 6.1991 - val_loss: 84.8154 - val_rmse: 7.3238\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 64.4887 - rmse: 6.0737\n",
            "Epoch 17: val_rmse did not improve from 6.62113\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 64.4887 - rmse: 6.0737 - val_loss: 84.6554 - val_rmse: 6.6559\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 61.9320 - rmse: 5.9548\n",
            "Epoch 18: val_rmse did not improve from 6.62113\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 61.9320 - rmse: 5.9548 - val_loss: 85.3397 - val_rmse: 6.6907\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 60.4583 - rmse: 5.8700\n",
            "Epoch 19: val_rmse improved from 6.62113 to 6.57643, saving model to best_bmi_model_v3_0.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 60.4583 - rmse: 5.8700 - val_loss: 88.4059 - val_rmse: 6.5764\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 58.5631 - rmse: 5.7618\n",
            "Epoch 20: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 58.5631 - rmse: 5.7618 - val_loss: 84.6794 - val_rmse: 7.1717\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 56.2279 - rmse: 5.6573\n",
            "Epoch 21: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 56.2279 - rmse: 5.6573 - val_loss: 84.7140 - val_rmse: 6.8214\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 54.8190 - rmse: 5.6106\n",
            "Epoch 22: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 54.8190 - rmse: 5.6106 - val_loss: 95.5364 - val_rmse: 6.7191\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 53.2289 - rmse: 5.4971\n",
            "Epoch 23: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 53.2289 - rmse: 5.4971 - val_loss: 84.5909 - val_rmse: 6.9083\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 49.2613 - rmse: 5.2933\n",
            "Epoch 24: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 49.2613 - rmse: 5.2933 - val_loss: 95.7469 - val_rmse: 6.7708\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 48.2817 - rmse: 5.2546\n",
            "Epoch 25: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 48.2817 - rmse: 5.2546 - val_loss: 86.4805 - val_rmse: 6.9310\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 46.3262 - rmse: 5.1508\n",
            "Epoch 26: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 46.3262 - rmse: 5.1508 - val_loss: 90.3931 - val_rmse: 6.8244\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 43.2962 - rmse: 4.9568\n",
            "Epoch 27: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 43.2962 - rmse: 4.9568 - val_loss: 88.5616 - val_rmse: 6.9342\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 42.1345 - rmse: 4.9135\n",
            "Epoch 28: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 42.1345 - rmse: 4.9135 - val_loss: 90.3761 - val_rmse: 6.9455\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 40.4312 - rmse: 4.8218\n",
            "Epoch 29: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 40.4312 - rmse: 4.8218 - val_loss: 89.1730 - val_rmse: 6.9847\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 37.8617 - rmse: 4.6590\n",
            "Epoch 30: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 37.8617 - rmse: 4.6590 - val_loss: 90.3333 - val_rmse: 7.0847\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 35.5765 - rmse: 4.5245\n",
            "Epoch 31: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 35.5765 - rmse: 4.5245 - val_loss: 92.8288 - val_rmse: 6.9899\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 33.8770 - rmse: 4.4334\n",
            "Epoch 32: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 33.8770 - rmse: 4.4334 - val_loss: 95.4386 - val_rmse: 7.0238\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 32.1356 - rmse: 4.3009\n",
            "Epoch 33: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 32.1356 - rmse: 4.3009 - val_loss: 91.1955 - val_rmse: 7.1688\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 29.7918 - rmse: 4.1704\n",
            "Epoch 34: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 29.7918 - rmse: 4.1704 - val_loss: 92.6985 - val_rmse: 7.1205\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 29.0770 - rmse: 4.1144\n",
            "Epoch 35: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 29.0770 - rmse: 4.1144 - val_loss: 92.7857 - val_rmse: 7.2759\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 27.0946 - rmse: 3.9728\n",
            "Epoch 36: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 27.0946 - rmse: 3.9728 - val_loss: 94.3497 - val_rmse: 7.1279\n",
            "Epoch 37/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 26.2457 - rmse: 3.9190\n",
            "Epoch 37: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 26.2018 - rmse: 3.9171 - val_loss: 92.8476 - val_rmse: 7.3799\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 24.8332 - rmse: 3.8286\n",
            "Epoch 38: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 24.8332 - rmse: 3.8286 - val_loss: 94.7892 - val_rmse: 7.2397\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 23.9205 - rmse: 3.7471\n",
            "Epoch 39: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 23.9205 - rmse: 3.7471 - val_loss: 95.7052 - val_rmse: 7.3923\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 24.1326 - rmse: 3.7755\n",
            "Epoch 40: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 24.1326 - rmse: 3.7755 - val_loss: 95.4949 - val_rmse: 7.4931\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 22.5363 - rmse: 3.6620\n",
            "Epoch 41: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 22.5363 - rmse: 3.6620 - val_loss: 101.8888 - val_rmse: 7.4412\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 20.5149 - rmse: 3.4971\n",
            "Epoch 42: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 20.5149 - rmse: 3.4971 - val_loss: 99.1130 - val_rmse: 7.2222\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.1290 - rmse: 3.3806\n",
            "Epoch 43: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 19.1290 - rmse: 3.3806 - val_loss: 101.9580 - val_rmse: 7.3119\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.0300 - rmse: 3.3606\n",
            "Epoch 44: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 19.0300 - rmse: 3.3606 - val_loss: 99.2665 - val_rmse: 7.4589\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.7232 - rmse: 3.2655\n",
            "Epoch 45: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 17.7232 - rmse: 3.2655 - val_loss: 98.8653 - val_rmse: 7.6199\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.6535 - rmse: 3.2605\n",
            "Epoch 46: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 17.6535 - rmse: 3.2605 - val_loss: 100.4150 - val_rmse: 7.3898\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.8121 - rmse: 3.1669\n",
            "Epoch 47: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 16.8121 - rmse: 3.1669 - val_loss: 99.8050 - val_rmse: 7.6022\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.5873 - rmse: 3.1890\n",
            "Epoch 48: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 16.5873 - rmse: 3.1890 - val_loss: 98.3614 - val_rmse: 7.5124\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.1246 - rmse: 3.0983\n",
            "Epoch 49: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 16.1246 - rmse: 3.0983 - val_loss: 101.0468 - val_rmse: 7.5278\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.1265 - rmse: 3.1015\n",
            "Epoch 50: val_rmse did not improve from 6.57643\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 16.1265 - rmse: 3.1015 - val_loss: 101.3545 - val_rmse: 7.6368\n",
            "Checking with unfreezing last 0 layers: \n",
            "88/88 [==============================] - 6s 61ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "Predicted BMI pi: 21.527758\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 30.13539\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI pi: 30.650173\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 26.701988\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_113\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_117 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_374 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_24 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_66 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_255 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_256 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 102,689\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 421.2154 - rmse: 17.1141\n",
            "Epoch 1: val_rmse improved from inf to 11.72421, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 9s 81ms/step - loss: 421.2154 - rmse: 17.1141 - val_loss: 203.7872 - val_rmse: 11.7242\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 153.6349 - rmse: 9.6821\n",
            "Epoch 2: val_rmse improved from 11.72421 to 9.77108, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 153.6349 - rmse: 9.6821 - val_loss: 141.4637 - val_rmse: 9.7711\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 122.9557 - rmse: 8.5459\n",
            "Epoch 3: val_rmse improved from 9.77108 to 8.38053, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 122.9557 - rmse: 8.5459 - val_loss: 113.5938 - val_rmse: 8.3805\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 109.8914 - rmse: 8.0678\n",
            "Epoch 4: val_rmse improved from 8.38053 to 8.08589, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 109.8914 - rmse: 8.0678 - val_loss: 104.6302 - val_rmse: 8.0859\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 103.0101 - rmse: 7.7580\n",
            "Epoch 5: val_rmse improved from 8.08589 to 7.87820, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 103.0101 - rmse: 7.7580 - val_loss: 97.7989 - val_rmse: 7.8782\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 92.2434 - rmse: 7.3386\n",
            "Epoch 6: val_rmse improved from 7.87820 to 7.24321, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 92.2434 - rmse: 7.3386 - val_loss: 90.8764 - val_rmse: 7.2432\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 87.6118 - rmse: 7.1203\n",
            "Epoch 7: val_rmse improved from 7.24321 to 6.96599, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 87.6118 - rmse: 7.1203 - val_loss: 86.2427 - val_rmse: 6.9660\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 81.5938 - rmse: 6.8839\n",
            "Epoch 8: val_rmse improved from 6.96599 to 6.77871, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 81.5938 - rmse: 6.8839 - val_loss: 97.0964 - val_rmse: 6.7787\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 81.8084 - rmse: 6.8497\n",
            "Epoch 9: val_rmse improved from 6.77871 to 6.70653, saving model to best_bmi_model_v3_1.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 81.8084 - rmse: 6.8497 - val_loss: 90.9855 - val_rmse: 6.7065\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 75.3983 - rmse: 6.5859\n",
            "Epoch 10: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 75.3983 - rmse: 6.5859 - val_loss: 82.2974 - val_rmse: 6.9225\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 72.2338 - rmse: 6.4415\n",
            "Epoch 11: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 72.2338 - rmse: 6.4415 - val_loss: 84.5479 - val_rmse: 7.2130\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 69.4423 - rmse: 6.3064\n",
            "Epoch 12: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 69.4423 - rmse: 6.3064 - val_loss: 83.5146 - val_rmse: 6.8742\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 70.2423 - rmse: 6.3287\n",
            "Epoch 13: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 70.2423 - rmse: 6.3287 - val_loss: 83.5904 - val_rmse: 7.1815\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 65.9500 - rmse: 6.1418\n",
            "Epoch 14: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 65.9500 - rmse: 6.1418 - val_loss: 82.8714 - val_rmse: 7.1280\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 63.0599 - rmse: 6.0060\n",
            "Epoch 15: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 63.0599 - rmse: 6.0060 - val_loss: 83.0914 - val_rmse: 6.9183\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 60.9811 - rmse: 5.8967\n",
            "Epoch 16: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 60.9811 - rmse: 5.8967 - val_loss: 83.0106 - val_rmse: 6.9194\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 59.7749 - rmse: 5.8057\n",
            "Epoch 17: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 59.7749 - rmse: 5.8057 - val_loss: 85.1633 - val_rmse: 7.2606\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 56.8322 - rmse: 5.7175\n",
            "Epoch 18: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 56.8322 - rmse: 5.7175 - val_loss: 85.3992 - val_rmse: 6.8463\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 54.9656 - rmse: 5.6364\n",
            "Epoch 19: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 54.9656 - rmse: 5.6364 - val_loss: 85.8136 - val_rmse: 6.7859\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 54.0894 - rmse: 5.5466\n",
            "Epoch 20: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 54.0894 - rmse: 5.5466 - val_loss: 85.5969 - val_rmse: 7.1099\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 50.3405 - rmse: 5.3707\n",
            "Epoch 21: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 50.3405 - rmse: 5.3707 - val_loss: 87.6901 - val_rmse: 6.7120\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 47.3983 - rmse: 5.2214\n",
            "Epoch 22: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 47.3983 - rmse: 5.2214 - val_loss: 86.1849 - val_rmse: 6.9399\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 47.1978 - rmse: 5.2019\n",
            "Epoch 23: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 47.1978 - rmse: 5.2019 - val_loss: 92.3661 - val_rmse: 6.7378\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 44.1510 - rmse: 5.0103\n",
            "Epoch 24: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 44.1510 - rmse: 5.0103 - val_loss: 87.5798 - val_rmse: 7.1525\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 42.4833 - rmse: 4.9191\n",
            "Epoch 25: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 42.4833 - rmse: 4.9191 - val_loss: 87.9637 - val_rmse: 6.9300\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 40.0289 - rmse: 4.8019\n",
            "Epoch 26: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 40.0289 - rmse: 4.8019 - val_loss: 90.3243 - val_rmse: 7.0651\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 39.1862 - rmse: 4.7372\n",
            "Epoch 27: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 39.1862 - rmse: 4.7372 - val_loss: 89.4009 - val_rmse: 7.1299\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 35.9415 - rmse: 4.5612\n",
            "Epoch 28: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 35.9415 - rmse: 4.5612 - val_loss: 92.7848 - val_rmse: 7.1853\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 34.2334 - rmse: 4.4412\n",
            "Epoch 29: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 34.2334 - rmse: 4.4412 - val_loss: 100.0746 - val_rmse: 7.0109\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 33.1653 - rmse: 4.3710\n",
            "Epoch 30: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 33.1653 - rmse: 4.3710 - val_loss: 94.8311 - val_rmse: 7.2104\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 30.5137 - rmse: 4.2035\n",
            "Epoch 31: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 30.5137 - rmse: 4.2035 - val_loss: 92.9884 - val_rmse: 7.0973\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 29.5352 - rmse: 4.1679\n",
            "Epoch 32: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 29.5352 - rmse: 4.1679 - val_loss: 93.1430 - val_rmse: 7.1614\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 28.2849 - rmse: 4.0434\n",
            "Epoch 33: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 28.2849 - rmse: 4.0434 - val_loss: 94.4687 - val_rmse: 7.4552\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 26.1466 - rmse: 3.9165\n",
            "Epoch 34: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 26.1466 - rmse: 3.9165 - val_loss: 95.3732 - val_rmse: 7.3294\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 25.4311 - rmse: 3.8500\n",
            "Epoch 35: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 25.4311 - rmse: 3.8500 - val_loss: 97.2068 - val_rmse: 7.3250\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 23.5837 - rmse: 3.7303\n",
            "Epoch 36: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 23.5837 - rmse: 3.7303 - val_loss: 97.8171 - val_rmse: 7.3689\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 22.8798 - rmse: 3.6512\n",
            "Epoch 37: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 22.8798 - rmse: 3.6512 - val_loss: 98.1125 - val_rmse: 7.2523\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 20.9650 - rmse: 3.5408\n",
            "Epoch 38: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 20.9650 - rmse: 3.5408 - val_loss: 99.3701 - val_rmse: 7.2439\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 20.8182 - rmse: 3.5040\n",
            "Epoch 39: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 20.8182 - rmse: 3.5040 - val_loss: 98.3245 - val_rmse: 7.3618\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.2181 - rmse: 3.3958\n",
            "Epoch 40: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 19.2181 - rmse: 3.3958 - val_loss: 100.0547 - val_rmse: 7.2381\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.3188 - rmse: 3.2998\n",
            "Epoch 41: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 18.3188 - rmse: 3.2998 - val_loss: 98.3893 - val_rmse: 7.4093\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.9764 - rmse: 3.2668\n",
            "Epoch 42: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 17.9764 - rmse: 3.2668 - val_loss: 99.9965 - val_rmse: 7.5627\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.3618 - rmse: 3.2269\n",
            "Epoch 43: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 17.3618 - rmse: 3.2269 - val_loss: 101.7899 - val_rmse: 7.4660\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.7329 - rmse: 3.1917\n",
            "Epoch 44: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 16.7329 - rmse: 3.1917 - val_loss: 102.2694 - val_rmse: 7.6114\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.8238 - rmse: 3.0760\n",
            "Epoch 45: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 15.8238 - rmse: 3.0760 - val_loss: 106.0234 - val_rmse: 7.6002\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.2183 - rmse: 3.0134\n",
            "Epoch 46: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 15.2183 - rmse: 3.0134 - val_loss: 102.9579 - val_rmse: 7.5921\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.4615 - rmse: 2.9741\n",
            "Epoch 47: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 14.4615 - rmse: 2.9741 - val_loss: 101.4494 - val_rmse: 7.5640\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.5162 - rmse: 2.8476\n",
            "Epoch 48: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 13.5162 - rmse: 2.8476 - val_loss: 102.7709 - val_rmse: 7.6248\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.5930 - rmse: 2.8800\n",
            "Epoch 49: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 13.5930 - rmse: 2.8800 - val_loss: 103.9475 - val_rmse: 7.6252\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.8686 - rmse: 2.8932\n",
            "Epoch 50: val_rmse did not improve from 6.70653\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 13.8686 - rmse: 2.8932 - val_loss: 103.1852 - val_rmse: 7.7670\n",
            "Checking with unfreezing last 1 layers: \n",
            "88/88 [==============================] - 6s 62ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "Predicted BMI pi: 24.54591\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 28.40789\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI pi: 30.434587\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted BMI pi: 25.18641\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_114\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_118 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_375 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_25 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_67 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_257 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_258 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 2,462,497\n",
            "Non-trainable params: 12,354,880\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 259.3701 - rmse: 13.0401\n",
            "Epoch 1: val_rmse improved from inf to 9.94582, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 9s 83ms/step - loss: 259.3701 - rmse: 13.0401 - val_loss: 148.9141 - val_rmse: 9.9458\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 136.0309 - rmse: 9.0400\n",
            "Epoch 2: val_rmse improved from 9.94582 to 9.91750, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 136.0309 - rmse: 9.0400 - val_loss: 143.6322 - val_rmse: 9.9175\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 117.2777 - rmse: 8.3645\n",
            "Epoch 3: val_rmse improved from 9.91750 to 8.30524, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 117.2777 - rmse: 8.3645 - val_loss: 124.3389 - val_rmse: 8.3052\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 100.9766 - rmse: 7.7241\n",
            "Epoch 4: val_rmse improved from 8.30524 to 8.01880, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 100.9766 - rmse: 7.7241 - val_loss: 111.6505 - val_rmse: 8.0188\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 78.5002 - rmse: 6.7317\n",
            "Epoch 5: val_rmse did not improve from 8.01880\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 78.5002 - rmse: 6.7317 - val_loss: 139.3724 - val_rmse: 9.8858\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 67.2723 - rmse: 6.2360\n",
            "Epoch 6: val_rmse improved from 8.01880 to 7.90143, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 67.2723 - rmse: 6.2360 - val_loss: 128.7562 - val_rmse: 7.9014\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 49.1056 - rmse: 5.3045\n",
            "Epoch 7: val_rmse improved from 7.90143 to 7.80450, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 49.1056 - rmse: 5.3045 - val_loss: 104.8322 - val_rmse: 7.8045\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 41.1749 - rmse: 4.8370\n",
            "Epoch 8: val_rmse did not improve from 7.80450\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 41.1749 - rmse: 4.8370 - val_loss: 125.0843 - val_rmse: 9.2521\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 33.6034 - rmse: 4.4492\n",
            "Epoch 9: val_rmse improved from 7.80450 to 7.32896, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 33.6034 - rmse: 4.4492 - val_loss: 96.3464 - val_rmse: 7.3290\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 21.7041 - rmse: 3.5819\n",
            "Epoch 10: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 21.7041 - rmse: 3.5819 - val_loss: 105.0000 - val_rmse: 8.2914\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.0877 - rmse: 3.3012\n",
            "Epoch 11: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 18.0877 - rmse: 3.3012 - val_loss: 96.8227 - val_rmse: 7.5054\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.2247 - rmse: 2.9385\n",
            "Epoch 12: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 14.2247 - rmse: 2.9385 - val_loss: 98.8690 - val_rmse: 7.5023\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.9999 - rmse: 2.7184\n",
            "Epoch 13: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 11.9999 - rmse: 2.7184 - val_loss: 99.6270 - val_rmse: 7.3842\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.9729 - rmse: 3.0897\n",
            "Epoch 14: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 15.9729 - rmse: 3.0897 - val_loss: 102.0056 - val_rmse: 7.3317\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.5985 - rmse: 2.7673\n",
            "Epoch 15: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 12.5985 - rmse: 2.7673 - val_loss: 99.0929 - val_rmse: 7.5469\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.0161 - rmse: 2.6143\n",
            "Epoch 16: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 11.0161 - rmse: 2.6143 - val_loss: 98.7994 - val_rmse: 7.8059\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.7921 - rmse: 2.7107\n",
            "Epoch 17: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 11.7921 - rmse: 2.7107 - val_loss: 98.0058 - val_rmse: 7.7303\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.8046 - rmse: 2.4808\n",
            "Epoch 18: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.8046 - rmse: 2.4808 - val_loss: 100.4489 - val_rmse: 7.5239\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.7852 - rmse: 2.6205\n",
            "Epoch 19: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 10.7852 - rmse: 2.6205 - val_loss: 103.6827 - val_rmse: 7.3722\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.1813 - rmse: 2.4108\n",
            "Epoch 20: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 9.1813 - rmse: 2.4108 - val_loss: 97.0949 - val_rmse: 7.6988\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.3996 - rmse: 2.1563\n",
            "Epoch 21: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.3996 - rmse: 2.1563 - val_loss: 99.2891 - val_rmse: 7.8216\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.5411 - rmse: 2.0543\n",
            "Epoch 22: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.5411 - rmse: 2.0543 - val_loss: 97.4656 - val_rmse: 7.3523\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.0012 - rmse: 1.9677\n",
            "Epoch 23: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 6.0012 - rmse: 1.9677 - val_loss: 101.6259 - val_rmse: 7.4280\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.5951 - rmse: 2.0757\n",
            "Epoch 24: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.5951 - rmse: 2.0757 - val_loss: 98.6383 - val_rmse: 7.5233\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.6384 - rmse: 1.9319\n",
            "Epoch 25: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 5.6384 - rmse: 1.9319 - val_loss: 96.1114 - val_rmse: 7.4379\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.1596 - rmse: 2.0471\n",
            "Epoch 26: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.1596 - rmse: 2.0471 - val_loss: 103.5850 - val_rmse: 7.4458\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.7075 - rmse: 1.9340\n",
            "Epoch 27: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 5.7075 - rmse: 1.9340 - val_loss: 102.6777 - val_rmse: 7.3968\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.9247 - rmse: 1.9739\n",
            "Epoch 28: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 5.9247 - rmse: 1.9739 - val_loss: 103.7556 - val_rmse: 7.4903\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.3050 - rmse: 2.2006\n",
            "Epoch 29: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.3050 - rmse: 2.2006 - val_loss: 108.2683 - val_rmse: 7.4982\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4565 - rmse: 2.3051\n",
            "Epoch 30: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.4565 - rmse: 2.3051 - val_loss: 98.3163 - val_rmse: 7.4619\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.5117 - rmse: 1.9087\n",
            "Epoch 31: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 5.5117 - rmse: 1.9087 - val_loss: 101.9932 - val_rmse: 7.4450\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.8614 - rmse: 1.9489\n",
            "Epoch 32: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 5.8614 - rmse: 1.9489 - val_loss: 101.5303 - val_rmse: 7.4233\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.2090 - rmse: 1.8290\n",
            "Epoch 33: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 5.2090 - rmse: 1.8290 - val_loss: 97.1946 - val_rmse: 7.6032\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.1943 - rmse: 1.8644\n",
            "Epoch 34: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 5.1943 - rmse: 1.8644 - val_loss: 95.7024 - val_rmse: 7.4706\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.3003 - rmse: 2.2894\n",
            "Epoch 35: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 8.3003 - rmse: 2.2894 - val_loss: 100.0596 - val_rmse: 7.4734\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.2706 - rmse: 2.1690\n",
            "Epoch 36: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 7.2706 - rmse: 2.1690 - val_loss: 96.1923 - val_rmse: 7.3952\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.0762 - rmse: 1.8428\n",
            "Epoch 37: val_rmse did not improve from 7.32896\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 5.0762 - rmse: 1.8428 - val_loss: 95.9886 - val_rmse: 7.4808\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.1456 - rmse: 1.8497\n",
            "Epoch 38: val_rmse improved from 7.32896 to 7.28181, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 5.1456 - rmse: 1.8497 - val_loss: 97.1325 - val_rmse: 7.2818\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.6951 - rmse: 1.7538\n",
            "Epoch 39: val_rmse did not improve from 7.28181\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 4.6951 - rmse: 1.7538 - val_loss: 98.5127 - val_rmse: 7.3763\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.7098 - rmse: 1.7563\n",
            "Epoch 40: val_rmse did not improve from 7.28181\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 4.7098 - rmse: 1.7563 - val_loss: 98.0589 - val_rmse: 7.5132\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.6753 - rmse: 1.7578\n",
            "Epoch 41: val_rmse improved from 7.28181 to 7.23480, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 4.6753 - rmse: 1.7578 - val_loss: 98.2362 - val_rmse: 7.2348\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.4028 - rmse: 1.7243\n",
            "Epoch 42: val_rmse did not improve from 7.23480\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 4.4028 - rmse: 1.7243 - val_loss: 95.9635 - val_rmse: 7.4229\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.8529 - rmse: 1.7952\n",
            "Epoch 43: val_rmse did not improve from 7.23480\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 4.8529 - rmse: 1.7952 - val_loss: 95.6871 - val_rmse: 7.5767\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.6702 - rmse: 2.3756\n",
            "Epoch 44: val_rmse did not improve from 7.23480\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 8.6702 - rmse: 2.3756 - val_loss: 98.5501 - val_rmse: 7.8028\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3107 - rmse: 2.0267\n",
            "Epoch 45: val_rmse did not improve from 7.23480\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.3107 - rmse: 2.0267 - val_loss: 95.9928 - val_rmse: 7.5403\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.9309 - rmse: 1.8191\n",
            "Epoch 46: val_rmse improved from 7.23480 to 7.22799, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 4.9309 - rmse: 1.8191 - val_loss: 95.9179 - val_rmse: 7.2280\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.3154 - rmse: 1.7154\n",
            "Epoch 47: val_rmse did not improve from 7.22799\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 4.3154 - rmse: 1.7154 - val_loss: 96.1224 - val_rmse: 7.5961\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 3.8317 - rmse: 1.6343\n",
            "Epoch 48: val_rmse improved from 7.22799 to 7.22733, saving model to best_bmi_model_v3_2.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 3.8317 - rmse: 1.6343 - val_loss: 96.3964 - val_rmse: 7.2273\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.0889 - rmse: 1.6715\n",
            "Epoch 49: val_rmse did not improve from 7.22733\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 4.0889 - rmse: 1.6715 - val_loss: 94.9682 - val_rmse: 7.3772\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.4550 - rmse: 1.7409\n",
            "Epoch 50: val_rmse did not improve from 7.22733\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 4.4550 - rmse: 1.7409 - val_loss: 96.9251 - val_rmse: 7.2883\n",
            "Checking with unfreezing last 2 layers: \n",
            "88/88 [==============================] - 6s 62ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "Predicted BMI pi: 25.751207\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted BMI pi: 31.077726\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted BMI pi: 34.2332\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted BMI pi: 29.430784\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_115\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_119 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_376 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_26 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_68 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_259 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_260 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 4,822,305\n",
            "Non-trainable params: 9,995,072\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 261.4308 - rmse: 13.0967\n",
            "Epoch 1: val_rmse improved from inf to 9.86920, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 10s 82ms/step - loss: 261.4308 - rmse: 13.0967 - val_loss: 148.0156 - val_rmse: 9.8692\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 147.1402 - rmse: 9.4213\n",
            "Epoch 2: val_rmse improved from 9.86920 to 8.74717, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 147.1402 - rmse: 9.4213 - val_loss: 133.9913 - val_rmse: 8.7472\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 134.2402 - rmse: 8.9665\n",
            "Epoch 3: val_rmse did not improve from 8.74717\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 134.2402 - rmse: 8.9665 - val_loss: 153.0442 - val_rmse: 8.9658\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 114.3678 - rmse: 8.2228\n",
            "Epoch 4: val_rmse did not improve from 8.74717\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 114.3678 - rmse: 8.2228 - val_loss: 136.7402 - val_rmse: 9.5833\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 91.9068 - rmse: 7.3598\n",
            "Epoch 5: val_rmse improved from 8.74717 to 8.38659, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 91.9068 - rmse: 7.3598 - val_loss: 120.9029 - val_rmse: 8.3866\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 77.6588 - rmse: 6.7308\n",
            "Epoch 6: val_rmse improved from 8.38659 to 8.04968, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 77.6588 - rmse: 6.7308 - val_loss: 116.4661 - val_rmse: 8.0497\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 58.7875 - rmse: 5.8084\n",
            "Epoch 7: val_rmse improved from 8.04968 to 8.03498, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 58.7875 - rmse: 5.8084 - val_loss: 109.1639 - val_rmse: 8.0350\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 45.4258 - rmse: 5.1056\n",
            "Epoch 8: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 45.4258 - rmse: 5.1056 - val_loss: 112.9984 - val_rmse: 8.3669\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 34.6818 - rmse: 4.4900\n",
            "Epoch 9: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 34.6818 - rmse: 4.4900 - val_loss: 127.3868 - val_rmse: 8.0452\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 26.3869 - rmse: 3.9436\n",
            "Epoch 10: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 26.3869 - rmse: 3.9436 - val_loss: 117.2332 - val_rmse: 8.2689\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 25.5537 - rmse: 3.8900\n",
            "Epoch 11: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 25.5537 - rmse: 3.8900 - val_loss: 116.6310 - val_rmse: 8.5065\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.6971 - rmse: 3.4382\n",
            "Epoch 12: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 19.6971 - rmse: 3.4382 - val_loss: 115.0238 - val_rmse: 8.3222\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.5943 - rmse: 2.9998\n",
            "Epoch 13: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 14.5943 - rmse: 2.9998 - val_loss: 140.5984 - val_rmse: 8.4074\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.0656 - rmse: 2.9127\n",
            "Epoch 14: val_rmse did not improve from 8.03498\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 14.0656 - rmse: 2.9127 - val_loss: 113.3982 - val_rmse: 8.5171\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.8763 - rmse: 3.0131\n",
            "Epoch 15: val_rmse improved from 8.03498 to 7.92073, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 14.8763 - rmse: 3.0131 - val_loss: 110.0423 - val_rmse: 7.9207\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.8157 - rmse: 2.6965\n",
            "Epoch 16: val_rmse improved from 7.92073 to 7.82809, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 11.8157 - rmse: 2.6965 - val_loss: 111.5261 - val_rmse: 7.8281\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.7169 - rmse: 2.6833\n",
            "Epoch 17: val_rmse did not improve from 7.82809\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 11.7169 - rmse: 2.6833 - val_loss: 116.6155 - val_rmse: 8.5218\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.4336 - rmse: 2.4656\n",
            "Epoch 18: val_rmse did not improve from 7.82809\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.4336 - rmse: 2.4656 - val_loss: 119.3670 - val_rmse: 7.9576\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.0520 - rmse: 2.4178\n",
            "Epoch 19: val_rmse improved from 7.82809 to 7.72518, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.0520 - rmse: 2.4178 - val_loss: 110.0578 - val_rmse: 7.7252\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.5060 - rmse: 2.3500\n",
            "Epoch 20: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.5060 - rmse: 2.3500 - val_loss: 108.7697 - val_rmse: 7.7910\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.5337 - rmse: 2.2880\n",
            "Epoch 21: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 8.5337 - rmse: 2.2880 - val_loss: 107.1203 - val_rmse: 8.0991\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.8126 - rmse: 2.2449\n",
            "Epoch 22: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 7.8126 - rmse: 2.2449 - val_loss: 114.0273 - val_rmse: 7.7362\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.3323 - rmse: 2.1862\n",
            "Epoch 23: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.3323 - rmse: 2.1862 - val_loss: 115.0862 - val_rmse: 7.9230\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9206 - rmse: 2.0994\n",
            "Epoch 24: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 6.9206 - rmse: 2.0994 - val_loss: 110.7045 - val_rmse: 8.1826\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.8646 - rmse: 1.9919\n",
            "Epoch 25: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 5.8646 - rmse: 1.9919 - val_loss: 109.1729 - val_rmse: 7.7348\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3531 - rmse: 2.0410\n",
            "Epoch 26: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 6.3531 - rmse: 2.0410 - val_loss: 111.1165 - val_rmse: 8.0443\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9297 - rmse: 2.1238\n",
            "Epoch 27: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.9297 - rmse: 2.1238 - val_loss: 119.0250 - val_rmse: 7.8128\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.5209 - rmse: 2.2231\n",
            "Epoch 28: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 7.5209 - rmse: 2.2231 - val_loss: 110.3651 - val_rmse: 7.8099\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.7152 - rmse: 2.0782\n",
            "Epoch 29: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.7152 - rmse: 2.0782 - val_loss: 108.4084 - val_rmse: 8.0819\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.6459 - rmse: 2.0817\n",
            "Epoch 30: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.6459 - rmse: 2.0817 - val_loss: 111.4425 - val_rmse: 7.7963\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9631 - rmse: 2.1596\n",
            "Epoch 31: val_rmse did not improve from 7.72518\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.9631 - rmse: 2.1596 - val_loss: 108.7493 - val_rmse: 7.7330\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.1210 - rmse: 2.0139\n",
            "Epoch 32: val_rmse improved from 7.72518 to 7.61112, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 6.1210 - rmse: 2.0139 - val_loss: 104.3970 - val_rmse: 7.6111\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.2659 - rmse: 1.8545\n",
            "Epoch 33: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 5.2659 - rmse: 1.8545 - val_loss: 104.6673 - val_rmse: 7.8848\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.3049 - rmse: 2.1842\n",
            "Epoch 34: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.3049 - rmse: 2.1842 - val_loss: 106.6895 - val_rmse: 7.7911\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.2472 - rmse: 2.3064\n",
            "Epoch 35: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 8.2472 - rmse: 2.3064 - val_loss: 110.8542 - val_rmse: 8.3455\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4687 - rmse: 2.2411\n",
            "Epoch 36: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.4687 - rmse: 2.2411 - val_loss: 107.2877 - val_rmse: 7.7795\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.8122 - rmse: 1.9822\n",
            "Epoch 37: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 5.8122 - rmse: 1.9822 - val_loss: 106.6573 - val_rmse: 7.8187\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.0193 - rmse: 2.0115\n",
            "Epoch 38: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 6.0193 - rmse: 2.0115 - val_loss: 113.5268 - val_rmse: 7.7298\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.9401 - rmse: 1.9552\n",
            "Epoch 39: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 5.9401 - rmse: 1.9552 - val_loss: 108.5821 - val_rmse: 7.6597\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.6627 - rmse: 1.9319\n",
            "Epoch 40: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 5.6627 - rmse: 1.9319 - val_loss: 107.0987 - val_rmse: 7.6137\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.5585 - rmse: 1.9006\n",
            "Epoch 41: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 5.5585 - rmse: 1.9006 - val_loss: 116.6929 - val_rmse: 7.8268\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.1745 - rmse: 2.0140\n",
            "Epoch 42: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.1745 - rmse: 2.0140 - val_loss: 105.9166 - val_rmse: 7.6687\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3078 - rmse: 2.0525\n",
            "Epoch 43: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.3078 - rmse: 2.0525 - val_loss: 105.5658 - val_rmse: 7.7444\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.2313 - rmse: 2.0320\n",
            "Epoch 44: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.2313 - rmse: 2.0320 - val_loss: 104.7722 - val_rmse: 7.8116\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.2489 - rmse: 2.0159\n",
            "Epoch 45: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.2489 - rmse: 2.0159 - val_loss: 104.7984 - val_rmse: 7.7103\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.0610 - rmse: 2.0116\n",
            "Epoch 46: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.0610 - rmse: 2.0116 - val_loss: 111.2184 - val_rmse: 7.7040\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.5178 - rmse: 1.8954\n",
            "Epoch 47: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 5.5178 - rmse: 1.8954 - val_loss: 106.0403 - val_rmse: 7.6852\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.0703 - rmse: 2.1049\n",
            "Epoch 48: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 7.0703 - rmse: 2.1049 - val_loss: 103.2921 - val_rmse: 7.6614\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.0342 - rmse: 2.0096\n",
            "Epoch 49: val_rmse did not improve from 7.61112\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.0342 - rmse: 2.0096 - val_loss: 114.4195 - val_rmse: 7.7507\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3436 - rmse: 2.0169\n",
            "Epoch 50: val_rmse improved from 7.61112 to 7.55984, saving model to best_bmi_model_v3_3.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.3436 - rmse: 2.0169 - val_loss: 103.3900 - val_rmse: 7.5598\n",
            "Checking with unfreezing last 3 layers: \n",
            "88/88 [==============================] - 6s 61ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "Predicted BMI pi: 21.34361\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted BMI pi: 30.76288\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 34.705406\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted BMI pi: 32.921276\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_116\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_120 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_377 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_27 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_69 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_261 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_262 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 7,182,113\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 678.7958 - rmse: 23.3208\n",
            "Epoch 1: val_rmse improved from inf to 10.37989, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 10s 84ms/step - loss: 678.7958 - rmse: 23.3208 - val_loss: 160.4815 - val_rmse: 10.3799\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 151.7047 - rmse: 9.5614\n",
            "Epoch 2: val_rmse improved from 10.37989 to 8.94964, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 151.7047 - rmse: 9.5614 - val_loss: 137.7100 - val_rmse: 8.9496\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 135.5014 - rmse: 9.0490\n",
            "Epoch 3: val_rmse did not improve from 8.94964\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 135.5014 - rmse: 9.0490 - val_loss: 177.9980 - val_rmse: 9.5097\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 118.2595 - rmse: 8.3957\n",
            "Epoch 4: val_rmse did not improve from 8.94964\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 118.2595 - rmse: 8.3957 - val_loss: 138.4171 - val_rmse: 9.5757\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 91.1186 - rmse: 7.3027\n",
            "Epoch 5: val_rmse improved from 8.94964 to 8.68662, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 91.1186 - rmse: 7.3027 - val_loss: 121.0838 - val_rmse: 8.6866\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 67.8375 - rmse: 6.2511\n",
            "Epoch 6: val_rmse improved from 8.68662 to 8.06577, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 67.8375 - rmse: 6.2511 - val_loss: 111.5516 - val_rmse: 8.0658\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 54.5255 - rmse: 5.6052\n",
            "Epoch 7: val_rmse did not improve from 8.06577\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 54.5255 - rmse: 5.6052 - val_loss: 133.2770 - val_rmse: 8.4065\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 32.9521 - rmse: 4.3697\n",
            "Epoch 8: val_rmse improved from 8.06577 to 8.04369, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 32.9521 - rmse: 4.3697 - val_loss: 114.5454 - val_rmse: 8.0437\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 24.2188 - rmse: 3.7730\n",
            "Epoch 9: val_rmse did not improve from 8.04369\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 24.2188 - rmse: 3.7730 - val_loss: 116.8540 - val_rmse: 8.2637\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.6200 - rmse: 3.3453\n",
            "Epoch 10: val_rmse improved from 8.04369 to 7.98347, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 18.6200 - rmse: 3.3453 - val_loss: 122.4531 - val_rmse: 7.9835\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.1943 - rmse: 3.1257\n",
            "Epoch 11: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 16.1943 - rmse: 3.1257 - val_loss: 113.3811 - val_rmse: 8.2504\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.8176 - rmse: 3.0352\n",
            "Epoch 12: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 14.8176 - rmse: 3.0352 - val_loss: 144.6443 - val_rmse: 8.5669\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.2531 - rmse: 2.8675\n",
            "Epoch 13: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 13.2531 - rmse: 2.8675 - val_loss: 114.8575 - val_rmse: 8.4073\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.3216 - rmse: 2.6479\n",
            "Epoch 14: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 11.3216 - rmse: 2.6479 - val_loss: 120.4697 - val_rmse: 8.1256\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.5470 - rmse: 2.7898\n",
            "Epoch 15: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 12.5470 - rmse: 2.7898 - val_loss: 114.4760 - val_rmse: 7.9901\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.3484 - rmse: 3.1849\n",
            "Epoch 16: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 16.3484 - rmse: 3.1849 - val_loss: 129.2213 - val_rmse: 8.2289\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.8272 - rmse: 2.6176\n",
            "Epoch 17: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 10.8272 - rmse: 2.6176 - val_loss: 130.7834 - val_rmse: 8.1611\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.2817 - rmse: 2.4361\n",
            "Epoch 18: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 9.2817 - rmse: 2.4361 - val_loss: 129.7750 - val_rmse: 8.1225\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4663 - rmse: 2.1630\n",
            "Epoch 19: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 7.4663 - rmse: 2.1630 - val_loss: 121.2490 - val_rmse: 8.0103\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.1853 - rmse: 2.1879\n",
            "Epoch 20: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 7.1853 - rmse: 2.1879 - val_loss: 118.2726 - val_rmse: 8.0149\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.1011 - rmse: 2.4161\n",
            "Epoch 21: val_rmse did not improve from 7.98347\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 9.1011 - rmse: 2.4161 - val_loss: 112.3120 - val_rmse: 8.2940\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.1140 - rmse: 2.9579\n",
            "Epoch 22: val_rmse improved from 7.98347 to 7.89732, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 14.1140 - rmse: 2.9579 - val_loss: 113.1549 - val_rmse: 7.8973\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.3594 - rmse: 2.4560\n",
            "Epoch 23: val_rmse improved from 7.89732 to 7.80469, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 9.3594 - rmse: 2.4560 - val_loss: 111.6836 - val_rmse: 7.8047\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9076 - rmse: 2.1091\n",
            "Epoch 24: val_rmse did not improve from 7.80469\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 6.9076 - rmse: 2.1091 - val_loss: 111.2285 - val_rmse: 7.8220\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.2588 - rmse: 2.2958\n",
            "Epoch 25: val_rmse did not improve from 7.80469\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 8.2588 - rmse: 2.2958 - val_loss: 110.3602 - val_rmse: 8.2948\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.7544 - rmse: 2.2585\n",
            "Epoch 26: val_rmse did not improve from 7.80469\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 7.7544 - rmse: 2.2585 - val_loss: 116.0394 - val_rmse: 7.8992\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.5979 - rmse: 2.6166\n",
            "Epoch 27: val_rmse did not improve from 7.80469\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 10.5979 - rmse: 2.6166 - val_loss: 112.3307 - val_rmse: 7.8864\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.6323 - rmse: 2.0848\n",
            "Epoch 28: val_rmse did not improve from 7.80469\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 6.6323 - rmse: 2.0848 - val_loss: 112.3240 - val_rmse: 7.8110\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.4323 - rmse: 1.8970\n",
            "Epoch 29: val_rmse did not improve from 7.80469\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 5.4323 - rmse: 1.8970 - val_loss: 119.4377 - val_rmse: 7.8761\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.5390 - rmse: 1.9253\n",
            "Epoch 30: val_rmse improved from 7.80469 to 7.79264, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 5.5390 - rmse: 1.9253 - val_loss: 110.2552 - val_rmse: 7.7926\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.2913 - rmse: 1.8853\n",
            "Epoch 31: val_rmse did not improve from 7.79264\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 5.2913 - rmse: 1.8853 - val_loss: 109.7289 - val_rmse: 8.1376\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.8010 - rmse: 1.9553\n",
            "Epoch 32: val_rmse did not improve from 7.79264\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 5.8010 - rmse: 1.9553 - val_loss: 110.6959 - val_rmse: 7.8314\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.1985 - rmse: 2.0273\n",
            "Epoch 33: val_rmse did not improve from 7.79264\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.1985 - rmse: 2.0273 - val_loss: 110.2184 - val_rmse: 7.9464\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.4644 - rmse: 1.9222\n",
            "Epoch 34: val_rmse did not improve from 7.79264\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 5.4644 - rmse: 1.9222 - val_loss: 112.7099 - val_rmse: 7.7975\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.2882 - rmse: 2.1730\n",
            "Epoch 35: val_rmse did not improve from 7.79264\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 7.2882 - rmse: 2.1730 - val_loss: 114.1283 - val_rmse: 7.9059\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.7986 - rmse: 2.2434\n",
            "Epoch 36: val_rmse improved from 7.79264 to 7.73117, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 7.7986 - rmse: 2.2434 - val_loss: 111.2385 - val_rmse: 7.7312\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.6404 - rmse: 1.9174\n",
            "Epoch 37: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 5.6404 - rmse: 1.9174 - val_loss: 112.4235 - val_rmse: 7.7739\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.1752 - rmse: 2.4299\n",
            "Epoch 38: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.1752 - rmse: 2.4299 - val_loss: 130.2049 - val_rmse: 8.1693\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9195 - rmse: 2.1076\n",
            "Epoch 39: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.9195 - rmse: 2.1076 - val_loss: 108.4088 - val_rmse: 8.1845\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.7906 - rmse: 2.3621\n",
            "Epoch 40: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 8.7906 - rmse: 2.3621 - val_loss: 108.0016 - val_rmse: 7.8219\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.2453 - rmse: 2.1805\n",
            "Epoch 41: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 7.2453 - rmse: 2.1805 - val_loss: 117.7433 - val_rmse: 7.8007\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4240 - rmse: 2.3127\n",
            "Epoch 42: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 8.4240 - rmse: 2.3127 - val_loss: 118.9266 - val_rmse: 7.8141\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.2873 - rmse: 1.9997\n",
            "Epoch 43: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 6.2873 - rmse: 1.9997 - val_loss: 109.0865 - val_rmse: 7.9153\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9562 - rmse: 2.1278\n",
            "Epoch 44: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.9562 - rmse: 2.1278 - val_loss: 115.9716 - val_rmse: 7.7688\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3816 - rmse: 2.0607\n",
            "Epoch 45: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.3816 - rmse: 2.0607 - val_loss: 115.8847 - val_rmse: 7.7644\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.8278 - rmse: 1.7878\n",
            "Epoch 46: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 4.8278 - rmse: 1.7878 - val_loss: 107.3849 - val_rmse: 7.7697\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.8759 - rmse: 1.8197\n",
            "Epoch 47: val_rmse did not improve from 7.73117\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 4.8759 - rmse: 1.8197 - val_loss: 110.1894 - val_rmse: 7.7586\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.0187 - rmse: 2.1268\n",
            "Epoch 48: val_rmse improved from 7.73117 to 7.73013, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.0187 - rmse: 2.1268 - val_loss: 109.8958 - val_rmse: 7.7301\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.3852 - rmse: 1.8847\n",
            "Epoch 49: val_rmse improved from 7.73013 to 7.68152, saving model to best_bmi_model_v3_4.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 5.3852 - rmse: 1.8847 - val_loss: 107.7931 - val_rmse: 7.6815\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.4481 - rmse: 1.8854\n",
            "Epoch 50: val_rmse did not improve from 7.68152\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 5.4481 - rmse: 1.8854 - val_loss: 106.0321 - val_rmse: 8.0394\n",
            "Checking with unfreezing last 4 layers: \n",
            "88/88 [==============================] - 6s 61ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "Predicted BMI pi: 27.493254\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 30.439037\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted BMI pi: 38.465084\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted BMI pi: 33.155586\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_117\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_121 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_378 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_28 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_70 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_263 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_264 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 7,182,113\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 269.8765 - rmse: 13.3160\n",
            "Epoch 1: val_rmse improved from inf to 10.16616, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 10s 89ms/step - loss: 269.8765 - rmse: 13.3160 - val_loss: 200.8266 - val_rmse: 10.1662\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 163.2858 - rmse: 9.9867\n",
            "Epoch 2: val_rmse improved from 10.16616 to 9.16943, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 163.2858 - rmse: 9.9867 - val_loss: 160.4023 - val_rmse: 9.1694\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 132.2796 - rmse: 8.8867\n",
            "Epoch 3: val_rmse improved from 9.16943 to 8.90796, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 132.2796 - rmse: 8.8867 - val_loss: 158.0545 - val_rmse: 8.9080\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 101.5611 - rmse: 7.7158\n",
            "Epoch 4: val_rmse improved from 8.90796 to 8.49802, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 101.5611 - rmse: 7.7158 - val_loss: 114.1838 - val_rmse: 8.4980\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 77.1728 - rmse: 6.6644\n",
            "Epoch 5: val_rmse improved from 8.49802 to 7.91888, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 77.1728 - rmse: 6.6644 - val_loss: 112.1294 - val_rmse: 7.9189\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 55.0032 - rmse: 5.5856\n",
            "Epoch 6: val_rmse did not improve from 7.91888\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 55.0032 - rmse: 5.5856 - val_loss: 108.3238 - val_rmse: 8.2141\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 38.7820 - rmse: 4.7061\n",
            "Epoch 7: val_rmse did not improve from 7.91888\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 38.7820 - rmse: 4.7061 - val_loss: 108.4082 - val_rmse: 8.3322\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 28.0599 - rmse: 4.0652\n",
            "Epoch 8: val_rmse improved from 7.91888 to 7.82746, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 28.0599 - rmse: 4.0652 - val_loss: 105.8015 - val_rmse: 7.8275\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 22.8208 - rmse: 3.6707\n",
            "Epoch 9: val_rmse did not improve from 7.82746\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 22.8208 - rmse: 3.6707 - val_loss: 107.2469 - val_rmse: 7.8489\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.5462 - rmse: 3.3472\n",
            "Epoch 10: val_rmse improved from 7.82746 to 7.80594, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 18.5462 - rmse: 3.3472 - val_loss: 106.3822 - val_rmse: 7.8059\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.1600 - rmse: 2.8536\n",
            "Epoch 11: val_rmse improved from 7.80594 to 7.79395, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 13.1600 - rmse: 2.8536 - val_loss: 105.7715 - val_rmse: 7.7940\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.9807 - rmse: 2.8320\n",
            "Epoch 12: val_rmse improved from 7.79395 to 7.72647, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 12.9807 - rmse: 2.8320 - val_loss: 108.2889 - val_rmse: 7.7265\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.4777 - rmse: 2.7675\n",
            "Epoch 13: val_rmse did not improve from 7.72647\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 12.4777 - rmse: 2.7675 - val_loss: 103.8981 - val_rmse: 7.7340\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.9475 - rmse: 2.6312\n",
            "Epoch 14: val_rmse improved from 7.72647 to 7.69341, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 10.9475 - rmse: 2.6312 - val_loss: 114.2859 - val_rmse: 7.6934\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.3412 - rmse: 2.5374\n",
            "Epoch 15: val_rmse did not improve from 7.69341\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 10.3412 - rmse: 2.5374 - val_loss: 106.6533 - val_rmse: 8.2992\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.6248 - rmse: 2.3711\n",
            "Epoch 16: val_rmse did not improve from 7.69341\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 8.6248 - rmse: 2.3711 - val_loss: 110.8132 - val_rmse: 7.8361\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.8597 - rmse: 2.6147\n",
            "Epoch 17: val_rmse improved from 7.69341 to 7.62056, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 10.8597 - rmse: 2.6147 - val_loss: 106.2759 - val_rmse: 7.6206\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.0845 - rmse: 2.3993\n",
            "Epoch 18: val_rmse did not improve from 7.62056\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 9.0845 - rmse: 2.3993 - val_loss: 108.2185 - val_rmse: 7.7990\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.6589 - rmse: 2.3413\n",
            "Epoch 19: val_rmse did not improve from 7.62056\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 8.6589 - rmse: 2.3413 - val_loss: 107.3637 - val_rmse: 7.6766\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.5230 - rmse: 2.0501\n",
            "Epoch 20: val_rmse did not improve from 7.62056\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.5230 - rmse: 2.0501 - val_loss: 104.2673 - val_rmse: 7.9838\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.0413 - rmse: 2.1463\n",
            "Epoch 21: val_rmse improved from 7.62056 to 7.61858, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 7.0413 - rmse: 2.1463 - val_loss: 103.9239 - val_rmse: 7.6186\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.8923 - rmse: 2.1507\n",
            "Epoch 22: val_rmse improved from 7.61858 to 7.49600, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 6.8923 - rmse: 2.1507 - val_loss: 105.0433 - val_rmse: 7.4960\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.0428 - rmse: 2.3907\n",
            "Epoch 23: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 9.0428 - rmse: 2.3907 - val_loss: 103.3851 - val_rmse: 8.0566\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.9646 - rmse: 2.2778\n",
            "Epoch 24: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.9646 - rmse: 2.2778 - val_loss: 104.3412 - val_rmse: 7.9114\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.0380 - rmse: 2.3748\n",
            "Epoch 25: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 9.0380 - rmse: 2.3748 - val_loss: 105.8867 - val_rmse: 8.1877\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.7534 - rmse: 2.1102\n",
            "Epoch 26: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.7534 - rmse: 2.1102 - val_loss: 101.6856 - val_rmse: 7.6407\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.0405 - rmse: 1.9943\n",
            "Epoch 27: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.0405 - rmse: 1.9943 - val_loss: 110.4933 - val_rmse: 8.5616\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.5823 - rmse: 2.2048\n",
            "Epoch 28: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 7.5823 - rmse: 2.2048 - val_loss: 105.4664 - val_rmse: 7.6647\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.5078 - rmse: 2.2131\n",
            "Epoch 29: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.5078 - rmse: 2.2131 - val_loss: 107.2730 - val_rmse: 7.5982\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.7415 - rmse: 2.1082\n",
            "Epoch 30: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.7415 - rmse: 2.1082 - val_loss: 102.4644 - val_rmse: 7.5269\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3833 - rmse: 2.0326\n",
            "Epoch 31: val_rmse did not improve from 7.49600\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 6.3833 - rmse: 2.0326 - val_loss: 104.8883 - val_rmse: 8.2427\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.2056 - rmse: 2.1674\n",
            "Epoch 32: val_rmse improved from 7.49600 to 7.43784, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 7.2056 - rmse: 2.1674 - val_loss: 102.7885 - val_rmse: 7.4378\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.7586 - rmse: 2.3465\n",
            "Epoch 33: val_rmse did not improve from 7.43784\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.7586 - rmse: 2.3465 - val_loss: 106.6318 - val_rmse: 7.5912\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.7455 - rmse: 2.4768\n",
            "Epoch 34: val_rmse did not improve from 7.43784\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.7455 - rmse: 2.4768 - val_loss: 103.3633 - val_rmse: 7.5230\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3407 - rmse: 2.0391\n",
            "Epoch 35: val_rmse did not improve from 7.43784\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.3407 - rmse: 2.0391 - val_loss: 100.0588 - val_rmse: 7.6244\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.6488 - rmse: 2.0807\n",
            "Epoch 36: val_rmse did not improve from 7.43784\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.6488 - rmse: 2.0807 - val_loss: 100.5938 - val_rmse: 7.5080\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.9955 - rmse: 2.5105 \n",
            "Epoch 37: val_rmse did not improve from 7.43784\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 9.9955 - rmse: 2.5105 - val_loss: 102.7999 - val_rmse: 7.5931\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4034 - rmse: 2.2908\n",
            "Epoch 38: val_rmse did not improve from 7.43784\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.4034 - rmse: 2.2908 - val_loss: 100.7321 - val_rmse: 7.6075\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.8688 - rmse: 2.3958\n",
            "Epoch 39: val_rmse improved from 7.43784 to 7.40956, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 8.8688 - rmse: 2.3958 - val_loss: 103.6526 - val_rmse: 7.4096\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4616 - rmse: 2.2156\n",
            "Epoch 40: val_rmse did not improve from 7.40956\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.4616 - rmse: 2.2156 - val_loss: 100.4898 - val_rmse: 7.7440\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.7880 - rmse: 2.0866\n",
            "Epoch 41: val_rmse did not improve from 7.40956\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.7880 - rmse: 2.0866 - val_loss: 102.1791 - val_rmse: 7.4145\n",
            "Epoch 42/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 7.4553 - rmse: 2.1917\n",
            "Epoch 42: val_rmse did not improve from 7.40956\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.4739 - rmse: 2.1999 - val_loss: 101.8837 - val_rmse: 7.6631\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4835 - rmse: 2.2019\n",
            "Epoch 43: val_rmse did not improve from 7.40956\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 7.4835 - rmse: 2.2019 - val_loss: 103.1670 - val_rmse: 7.4208\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.9894 - rmse: 2.0027\n",
            "Epoch 44: val_rmse improved from 7.40956 to 7.40854, saving model to best_bmi_model_v3_5.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 5.9894 - rmse: 2.0027 - val_loss: 106.5332 - val_rmse: 7.4085\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.3791 - rmse: 2.0509\n",
            "Epoch 45: val_rmse did not improve from 7.40854\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 6.3791 - rmse: 2.0509 - val_loss: 102.8161 - val_rmse: 8.1392\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.9670 - rmse: 1.8302\n",
            "Epoch 46: val_rmse did not improve from 7.40854\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 4.9670 - rmse: 1.8302 - val_loss: 99.2738 - val_rmse: 7.4473\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.7854 - rmse: 1.8056\n",
            "Epoch 47: val_rmse did not improve from 7.40854\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 4.7854 - rmse: 1.8056 - val_loss: 99.7652 - val_rmse: 7.4157\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.2530 - rmse: 1.6879\n",
            "Epoch 48: val_rmse did not improve from 7.40854\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 4.2530 - rmse: 1.6879 - val_loss: 98.6216 - val_rmse: 7.5039\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.1314 - rmse: 1.6742\n",
            "Epoch 49: val_rmse did not improve from 7.40854\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 4.1314 - rmse: 1.6742 - val_loss: 99.8349 - val_rmse: 7.8742\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.4705 - rmse: 1.8932\n",
            "Epoch 50: val_rmse did not improve from 7.40854\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 5.4705 - rmse: 1.8932 - val_loss: 100.7985 - val_rmse: 7.4156\n",
            "Checking with unfreezing last 5 layers: \n",
            "88/88 [==============================] - 6s 62ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "Predicted BMI pi: 30.152449\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI pi: 30.511333\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI pi: 34.826706\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 36.040455\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_118\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_122 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_379 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_29 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_71 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_265 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_266 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 9,541,921\n",
            "Non-trainable params: 5,275,456\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 722.7626 - rmse: 24.4418\n",
            "Epoch 1: val_rmse improved from inf to 10.34961, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 10s 85ms/step - loss: 722.7626 - rmse: 24.4418 - val_loss: 163.7455 - val_rmse: 10.3496\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 160.1793 - rmse: 9.8614\n",
            "Epoch 2: val_rmse improved from 10.34961 to 9.23200, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 160.1793 - rmse: 9.8614 - val_loss: 157.9839 - val_rmse: 9.2320\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 156.1086 - rmse: 9.7124\n",
            "Epoch 3: val_rmse did not improve from 9.23200\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 156.1086 - rmse: 9.7124 - val_loss: 155.7822 - val_rmse: 10.2288\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 137.9668 - rmse: 9.0986\n",
            "Epoch 4: val_rmse improved from 9.23200 to 9.11199, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 8s 86ms/step - loss: 137.9668 - rmse: 9.0986 - val_loss: 142.9207 - val_rmse: 9.1120\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 120.7506 - rmse: 8.4978\n",
            "Epoch 5: val_rmse improved from 9.11199 to 8.95276, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 84ms/step - loss: 120.7506 - rmse: 8.4978 - val_loss: 130.1898 - val_rmse: 8.9528\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 115.1484 - rmse: 8.2707\n",
            "Epoch 6: val_rmse improved from 8.95276 to 8.90095, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 115.1484 - rmse: 8.2707 - val_loss: 143.2067 - val_rmse: 8.9009\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 98.3285 - rmse: 7.5886\n",
            "Epoch 7: val_rmse did not improve from 8.90095\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 98.3285 - rmse: 7.5886 - val_loss: 134.8200 - val_rmse: 9.0333\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 67.2380 - rmse: 6.2575\n",
            "Epoch 8: val_rmse did not improve from 8.90095\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 67.2380 - rmse: 6.2575 - val_loss: 131.8075 - val_rmse: 9.0966\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 55.5687 - rmse: 5.6789\n",
            "Epoch 9: val_rmse improved from 8.90095 to 8.83304, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 55.5687 - rmse: 5.6789 - val_loss: 131.8250 - val_rmse: 8.8330\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 38.4633 - rmse: 4.7232\n",
            "Epoch 10: val_rmse improved from 8.83304 to 8.75121, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 38.4633 - rmse: 4.7232 - val_loss: 124.3312 - val_rmse: 8.7512\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 38.6410 - rmse: 4.7424\n",
            "Epoch 11: val_rmse did not improve from 8.75121\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 38.6410 - rmse: 4.7424 - val_loss: 143.8436 - val_rmse: 9.8686\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 29.2840 - rmse: 4.1350\n",
            "Epoch 12: val_rmse did not improve from 8.75121\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 29.2840 - rmse: 4.1350 - val_loss: 128.6454 - val_rmse: 9.1339\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 25.7623 - rmse: 3.9026\n",
            "Epoch 13: val_rmse improved from 8.75121 to 8.42139, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 25.7623 - rmse: 3.9026 - val_loss: 126.5217 - val_rmse: 8.4214\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 23.9829 - rmse: 3.7938\n",
            "Epoch 14: val_rmse did not improve from 8.42139\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 23.9829 - rmse: 3.7938 - val_loss: 122.5644 - val_rmse: 8.7333\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.8753 - rmse: 3.4563\n",
            "Epoch 15: val_rmse did not improve from 8.42139\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 19.8753 - rmse: 3.4563 - val_loss: 129.6409 - val_rmse: 8.4275\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.4002 - rmse: 3.1652\n",
            "Epoch 16: val_rmse improved from 8.42139 to 8.37554, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 16.4002 - rmse: 3.1652 - val_loss: 123.6483 - val_rmse: 8.3755\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.0872 - rmse: 2.7379\n",
            "Epoch 17: val_rmse improved from 8.37554 to 8.34169, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 12.0872 - rmse: 2.7379 - val_loss: 121.9659 - val_rmse: 8.3417\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.6906 - rmse: 2.4749\n",
            "Epoch 18: val_rmse did not improve from 8.34169\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 9.6906 - rmse: 2.4749 - val_loss: 120.9875 - val_rmse: 8.6736\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4421 - rmse: 2.3134\n",
            "Epoch 19: val_rmse did not improve from 8.34169\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 8.4421 - rmse: 2.3134 - val_loss: 120.7086 - val_rmse: 8.6202\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.9678 - rmse: 2.2630\n",
            "Epoch 20: val_rmse did not improve from 8.34169\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 7.9678 - rmse: 2.2630 - val_loss: 117.4970 - val_rmse: 8.3870\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4024 - rmse: 2.3314\n",
            "Epoch 21: val_rmse improved from 8.34169 to 8.25253, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 8.4024 - rmse: 2.3314 - val_loss: 124.8126 - val_rmse: 8.2525\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.4262 - rmse: 2.5535\n",
            "Epoch 22: val_rmse improved from 8.25253 to 8.20256, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 10.4262 - rmse: 2.5535 - val_loss: 119.3228 - val_rmse: 8.2026\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.3368 - rmse: 2.5639\n",
            "Epoch 23: val_rmse did not improve from 8.20256\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 10.3368 - rmse: 2.5639 - val_loss: 138.7435 - val_rmse: 8.4775\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.5578 - rmse: 2.8132\n",
            "Epoch 24: val_rmse did not improve from 8.20256\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 12.5578 - rmse: 2.8132 - val_loss: 118.1412 - val_rmse: 8.6202\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.1384 - rmse: 2.5338\n",
            "Epoch 25: val_rmse did not improve from 8.20256\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 10.1384 - rmse: 2.5338 - val_loss: 117.7132 - val_rmse: 8.4299\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.7288 - rmse: 2.4920\n",
            "Epoch 26: val_rmse improved from 8.20256 to 8.15981, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 9.7288 - rmse: 2.4920 - val_loss: 121.7386 - val_rmse: 8.1598\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.9020 - rmse: 2.1144\n",
            "Epoch 27: val_rmse did not improve from 8.15981\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 6.9020 - rmse: 2.1144 - val_loss: 117.7097 - val_rmse: 8.4050\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4231 - rmse: 2.3089\n",
            "Epoch 28: val_rmse improved from 8.15981 to 8.10403, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 85ms/step - loss: 8.4231 - rmse: 2.3089 - val_loss: 117.9491 - val_rmse: 8.1040\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.4361 - rmse: 2.3616\n",
            "Epoch 29: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 8.4361 - rmse: 2.3616 - val_loss: 127.0141 - val_rmse: 8.2221\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.9347 - rmse: 2.6364\n",
            "Epoch 30: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 10.9347 - rmse: 2.6364 - val_loss: 120.8763 - val_rmse: 8.1306\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.1701 - rmse: 2.1540\n",
            "Epoch 31: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 7.1701 - rmse: 2.1540 - val_loss: 133.2387 - val_rmse: 8.2492\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.7637 - rmse: 2.3514\n",
            "Epoch 32: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 8.7637 - rmse: 2.3514 - val_loss: 114.9373 - val_rmse: 8.2090\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.7299 - rmse: 2.6047\n",
            "Epoch 33: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 10.7299 - rmse: 2.6047 - val_loss: 117.4430 - val_rmse: 8.2562\n",
            "Epoch 34/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 13.1028 - rmse: 2.8663\n",
            "Epoch 34: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 13.1230 - rmse: 2.8590 - val_loss: 119.6237 - val_rmse: 8.1657\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.9923 - rmse: 2.2738\n",
            "Epoch 35: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 7.9923 - rmse: 2.2738 - val_loss: 121.3611 - val_rmse: 8.1202\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.3183 - rmse: 2.3202\n",
            "Epoch 36: val_rmse did not improve from 8.10403\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 8.3183 - rmse: 2.3202 - val_loss: 133.0925 - val_rmse: 8.2278\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.9971 - rmse: 2.2659\n",
            "Epoch 37: val_rmse improved from 8.10403 to 8.05235, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 7.9971 - rmse: 2.2659 - val_loss: 115.0716 - val_rmse: 8.0524\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.6829 - rmse: 2.2255\n",
            "Epoch 38: val_rmse did not improve from 8.05235\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 7.6829 - rmse: 2.2255 - val_loss: 114.6785 - val_rmse: 8.2223\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.5104 - rmse: 2.4515\n",
            "Epoch 39: val_rmse did not improve from 8.05235\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 9.5104 - rmse: 2.4515 - val_loss: 112.6077 - val_rmse: 8.1453\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.8439 - rmse: 2.3859\n",
            "Epoch 40: val_rmse did not improve from 8.05235\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.8439 - rmse: 2.3859 - val_loss: 114.8197 - val_rmse: 8.4138\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.6767 - rmse: 2.0785\n",
            "Epoch 41: val_rmse did not improve from 8.05235\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.6767 - rmse: 2.0785 - val_loss: 114.0843 - val_rmse: 8.1201\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.1659 - rmse: 2.1558\n",
            "Epoch 42: val_rmse did not improve from 8.05235\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.1659 - rmse: 2.1558 - val_loss: 124.5632 - val_rmse: 8.1311\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.1232 - rmse: 2.2972\n",
            "Epoch 43: val_rmse improved from 8.05235 to 7.99317, saving model to best_bmi_model_v3_6.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 8.1232 - rmse: 2.2972 - val_loss: 117.5605 - val_rmse: 7.9932\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.4271 - rmse: 1.8777\n",
            "Epoch 44: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 5.4271 - rmse: 1.8777 - val_loss: 114.5636 - val_rmse: 8.4479\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.6517 - rmse: 2.3476\n",
            "Epoch 45: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.6517 - rmse: 2.3476 - val_loss: 121.0626 - val_rmse: 8.9312\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4187 - rmse: 2.2112\n",
            "Epoch 46: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 7.4187 - rmse: 2.2112 - val_loss: 115.2790 - val_rmse: 8.1120\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.7815 - rmse: 2.1170\n",
            "Epoch 47: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 6.7815 - rmse: 2.1170 - val_loss: 115.7887 - val_rmse: 8.0086\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.9928 - rmse: 2.2703\n",
            "Epoch 48: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 7.9928 - rmse: 2.2703 - val_loss: 113.5971 - val_rmse: 8.0054\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.3446 - rmse: 2.3073\n",
            "Epoch 49: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 8.3446 - rmse: 2.3073 - val_loss: 116.2777 - val_rmse: 8.6113\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.6244 - rmse: 2.2421\n",
            "Epoch 50: val_rmse did not improve from 7.99317\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 7.6244 - rmse: 2.2421 - val_loss: 114.3323 - val_rmse: 8.1913\n",
            "Checking with unfreezing last 6 layers: \n",
            "88/88 [==============================] - 6s 61ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "Predicted BMI pi: 26.455084\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted BMI pi: 29.727598\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 43.434937\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI pi: 38.789875\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_119\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_123 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_380 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_30 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_72 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_267 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_268 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 11,901,729\n",
            "Non-trainable params: 2,915,648\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 808.2883 - rmse: 25.8651\n",
            "Epoch 1: val_rmse improved from inf to 12.24099, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 11s 85ms/step - loss: 808.2883 - rmse: 25.8651 - val_loss: 214.0039 - val_rmse: 12.2410\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 196.5301 - rmse: 11.0388\n",
            "Epoch 2: val_rmse improved from 12.24099 to 10.50611, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 196.5301 - rmse: 11.0388 - val_loss: 210.4087 - val_rmse: 10.5061\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 181.7071 - rmse: 10.5710\n",
            "Epoch 3: val_rmse improved from 10.50611 to 10.14429, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 181.7071 - rmse: 10.5710 - val_loss: 197.7702 - val_rmse: 10.1443\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 164.1104 - rmse: 9.9838 \n",
            "Epoch 4: val_rmse improved from 10.14429 to 9.85954, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 164.1104 - rmse: 9.9838 - val_loss: 186.8752 - val_rmse: 9.8595\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 168.2583 - rmse: 10.1383\n",
            "Epoch 5: val_rmse improved from 9.85954 to 8.86930, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 168.2583 - rmse: 10.1383 - val_loss: 137.0350 - val_rmse: 8.8693\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 157.3021 - rmse: 9.7616\n",
            "Epoch 6: val_rmse improved from 8.86930 to 8.59244, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 84ms/step - loss: 157.3021 - rmse: 9.7616 - val_loss: 134.8875 - val_rmse: 8.5924\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 148.5794 - rmse: 9.4522\n",
            "Epoch 7: val_rmse did not improve from 8.59244\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 148.5794 - rmse: 9.4522 - val_loss: 142.4324 - val_rmse: 9.8101\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 133.2938 - rmse: 8.9295\n",
            "Epoch 8: val_rmse improved from 8.59244 to 8.38387, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 8s 85ms/step - loss: 133.2938 - rmse: 8.9295 - val_loss: 133.0088 - val_rmse: 8.3839\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 127.7907 - rmse: 8.7123\n",
            "Epoch 9: val_rmse did not improve from 8.38387\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 127.7907 - rmse: 8.7123 - val_loss: 121.6184 - val_rmse: 8.4879\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 118.4650 - rmse: 8.3710\n",
            "Epoch 10: val_rmse improved from 8.38387 to 8.14611, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 84ms/step - loss: 118.4650 - rmse: 8.3710 - val_loss: 125.0982 - val_rmse: 8.1461\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 113.8978 - rmse: 8.1872\n",
            "Epoch 11: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 113.8978 - rmse: 8.1872 - val_loss: 114.3959 - val_rmse: 8.3966\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 120.0222 - rmse: 8.4465\n",
            "Epoch 12: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 120.0222 - rmse: 8.4465 - val_loss: 151.7234 - val_rmse: 10.3434\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 108.2385 - rmse: 7.9737\n",
            "Epoch 13: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 108.2385 - rmse: 7.9737 - val_loss: 131.3461 - val_rmse: 8.2545\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 94.2263 - rmse: 7.3990\n",
            "Epoch 14: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 94.2263 - rmse: 7.3990 - val_loss: 122.8387 - val_rmse: 8.9664\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 89.0785 - rmse: 7.1928\n",
            "Epoch 15: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 89.0785 - rmse: 7.1928 - val_loss: 131.2388 - val_rmse: 8.5675\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 81.2585 - rmse: 6.8313\n",
            "Epoch 16: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 81.2585 - rmse: 6.8313 - val_loss: 116.3376 - val_rmse: 8.1835\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 64.6930 - rmse: 6.0893\n",
            "Epoch 17: val_rmse did not improve from 8.14611\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 64.6930 - rmse: 6.0893 - val_loss: 116.6959 - val_rmse: 8.5845\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 59.3227 - rmse: 5.8332\n",
            "Epoch 18: val_rmse improved from 8.14611 to 8.11752, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 59.3227 - rmse: 5.8332 - val_loss: 116.2943 - val_rmse: 8.1175\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 47.0231 - rmse: 5.2066\n",
            "Epoch 19: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 47.0231 - rmse: 5.2066 - val_loss: 119.2978 - val_rmse: 8.4826\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 37.8763 - rmse: 4.6556\n",
            "Epoch 20: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 37.8763 - rmse: 4.6556 - val_loss: 117.6587 - val_rmse: 8.4313\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 34.5988 - rmse: 4.4753\n",
            "Epoch 21: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 34.5988 - rmse: 4.4753 - val_loss: 125.7537 - val_rmse: 8.2428\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 28.2804 - rmse: 4.0596\n",
            "Epoch 22: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 28.2804 - rmse: 4.0596 - val_loss: 134.3228 - val_rmse: 8.4893\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 32.6895 - rmse: 4.3658\n",
            "Epoch 23: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 32.6895 - rmse: 4.3658 - val_loss: 140.3172 - val_rmse: 8.6045\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 34.9305 - rmse: 4.4800\n",
            "Epoch 24: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 34.9305 - rmse: 4.4800 - val_loss: 120.0703 - val_rmse: 8.7541\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 22.4964 - rmse: 3.6494\n",
            "Epoch 25: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 22.4964 - rmse: 3.6494 - val_loss: 131.1530 - val_rmse: 9.2439\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 20.6660 - rmse: 3.5076\n",
            "Epoch 26: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 20.6660 - rmse: 3.5076 - val_loss: 119.4195 - val_rmse: 8.3341\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.4946 - rmse: 3.2385\n",
            "Epoch 27: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 17.4946 - rmse: 3.2385 - val_loss: 117.1667 - val_rmse: 8.3030\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.3414 - rmse: 3.0831\n",
            "Epoch 28: val_rmse did not improve from 8.11752\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 15.3414 - rmse: 3.0831 - val_loss: 117.3858 - val_rmse: 8.2355\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.5477 - rmse: 2.9040\n",
            "Epoch 29: val_rmse improved from 8.11752 to 8.09615, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 13.5477 - rmse: 2.9040 - val_loss: 121.2902 - val_rmse: 8.0961\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.3170 - rmse: 2.7620\n",
            "Epoch 30: val_rmse did not improve from 8.09615\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 12.3170 - rmse: 2.7620 - val_loss: 123.1710 - val_rmse: 8.2692\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.7603 - rmse: 2.9300\n",
            "Epoch 31: val_rmse did not improve from 8.09615\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 13.7603 - rmse: 2.9300 - val_loss: 118.9784 - val_rmse: 8.4045\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.2571 - rmse: 3.0642\n",
            "Epoch 32: val_rmse did not improve from 8.09615\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 15.2571 - rmse: 3.0642 - val_loss: 118.1242 - val_rmse: 8.5914\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.5335 - rmse: 3.4124\n",
            "Epoch 33: val_rmse did not improve from 8.09615\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 19.5335 - rmse: 3.4124 - val_loss: 116.1245 - val_rmse: 8.1680\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.1059 - rmse: 2.7372\n",
            "Epoch 34: val_rmse improved from 8.09615 to 8.09574, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 12.1059 - rmse: 2.7372 - val_loss: 119.8580 - val_rmse: 8.0957\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.1465 - rmse: 2.5267\n",
            "Epoch 35: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 10.1465 - rmse: 2.5267 - val_loss: 136.1175 - val_rmse: 8.3949\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.9233 - rmse: 3.0179\n",
            "Epoch 36: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 14.9233 - rmse: 3.0179 - val_loss: 117.0423 - val_rmse: 8.3942\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.9959 - rmse: 2.7120\n",
            "Epoch 37: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 11.9959 - rmse: 2.7120 - val_loss: 119.7085 - val_rmse: 8.6487\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.9725 - rmse: 2.9254\n",
            "Epoch 38: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 13.9725 - rmse: 2.9254 - val_loss: 112.9566 - val_rmse: 8.2448\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.5201 - rmse: 2.5868\n",
            "Epoch 39: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 10.5201 - rmse: 2.5868 - val_loss: 122.5376 - val_rmse: 8.2186\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.5980 - rmse: 3.2561\n",
            "Epoch 40: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 17.5980 - rmse: 3.2561 - val_loss: 129.7797 - val_rmse: 8.2694\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.6753 - rmse: 2.7845\n",
            "Epoch 41: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 12.6753 - rmse: 2.7845 - val_loss: 112.6216 - val_rmse: 8.2968\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.9407 - rmse: 2.5538 \n",
            "Epoch 42: val_rmse did not improve from 8.09574\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.9407 - rmse: 2.5538 - val_loss: 117.2682 - val_rmse: 8.2161\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.2497 - rmse: 2.5523\n",
            "Epoch 43: val_rmse improved from 8.09574 to 8.06465, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 10.2497 - rmse: 2.5523 - val_loss: 114.6590 - val_rmse: 8.0647\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.6215 - rmse: 2.4597\n",
            "Epoch 44: val_rmse did not improve from 8.06465\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 9.6215 - rmse: 2.4597 - val_loss: 112.7561 - val_rmse: 8.2031\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.4357 - rmse: 2.5447\n",
            "Epoch 45: val_rmse did not improve from 8.06465\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 10.4357 - rmse: 2.5447 - val_loss: 117.0752 - val_rmse: 8.1477\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.8365 - rmse: 2.2378\n",
            "Epoch 46: val_rmse did not improve from 8.06465\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 7.8365 - rmse: 2.2378 - val_loss: 113.9434 - val_rmse: 8.2006\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.7709 - rmse: 2.1061\n",
            "Epoch 47: val_rmse did not improve from 8.06465\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 6.7709 - rmse: 2.1061 - val_loss: 112.8788 - val_rmse: 8.2530\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.1700 - rmse: 2.2747\n",
            "Epoch 48: val_rmse did not improve from 8.06465\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 8.1700 - rmse: 2.2747 - val_loss: 119.0886 - val_rmse: 8.0789\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4637 - rmse: 2.1938\n",
            "Epoch 49: val_rmse improved from 8.06465 to 8.02547, saving model to best_bmi_model_v3_7.h5\n",
            "88/88 [==============================] - 8s 85ms/step - loss: 7.4637 - rmse: 2.1938 - val_loss: 113.7471 - val_rmse: 8.0255\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.8728 - rmse: 2.6124\n",
            "Epoch 50: val_rmse did not improve from 8.02547\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 10.8728 - rmse: 2.6124 - val_loss: 121.0285 - val_rmse: 8.0378\n",
            "Checking with unfreezing last 7 layers: \n",
            "88/88 [==============================] - 6s 64ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "Predicted BMI pi: 39.48143\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted BMI pi: 41.85904\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted BMI pi: 39.82377\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 34.479134\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_120\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_124 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_381 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_31 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_73 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_269 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_270 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 13,081,889\n",
            "Non-trainable params: 1,735,488\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 812.3149 - rmse: 25.5471\n",
            "Epoch 1: val_rmse improved from inf to 10.36022, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 11s 85ms/step - loss: 812.3149 - rmse: 25.5471 - val_loss: 203.6848 - val_rmse: 10.3602\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 178.8600 - rmse: 10.4722\n",
            "Epoch 2: val_rmse did not improve from 10.36022\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 178.8600 - rmse: 10.4722 - val_loss: 210.4608 - val_rmse: 12.3104\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 156.6403 - rmse: 9.7071\n",
            "Epoch 3: val_rmse improved from 10.36022 to 9.24215, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 156.6403 - rmse: 9.7071 - val_loss: 129.9998 - val_rmse: 9.2421\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 137.2548 - rmse: 9.0107\n",
            "Epoch 4: val_rmse did not improve from 9.24215\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 137.2548 - rmse: 9.0107 - val_loss: 138.4109 - val_rmse: 9.7462\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 148.2621 - rmse: 9.3894\n",
            "Epoch 5: val_rmse improved from 9.24215 to 7.87750, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 148.2621 - rmse: 9.3894 - val_loss: 118.6185 - val_rmse: 7.8775\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 135.6460 - rmse: 8.9788\n",
            "Epoch 6: val_rmse did not improve from 7.87750\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 135.6460 - rmse: 8.9788 - val_loss: 113.6458 - val_rmse: 8.2011\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 124.0978 - rmse: 8.5402\n",
            "Epoch 7: val_rmse did not improve from 7.87750\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 124.0978 - rmse: 8.5402 - val_loss: 114.1600 - val_rmse: 8.3055\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 111.9294 - rmse: 8.0812\n",
            "Epoch 8: val_rmse did not improve from 7.87750\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 111.9294 - rmse: 8.0812 - val_loss: 161.6550 - val_rmse: 8.7545\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 110.6495 - rmse: 8.0236\n",
            "Epoch 9: val_rmse did not improve from 7.87750\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 110.6495 - rmse: 8.0236 - val_loss: 110.5383 - val_rmse: 8.3944\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 103.5455 - rmse: 7.7437\n",
            "Epoch 10: val_rmse did not improve from 7.87750\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 103.5455 - rmse: 7.7437 - val_loss: 122.6591 - val_rmse: 7.9686\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 97.7912 - rmse: 7.5238\n",
            "Epoch 11: val_rmse did not improve from 7.87750\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 97.7912 - rmse: 7.5238 - val_loss: 121.1970 - val_rmse: 8.1013\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 91.5174 - rmse: 7.2954\n",
            "Epoch 12: val_rmse improved from 7.87750 to 7.81009, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 91.5174 - rmse: 7.2954 - val_loss: 114.3112 - val_rmse: 7.8101\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 77.8249 - rmse: 6.6346\n",
            "Epoch 13: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 77.8249 - rmse: 6.6346 - val_loss: 113.2309 - val_rmse: 8.3925\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 68.3832 - rmse: 6.2365\n",
            "Epoch 14: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 68.3832 - rmse: 6.2365 - val_loss: 114.8562 - val_rmse: 8.4790\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 64.4228 - rmse: 6.0425\n",
            "Epoch 15: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 64.4228 - rmse: 6.0425 - val_loss: 121.7561 - val_rmse: 9.0678\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 57.4180 - rmse: 5.7151\n",
            "Epoch 16: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 57.4180 - rmse: 5.7151 - val_loss: 143.6356 - val_rmse: 8.4197\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 47.5479 - rmse: 5.1728\n",
            "Epoch 17: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 47.5479 - rmse: 5.1728 - val_loss: 116.8247 - val_rmse: 8.5993\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 39.1886 - rmse: 4.7266\n",
            "Epoch 18: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 39.1886 - rmse: 4.7266 - val_loss: 115.2106 - val_rmse: 8.3546\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 39.0962 - rmse: 4.7456\n",
            "Epoch 19: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 39.0962 - rmse: 4.7456 - val_loss: 147.3536 - val_rmse: 8.6461\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 34.8470 - rmse: 4.4914\n",
            "Epoch 20: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 34.8470 - rmse: 4.4914 - val_loss: 112.0061 - val_rmse: 7.9299\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 32.1278 - rmse: 4.2849\n",
            "Epoch 21: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 32.1278 - rmse: 4.2849 - val_loss: 119.4653 - val_rmse: 8.7674\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 28.1172 - rmse: 4.0387\n",
            "Epoch 22: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 28.1172 - rmse: 4.0387 - val_loss: 111.2817 - val_rmse: 7.9952\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 33.5309 - rmse: 4.4040\n",
            "Epoch 23: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 33.5309 - rmse: 4.4040 - val_loss: 109.2000 - val_rmse: 8.2165\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 25.5581 - rmse: 3.8427\n",
            "Epoch 24: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 25.5581 - rmse: 3.8427 - val_loss: 114.7694 - val_rmse: 8.5254\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 23.0681 - rmse: 3.6897\n",
            "Epoch 25: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 23.0681 - rmse: 3.6897 - val_loss: 109.5936 - val_rmse: 8.1100\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.5320 - rmse: 3.3006\n",
            "Epoch 26: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 18.5320 - rmse: 3.3006 - val_loss: 118.9575 - val_rmse: 8.7829\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.0808 - rmse: 3.2023\n",
            "Epoch 27: val_rmse did not improve from 7.81009\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 17.0808 - rmse: 3.2023 - val_loss: 112.2579 - val_rmse: 8.2956\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 21.5752 - rmse: 3.5818\n",
            "Epoch 28: val_rmse improved from 7.81009 to 7.79584, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 21.5752 - rmse: 3.5818 - val_loss: 112.9970 - val_rmse: 7.7958\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.8176 - rmse: 3.0745\n",
            "Epoch 29: val_rmse did not improve from 7.79584\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 15.8176 - rmse: 3.0745 - val_loss: 109.9419 - val_rmse: 8.0078\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.5913 - rmse: 3.2631\n",
            "Epoch 30: val_rmse did not improve from 7.79584\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 17.5913 - rmse: 3.2631 - val_loss: 114.6976 - val_rmse: 7.8887\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.4646 - rmse: 3.0658\n",
            "Epoch 31: val_rmse improved from 7.79584 to 7.79274, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 8s 89ms/step - loss: 15.4646 - rmse: 3.0658 - val_loss: 110.2234 - val_rmse: 7.7927\n",
            "Epoch 32/50\n",
            "87/88 [============================>.] - ETA: 0s - loss: 14.2727 - rmse: 2.9436\n",
            "Epoch 32: val_rmse did not improve from 7.79274\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 14.2729 - rmse: 2.9365 - val_loss: 109.5787 - val_rmse: 7.8291\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.3904 - rmse: 3.1537\n",
            "Epoch 33: val_rmse did not improve from 7.79274\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 16.3904 - rmse: 3.1537 - val_loss: 112.4282 - val_rmse: 7.8675\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.4294 - rmse: 3.0429\n",
            "Epoch 34: val_rmse did not improve from 7.79274\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 15.4294 - rmse: 3.0429 - val_loss: 109.1033 - val_rmse: 8.0002\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 19.2036 - rmse: 3.3981\n",
            "Epoch 35: val_rmse improved from 7.79274 to 7.64734, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 19.2036 - rmse: 3.3981 - val_loss: 112.9363 - val_rmse: 7.6473\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.6813 - rmse: 3.2517\n",
            "Epoch 36: val_rmse did not improve from 7.64734\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 17.6813 - rmse: 3.2517 - val_loss: 106.9323 - val_rmse: 7.8292\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.4863 - rmse: 2.9870\n",
            "Epoch 37: val_rmse improved from 7.64734 to 7.62097, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 14.4863 - rmse: 2.9870 - val_loss: 106.0107 - val_rmse: 7.6210\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.8407 - rmse: 3.0192\n",
            "Epoch 38: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 14.8407 - rmse: 3.0192 - val_loss: 114.4835 - val_rmse: 8.5815\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.4990 - rmse: 2.8784\n",
            "Epoch 39: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 13.4990 - rmse: 2.8784 - val_loss: 116.5022 - val_rmse: 7.7349\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.9086 - rmse: 2.7213\n",
            "Epoch 40: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 11.9086 - rmse: 2.7213 - val_loss: 111.1023 - val_rmse: 7.6221\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.7911 - rmse: 2.6968\n",
            "Epoch 41: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 11.7911 - rmse: 2.6968 - val_loss: 106.7654 - val_rmse: 7.7534\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.5456 - rmse: 2.6957\n",
            "Epoch 42: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 11.5456 - rmse: 2.6957 - val_loss: 108.5447 - val_rmse: 7.6358\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.7243 - rmse: 3.1014\n",
            "Epoch 43: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 15.7243 - rmse: 3.1014 - val_loss: 109.2082 - val_rmse: 7.7078\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.2437 - rmse: 3.3032\n",
            "Epoch 44: val_rmse did not improve from 7.62097\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 18.2437 - rmse: 3.3032 - val_loss: 104.4581 - val_rmse: 7.7970\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.5139 - rmse: 3.0142\n",
            "Epoch 45: val_rmse improved from 7.62097 to 7.60611, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 14.5139 - rmse: 3.0142 - val_loss: 107.1610 - val_rmse: 7.6061\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.8950 - rmse: 2.6215\n",
            "Epoch 46: val_rmse did not improve from 7.60611\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 10.8950 - rmse: 2.6215 - val_loss: 107.3475 - val_rmse: 8.0931\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.9555 - rmse: 2.8325\n",
            "Epoch 47: val_rmse did not improve from 7.60611\n",
            "88/88 [==============================] - 7s 75ms/step - loss: 12.9555 - rmse: 2.8325 - val_loss: 114.3919 - val_rmse: 7.6824\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.3919 - rmse: 2.7759\n",
            "Epoch 48: val_rmse improved from 7.60611 to 7.58211, saving model to best_bmi_model_v3_8.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 12.3919 - rmse: 2.7759 - val_loss: 108.3163 - val_rmse: 7.5821\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.8404 - rmse: 2.7966\n",
            "Epoch 49: val_rmse did not improve from 7.58211\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 12.8404 - rmse: 2.7966 - val_loss: 103.9178 - val_rmse: 7.7863\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.0480 - rmse: 2.7592\n",
            "Epoch 50: val_rmse did not improve from 7.58211\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 12.0480 - rmse: 2.7592 - val_loss: 104.6269 - val_rmse: 7.9383\n",
            "Checking with unfreezing last 8 layers: \n",
            "88/88 [==============================] - 6s 63ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "Predicted BMI pi: 27.225622\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "4/4 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 23.981003\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 35.31153\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 36.075138\n",
            "\n",
            "\n",
            "\n",
            "Found 2805 validated image filenames.\n",
            "Found 701 validated image filenames.\n",
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_121\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_125 (InputLayer)      [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_382 (Conv2D)         (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_32 (Avera  (None, 3, 3, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_74 (Flatten)        (None, 288)               0         \n",
            "                                                                 \n",
            " dense_271 (Dense)           (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_272 (Dense)           (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,377\n",
            "Trainable params: 13,081,889\n",
            "Non-trainable params: 1,735,488\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 460.8998 - rmse: 18.1400\n",
            "Epoch 1: val_rmse improved from inf to 9.65305, saving model to best_bmi_model_v3_9.h5\n",
            "88/88 [==============================] - 11s 85ms/step - loss: 460.8998 - rmse: 18.1400 - val_loss: 163.3877 - val_rmse: 9.6531\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 184.6798 - rmse: 10.6397\n",
            "Epoch 2: val_rmse did not improve from 9.65305\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 184.6798 - rmse: 10.6397 - val_loss: 149.9845 - val_rmse: 9.9060\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 165.9646 - rmse: 10.0632\n",
            "Epoch 3: val_rmse improved from 9.65305 to 8.23061, saving model to best_bmi_model_v3_9.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 165.9646 - rmse: 10.0632 - val_loss: 129.2266 - val_rmse: 8.2306\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 138.7767 - rmse: 9.0508\n",
            "Epoch 4: val_rmse did not improve from 8.23061\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 138.7767 - rmse: 9.0508 - val_loss: 116.4569 - val_rmse: 8.3261\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 134.1085 - rmse: 8.9302\n",
            "Epoch 5: val_rmse did not improve from 8.23061\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 134.1085 - rmse: 8.9302 - val_loss: 115.0559 - val_rmse: 8.4818\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 129.3591 - rmse: 8.7393\n",
            "Epoch 6: val_rmse did not improve from 8.23061\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 129.3591 - rmse: 8.7393 - val_loss: 171.8291 - val_rmse: 11.0977\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 138.9514 - rmse: 9.1091\n",
            "Epoch 7: val_rmse improved from 8.23061 to 7.90300, saving model to best_bmi_model_v3_9.h5\n",
            "88/88 [==============================] - 7s 83ms/step - loss: 138.9514 - rmse: 9.1091 - val_loss: 119.4183 - val_rmse: 7.9030\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 114.1740 - rmse: 8.1634\n",
            "Epoch 8: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 114.1740 - rmse: 8.1634 - val_loss: 114.9921 - val_rmse: 8.0197\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 118.8176 - rmse: 8.3545\n",
            "Epoch 9: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 118.8176 - rmse: 8.3545 - val_loss: 112.8004 - val_rmse: 8.2809\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 106.7016 - rmse: 7.8868\n",
            "Epoch 10: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 106.7016 - rmse: 7.8868 - val_loss: 142.8564 - val_rmse: 8.3144\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 100.4189 - rmse: 7.6228\n",
            "Epoch 11: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 100.4189 - rmse: 7.6228 - val_loss: 161.6006 - val_rmse: 8.8944\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 96.9067 - rmse: 7.4823\n",
            "Epoch 12: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 96.9067 - rmse: 7.4823 - val_loss: 184.1169 - val_rmse: 9.4985\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 91.7812 - rmse: 7.2815\n",
            "Epoch 13: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 91.7812 - rmse: 7.2815 - val_loss: 109.5402 - val_rmse: 8.0128\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 76.4646 - rmse: 6.6188\n",
            "Epoch 14: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 76.4646 - rmse: 6.6188 - val_loss: 110.8454 - val_rmse: 8.1005\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 73.0529 - rmse: 6.4507\n",
            "Epoch 15: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 73.0529 - rmse: 6.4507 - val_loss: 123.8659 - val_rmse: 9.1234\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 58.4917 - rmse: 5.7666\n",
            "Epoch 16: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 58.4917 - rmse: 5.7666 - val_loss: 118.8851 - val_rmse: 8.2364\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 53.8694 - rmse: 5.5260\n",
            "Epoch 17: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 53.8694 - rmse: 5.5260 - val_loss: 121.2175 - val_rmse: 8.1871\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 44.1116 - rmse: 5.0385\n",
            "Epoch 18: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 44.1116 - rmse: 5.0385 - val_loss: 122.4633 - val_rmse: 8.3205\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 44.2939 - rmse: 4.9875\n",
            "Epoch 19: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 44.2939 - rmse: 4.9875 - val_loss: 114.7357 - val_rmse: 8.4303\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 42.2511 - rmse: 4.9490\n",
            "Epoch 20: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 42.2511 - rmse: 4.9490 - val_loss: 127.4867 - val_rmse: 9.0146\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 35.3459 - rmse: 4.4935\n",
            "Epoch 21: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 35.3459 - rmse: 4.4935 - val_loss: 126.8126 - val_rmse: 8.9099\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 31.5435 - rmse: 4.2877\n",
            "Epoch 22: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 31.5435 - rmse: 4.2877 - val_loss: 117.5219 - val_rmse: 8.4007\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 23.9019 - rmse: 3.7343\n",
            "Epoch 23: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 81ms/step - loss: 23.9019 - rmse: 3.7343 - val_loss: 124.0629 - val_rmse: 8.7033\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 25.8722 - rmse: 3.8996\n",
            "Epoch 24: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 82ms/step - loss: 25.8722 - rmse: 3.8996 - val_loss: 121.3557 - val_rmse: 8.4817\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 22.3448 - rmse: 3.6352\n",
            "Epoch 25: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 22.3448 - rmse: 3.6352 - val_loss: 131.8539 - val_rmse: 8.3229\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 22.0446 - rmse: 3.6143\n",
            "Epoch 26: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 22.0446 - rmse: 3.6143 - val_loss: 124.1322 - val_rmse: 8.3909\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 18.6780 - rmse: 3.3590\n",
            "Epoch 27: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 18.6780 - rmse: 3.3590 - val_loss: 133.5577 - val_rmse: 8.4108\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.4139 - rmse: 3.0455\n",
            "Epoch 28: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 15.4139 - rmse: 3.0455 - val_loss: 125.4536 - val_rmse: 8.8749\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 17.1633 - rmse: 3.2154\n",
            "Epoch 29: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 84ms/step - loss: 17.1633 - rmse: 3.2154 - val_loss: 118.9171 - val_rmse: 8.4315\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.2585 - rmse: 3.1688\n",
            "Epoch 30: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 16.2585 - rmse: 3.1688 - val_loss: 123.2801 - val_rmse: 8.3105\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.5672 - rmse: 2.9868\n",
            "Epoch 31: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 14.5672 - rmse: 2.9868 - val_loss: 118.8307 - val_rmse: 8.4623\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.3575 - rmse: 2.8699\n",
            "Epoch 32: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 13.3575 - rmse: 2.8699 - val_loss: 117.5381 - val_rmse: 8.4087\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.0689 - rmse: 2.8340\n",
            "Epoch 33: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 13.0689 - rmse: 2.8340 - val_loss: 121.5337 - val_rmse: 8.3154\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.1092 - rmse: 2.7243\n",
            "Epoch 34: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 12.1092 - rmse: 2.7243 - val_loss: 119.8625 - val_rmse: 8.1647\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.7323 - rmse: 2.7063\n",
            "Epoch 35: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 11.7323 - rmse: 2.7063 - val_loss: 131.9690 - val_rmse: 8.3188\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.5854 - rmse: 2.8704\n",
            "Epoch 36: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 13.5854 - rmse: 2.8704 - val_loss: 116.5129 - val_rmse: 8.1112\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.6254 - rmse: 2.9995\n",
            "Epoch 37: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 14.6254 - rmse: 2.9995 - val_loss: 118.4598 - val_rmse: 8.0754\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 14.8698 - rmse: 3.0163\n",
            "Epoch 38: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 14.8698 - rmse: 3.0163 - val_loss: 120.3954 - val_rmse: 8.2836\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.5127 - rmse: 3.0714\n",
            "Epoch 39: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 15.5127 - rmse: 3.0714 - val_loss: 120.9137 - val_rmse: 8.1370\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.8627 - rmse: 2.7118\n",
            "Epoch 40: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 11.8627 - rmse: 2.7118 - val_loss: 119.3046 - val_rmse: 8.5006\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 13.6694 - rmse: 2.8865\n",
            "Epoch 41: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 76ms/step - loss: 13.6694 - rmse: 2.8865 - val_loss: 116.2678 - val_rmse: 8.2756\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.8130 - rmse: 2.8283\n",
            "Epoch 42: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 12.8130 - rmse: 2.8283 - val_loss: 119.6314 - val_rmse: 8.2619\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.7068 - rmse: 2.8118\n",
            "Epoch 43: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 12.7068 - rmse: 2.8118 - val_loss: 125.3374 - val_rmse: 8.1248\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 16.2072 - rmse: 3.1325\n",
            "Epoch 44: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 16.2072 - rmse: 3.1325 - val_loss: 117.7113 - val_rmse: 8.5957\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.2956 - rmse: 2.7575\n",
            "Epoch 45: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 78ms/step - loss: 12.2956 - rmse: 2.7575 - val_loss: 116.6606 - val_rmse: 8.1845\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 10.5213 - rmse: 2.5812\n",
            "Epoch 46: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 80ms/step - loss: 10.5213 - rmse: 2.5812 - val_loss: 113.1301 - val_rmse: 8.1662\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.3574 - rmse: 2.7707\n",
            "Epoch 47: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 12.3574 - rmse: 2.7707 - val_loss: 117.6763 - val_rmse: 8.0864\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 11.7410 - rmse: 2.7180\n",
            "Epoch 48: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 11.7410 - rmse: 2.7180 - val_loss: 147.4112 - val_rmse: 8.5801\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 12.5703 - rmse: 2.8003\n",
            "Epoch 49: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 77ms/step - loss: 12.5703 - rmse: 2.8003 - val_loss: 115.0228 - val_rmse: 8.0001\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 15.9973 - rmse: 3.1117\n",
            "Epoch 50: val_rmse did not improve from 7.90300\n",
            "88/88 [==============================] - 7s 79ms/step - loss: 15.9973 - rmse: 3.1117 - val_loss: 125.1091 - val_rmse: 8.1924\n",
            "Checking with unfreezing last 9 layers: \n",
            "88/88 [==============================] - 5s 60ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "11/11 [==============================] - 0s 2ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "Predicted BMI pi: 29.728016\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted BMI pi: 35.865788\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicted BMI pi: 33.12716\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Preprocessed image shape: (1, 224, 224, 3)\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted BMI pi: 40.83295\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "def cropped_faces(image_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return None\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            padded_image[:resized_face.shape[0], :resized_face.shape[1]] = resized_face\n",
        "\n",
        "            # Save the padded image\n",
        "            cropped_image_path = '/content/drive/MyDrive/Cropped_Image.jpg'\n",
        "            cv2.imwrite(cropped_image_path, padded_image)\n",
        "            \n",
        "            return cropped_image_path\n",
        "        else:\n",
        "            print(\"No face detected in the image:\", image_path)\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "        return None\n",
        "\n",
        "i = 0\n",
        "for i in range(10):\n",
        "    \n",
        "\n",
        "    # Set up the training data generator with the cropped images\n",
        "    train_set = train_datagen.flow_from_dataframe(\n",
        "        dataframe=data,\n",
        "        directory=cropped_directory,\n",
        "        x_col='name',\n",
        "        y_col='bmi',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='other',\n",
        "        subset='training')\n",
        "\n",
        "    validation_set = train_datagen.flow_from_dataframe(\n",
        "        dataframe=data,\n",
        "        directory=cropped_directory,\n",
        "        x_col='name',\n",
        "        y_col='bmi',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='other',\n",
        "        subset='validation')\n",
        "\n",
        "    print(\"Image shape:\", train_set.image_shape)\n",
        "\n",
        "    # Load the VGGFace model without the top layer\n",
        "    base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    x = base_model.layers[-1].output\n",
        "    x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "    x = keras.layers.AveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=128, activation='relu')(x)\n",
        "    predictions = Dense(1, activation='linear')(x)  # Use linear activation for BMI prediction\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Freeze the layers of the base model\n",
        "    if i == 0:\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        for layer in base_model.layers[:-i]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # # Define the custom loss function\n",
        "    # def custom_loss(y_true, y_pred):\n",
        "    #     penalty = 10.0  # Penalty factor for overestimation\n",
        "    #     squared_difference = tf.square(y_true - y_pred)\n",
        "    #     overestimation_penalty = tf.maximum(0.0, y_pred - y_true) * penalty\n",
        "    #     return tf.reduce_mean(squared_difference + overestimation_penalty, axis=-1)\n",
        "\n",
        "    def custom_loss(y_true, y_pred):\n",
        "        penalty_overestimation_10 = 10.0  # Penalty factor for overestimation\n",
        "        penalty_underestimation_10 = 10.0  # Penalty factor for underestimation by 10\n",
        "        penalty_overestimation_5 = 5.0  # Penalty factor for overestimation\n",
        "        penalty_underestimation_5 = 5.0  # Penalty factor for underestimation by 5\n",
        "        \n",
        "        squared_difference = tf.square(y_true - y_pred)\n",
        "        overestimation_penalty_10 = tf.maximum(0.0, y_pred - y_true) * penalty_overestimation_10\n",
        "        underestimation_penalty_10 = tf.maximum(0.0, y_true - y_pred - 10) * penalty_underestimation_10\n",
        "        underestimation_penalty_5 = tf.maximum(0.0, y_true - y_pred - 5) * penalty_underestimation_5\n",
        "        overestimation_penalty_5 = tf.maximum(0.0, y_pred - y_true) * penalty_overestimation_5\n",
        "        total_loss = tf.reduce_mean(squared_difference + overestimation_penalty_10 + overestimation_penalty_5 + underestimation_penalty_10 + underestimation_penalty_5, axis=-1)\n",
        "        \n",
        "        return total_loss\n",
        "\n",
        "    # Register the custom loss function\n",
        "    keras.utils.get_custom_objects()['custom_loss'] = custom_loss\n",
        "\n",
        "    # Compile the model with the custom loss function\n",
        "    model.compile(optimizer='adam', loss=custom_loss, metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
        "\n",
        "    # Set up callbacks to save the best model and enable early stopping\n",
        "    checkpoint_bmi = ModelCheckpoint('best_bmi_model_v3_'+str(i)+'.h5', monitor='val_rmse', save_best_only=True, verbose=1)\n",
        "\n",
        "    # Print the model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_set,\n",
        "        epochs=50,\n",
        "        callbacks=[checkpoint_bmi],\n",
        "        validation_data=validation_set\n",
        "    )\n",
        "    print(\"Checking with unfreezing last \" + str(i) + \" layers: \")\n",
        "    train_predictions = model.predict(train_set)\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/PassportPhoto.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted BMI pi:\", predicted_bmi)\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NrMtmTe6JTbd"
      },
      "source": [
        "### Predict Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e0EVMaOmJWhW",
        "outputId": "87f5aa3c-b417-4a4f-9e32-4a519ac393b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 701 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: (224, 224, 3)\n",
            "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_vgg16.h5\n",
            "58909280/58909280 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 3, 3, 32)         0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 288)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,506\n",
            "Trainable params: 102,818\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            " 1/88 [..............................] - ETA: 28:52 - loss: 0.7013 - accuracy: 0.5938"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-036dcb5adfcc>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "def cropped_faces(image_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return None\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            padded_image[:resized_face.shape[0], :resized_face.shape[1]] = resized_face\n",
        "\n",
        "            # Save the padded image\n",
        "            cropped_image_path = '/content/drive/MyDrive/Cropped_Image.jpg'\n",
        "            cv2.imwrite(cropped_image_path, padded_image)\n",
        "            \n",
        "            return cropped_image_path\n",
        "        else:\n",
        "            print(\"No face detected in the image:\", image_path)\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "        return None\n",
        "cropped_directory = '/content/drive/MyDrive/BMI_Cropped/CroppedImages'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "        dataframe=data,\n",
        "        directory=cropped_directory,\n",
        "        x_col='name',\n",
        "        y_col='gender',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "\n",
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='gender',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='gender',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "i = 0\n",
        "for i in range(5):\n",
        "    # Load the VGGFace model without the top layer\n",
        "    base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    x = base_model.layers[-1].output\n",
        "    x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "    x = keras.layers.AveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(units=128, activation='relu')(x)\n",
        "    predictions = Dense(2, activation='softmax')(x)  # Use linear activation for BMI prediction\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Freeze the layers of the base model\n",
        "    if i == 0:\n",
        "        for layer in base_model.layers:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        for layer in base_model.layers[:-i]:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # # Define the custom loss function\n",
        "    # def custom_loss(y_true, y_pred):\n",
        "    #     penalty = 10.0  # Penalty factor for overestimation\n",
        "    #     squared_difference = tf.square(y_true - y_pred)\n",
        "    #     overestimation_penalty = tf.maximum(0.0, y_pred - y_true) * penalty\n",
        "    #     return tf.reduce_mean(squared_difference + overestimation_penalty, axis=-1)\n",
        "\n",
        "    # Register the custom loss function\n",
        "    # keras.utils.get_custom_objects()['custom_loss'] = custom_loss\n",
        "\n",
        "    # Compile the model with the custom loss function\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Set up callbacks to save the best model and enable early stopping\n",
        "    checkpoint_gender = ModelCheckpoint('best_gender_model_v1_'+str(i)+'.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "    # Print the model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_set,\n",
        "        epochs=50,\n",
        "        callbacks=[checkpoint_gender],\n",
        "        validation_data=validation_set\n",
        "    )\n",
        "    print(\"Checking with unfreezing last \" + str(i) + \" layers: \")\n",
        "    train_predictions = model.predict(train_set)\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/PassportPhoto.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted Gender pi:\", predicted_bmi)\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/PP_1.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted Gender pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/Test1.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted Gender pi:\", predicted_bmi)\n",
        "\n",
        "\n",
        "    # Load the new image and perform face cropping\n",
        "    image_path = '/content/drive/MyDrive/Test_Image/Test2.jpg' \n",
        "    cropped_image_path = cropped_faces(image_path)\n",
        "\n",
        "    if cropped_image_path is not None:\n",
        "        # Load the cropped image and preprocess\n",
        "        img = load_img(cropped_image_path, target_size=(224, 224))\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = img_array / 255.0  # Rescale the pixel values\n",
        "        print(\"Preprocessed image shape:\", img_array.shape)\n",
        "    else:\n",
        "        print(\"Failed to perform face cropping.\")\n",
        "\n",
        "\n",
        "    # Predict the BMI for the new image\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_bmi = prediction[0][0]\n",
        "    print(\"Predicted Gender pi:\", predicted_bmi)\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XEZw1RQl5qjr",
        "outputId": "f88e4328-8d7f-41fa-d221-9cc7ab0ef025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2805 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 701 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1137: UserWarning: Found 700 invalid image filename(s) in x_col=\"name\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: (224, 224, 3)\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 6, 6, 32)          65568     \n",
            "                                                                 \n",
            " average_pooling2d_3 (Averag  (None, 3, 3, 32)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 288)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               36992     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,817,506\n",
            "Trainable params: 7,182,242\n",
            "Non-trainable params: 7,635,264\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.4622 - accuracy: 0.8064\n",
            "Epoch 1: val_accuracy improved from -inf to 0.88445, saving model to best_gender_model_v1_4.h5\n",
            "88/88 [==============================] - 17s 166ms/step - loss: 0.4622 - accuracy: 0.8064 - val_loss: 0.2879 - val_accuracy: 0.8845\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9216\n",
            "Epoch 2: val_accuracy did not improve from 0.88445\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 0.1965 - accuracy: 0.9216 - val_loss: 0.2591 - val_accuracy: 0.8845\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.1347 - accuracy: 0.9430\n",
            "Epoch 3: val_accuracy improved from 0.88445 to 0.89729, saving model to best_gender_model_v1_4.h5\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 0.1347 - accuracy: 0.9430 - val_loss: 0.2543 - val_accuracy: 0.8973\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9693\n",
            "Epoch 4: val_accuracy did not improve from 0.89729\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 0.0730 - accuracy: 0.9693 - val_loss: 0.5089 - val_accuracy: 0.8859\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9672\n",
            "Epoch 5: val_accuracy improved from 0.89729 to 0.92725, saving model to best_gender_model_v1_4.h5\n",
            "88/88 [==============================] - 15s 173ms/step - loss: 0.0898 - accuracy: 0.9672 - val_loss: 0.2765 - val_accuracy: 0.9272\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9861\n",
            "Epoch 6: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 0.0481 - accuracy: 0.9861 - val_loss: 0.5079 - val_accuracy: 0.8802\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9775\n",
            "Epoch 7: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 0.0647 - accuracy: 0.9775 - val_loss: 0.4051 - val_accuracy: 0.9058\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9929\n",
            "Epoch 8: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.5487 - val_accuracy: 0.9087\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9900\n",
            "Epoch 9: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 0.0301 - accuracy: 0.9900 - val_loss: 0.3749 - val_accuracy: 0.9201\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9829\n",
            "Epoch 10: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 0.0663 - accuracy: 0.9829 - val_loss: 0.5329 - val_accuracy: 0.8887\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 0.9904\n",
            "Epoch 11: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.3688 - val_accuracy: 0.8973\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9936\n",
            "Epoch 12: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 0.0279 - accuracy: 0.9936 - val_loss: 0.4775 - val_accuracy: 0.9130\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9936\n",
            "Epoch 13: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 0.0199 - accuracy: 0.9936 - val_loss: 0.4962 - val_accuracy: 0.9158\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9914\n",
            "Epoch 14: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 0.0286 - accuracy: 0.9914 - val_loss: 0.3788 - val_accuracy: 0.9116\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9914\n",
            "Epoch 15: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 0.0280 - accuracy: 0.9914 - val_loss: 0.6751 - val_accuracy: 0.8873\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9900\n",
            "Epoch 16: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 0.0365 - accuracy: 0.9900 - val_loss: 0.8976 - val_accuracy: 0.8188\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9943\n",
            "Epoch 17: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 0.0236 - accuracy: 0.9943 - val_loss: 0.8657 - val_accuracy: 0.8431\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9986\n",
            "Epoch 18: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.9424 - val_accuracy: 0.9187\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.5742e-05 - accuracy: 1.0000\n",
            "Epoch 19: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 1.5742e-05 - accuracy: 1.0000 - val_loss: 1.0131 - val_accuracy: 0.9201\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.5974e-06 - accuracy: 1.0000\n",
            "Epoch 20: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 9.5974e-06 - accuracy: 1.0000 - val_loss: 1.0516 - val_accuracy: 0.9201\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.6948e-06 - accuracy: 1.0000\n",
            "Epoch 21: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 6.6948e-06 - accuracy: 1.0000 - val_loss: 1.0840 - val_accuracy: 0.9201\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.9451e-06 - accuracy: 1.0000\n",
            "Epoch 22: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 169ms/step - loss: 4.9451e-06 - accuracy: 1.0000 - val_loss: 1.1051 - val_accuracy: 0.9201\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 3.8275e-06 - accuracy: 1.0000\n",
            "Epoch 23: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 168ms/step - loss: 3.8275e-06 - accuracy: 1.0000 - val_loss: 1.1290 - val_accuracy: 0.9201\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.9960e-06 - accuracy: 1.0000\n",
            "Epoch 24: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 2.9960e-06 - accuracy: 1.0000 - val_loss: 1.1512 - val_accuracy: 0.9187\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.4051e-06 - accuracy: 1.0000\n",
            "Epoch 25: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 2.4051e-06 - accuracy: 1.0000 - val_loss: 1.1770 - val_accuracy: 0.9187\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.9280e-06 - accuracy: 1.0000\n",
            "Epoch 26: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.9280e-06 - accuracy: 1.0000 - val_loss: 1.1984 - val_accuracy: 0.9187\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.5614e-06 - accuracy: 1.0000\n",
            "Epoch 27: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.5614e-06 - accuracy: 1.0000 - val_loss: 1.2107 - val_accuracy: 0.9187\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.2861e-06 - accuracy: 1.0000\n",
            "Epoch 28: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.2861e-06 - accuracy: 1.0000 - val_loss: 1.2442 - val_accuracy: 0.9173\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.0777e-06 - accuracy: 1.0000\n",
            "Epoch 29: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.0777e-06 - accuracy: 1.0000 - val_loss: 1.2673 - val_accuracy: 0.9158\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.8602e-07 - accuracy: 1.0000\n",
            "Epoch 30: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 8.8602e-07 - accuracy: 1.0000 - val_loss: 1.2888 - val_accuracy: 0.9173\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 7.4414e-07 - accuracy: 1.0000\n",
            "Epoch 31: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 7.4414e-07 - accuracy: 1.0000 - val_loss: 1.3116 - val_accuracy: 0.9173\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 6.2981e-07 - accuracy: 1.0000\n",
            "Epoch 32: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 6.2981e-07 - accuracy: 1.0000 - val_loss: 1.3313 - val_accuracy: 0.9173\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 5.3578e-07 - accuracy: 1.0000\n",
            "Epoch 33: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 5.3578e-07 - accuracy: 1.0000 - val_loss: 1.3525 - val_accuracy: 0.9173\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.6193e-07 - accuracy: 1.0000\n",
            "Epoch 34: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 4.6193e-07 - accuracy: 1.0000 - val_loss: 1.3709 - val_accuracy: 0.9173\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 4.0201e-07 - accuracy: 1.0000\n",
            "Epoch 35: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 4.0201e-07 - accuracy: 1.0000 - val_loss: 1.3935 - val_accuracy: 0.9187\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 3.5170e-07 - accuracy: 1.0000\n",
            "Epoch 36: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 3.5170e-07 - accuracy: 1.0000 - val_loss: 1.4089 - val_accuracy: 0.9187\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 3.0653e-07 - accuracy: 1.0000\n",
            "Epoch 37: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 3.0653e-07 - accuracy: 1.0000 - val_loss: 1.4285 - val_accuracy: 0.9187\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.7151e-07 - accuracy: 1.0000\n",
            "Epoch 38: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 2.7151e-07 - accuracy: 1.0000 - val_loss: 1.4420 - val_accuracy: 0.9187\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.4189e-07 - accuracy: 1.0000\n",
            "Epoch 39: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 2.4189e-07 - accuracy: 1.0000 - val_loss: 1.4600 - val_accuracy: 0.9187\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 2.1516e-07 - accuracy: 1.0000\n",
            "Epoch 40: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 2.1516e-07 - accuracy: 1.0000 - val_loss: 1.4733 - val_accuracy: 0.9187\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.9536e-07 - accuracy: 1.0000\n",
            "Epoch 41: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.9536e-07 - accuracy: 1.0000 - val_loss: 1.4879 - val_accuracy: 0.9187\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.7620e-07 - accuracy: 1.0000\n",
            "Epoch 42: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.7620e-07 - accuracy: 1.0000 - val_loss: 1.5017 - val_accuracy: 0.9187\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.5733e-07 - accuracy: 1.0000\n",
            "Epoch 43: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.5733e-07 - accuracy: 1.0000 - val_loss: 1.5161 - val_accuracy: 0.9173\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.4245e-07 - accuracy: 1.0000\n",
            "Epoch 44: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 1.4245e-07 - accuracy: 1.0000 - val_loss: 1.5302 - val_accuracy: 0.9173\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.3017e-07 - accuracy: 1.0000\n",
            "Epoch 45: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 1.3017e-07 - accuracy: 1.0000 - val_loss: 1.5445 - val_accuracy: 0.9173\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.1870e-07 - accuracy: 1.0000\n",
            "Epoch 46: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 1.1870e-07 - accuracy: 1.0000 - val_loss: 1.5540 - val_accuracy: 0.9173\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.0939e-07 - accuracy: 1.0000\n",
            "Epoch 47: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 1.0939e-07 - accuracy: 1.0000 - val_loss: 1.5657 - val_accuracy: 0.9173\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 1.0004e-07 - accuracy: 1.0000\n",
            "Epoch 48: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 1.0004e-07 - accuracy: 1.0000 - val_loss: 1.5752 - val_accuracy: 0.9173\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 9.2774e-08 - accuracy: 1.0000\n",
            "Epoch 49: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 9.2774e-08 - accuracy: 1.0000 - val_loss: 1.5854 - val_accuracy: 0.9158\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - ETA: 0s - loss: 8.6144e-08 - accuracy: 1.0000\n",
            "Epoch 50: val_accuracy did not improve from 0.92725\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 8.6144e-08 - accuracy: 1.0000 - val_loss: 1.5935 - val_accuracy: 0.9158\n",
            "Checking with unfreezing last 4 layers: \n",
            "88/88 [==============================] - 10s 115ms/step\n",
            "Error processing image: /content/drive/MyDrive/Test_Image/PassportPhoto.jpg\n",
            "name 'cv2' is not defined\n",
            "Failed to perform face cropping.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dc1333585a43>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Predict the BMI for the new image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0mpredicted_bmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Gender pi:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_bmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_array' is not defined"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "def cropped_faces(image_path):\n",
        "    try:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(\"Invalid image:\", image_path)\n",
        "            return None\n",
        "        \n",
        "        result = mtcnn.detect_faces(image)\n",
        "        if result:\n",
        "            x, y, w, h = result[0]['box']\n",
        "        \n",
        "            # Calculate the aspect ratio\n",
        "            aspect_ratio = float(w) / h\n",
        "            \n",
        "            # Determine the new width and height while maintaining the aspect ratio\n",
        "            if aspect_ratio > 1.0:\n",
        "                new_width = int(224 * aspect_ratio)\n",
        "                x = x - (new_width - w) // 2\n",
        "                w = new_width\n",
        "            else:\n",
        "                new_height = int(224 / aspect_ratio)\n",
        "                y = y - (new_height - h) // 2\n",
        "                h = new_height\n",
        "            \n",
        "            # Crop the face region while maintaining the aspect ratio\n",
        "            face = image[max(0, y):y+h, max(0, x):x+w]\n",
        "\n",
        "            # Resize the face region to fit the desired padding size\n",
        "            resized_face = cv2.resize(face, (224, 224))\n",
        "\n",
        "            # Create a padded canvas\n",
        "            padded_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
        "            padded_image[:resized_face.shape[0], :resized_face.shape[1]] = resized_face\n",
        "\n",
        "            # Save the padded image\n",
        "            cropped_image_path = '/content/drive/MyDrive/Cropped_Image.jpg'\n",
        "            cv2.imwrite(cropped_image_path, padded_image)\n",
        "            \n",
        "            return cropped_image_path\n",
        "        else:\n",
        "            print(\"No face detected in the image:\", image_path)\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error processing image:\", image_path)\n",
        "        print(str(e))\n",
        "        return None\n",
        "cropped_directory = '/content/drive/MyDrive/BMI_Cropped/CroppedImages'\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "        dataframe=data,\n",
        "        directory=cropped_directory,\n",
        "        x_col='name',\n",
        "        y_col='gender',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "\n",
        "# Set up the training data generator with the cropped images\n",
        "train_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='gender',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training')\n",
        "\n",
        "validation_set = train_datagen.flow_from_dataframe(\n",
        "    dataframe=data,\n",
        "    directory=cropped_directory,\n",
        "    x_col='name',\n",
        "    y_col='gender',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')\n",
        "\n",
        "print(\"Image shape:\", train_set.image_shape)\n",
        "# i = 0\n",
        "# for i in range(5):\n",
        "# Load the VGGFace model without the top layer\n",
        "base_model = VGGFace(model='vgg16', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.layers[-1].output\n",
        "x = Conv2D(filters=32, kernel_size=(2,2), activation='relu')(x)\n",
        "x = keras.layers.AveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "predictions = Dense(2, activation='softmax')(x)  # Use linear activation for BMI prediction\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "# if i == 0:\n",
        "#     for layer in base_model.layers:\n",
        "#         layer.trainable = False\n",
        "# else:\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# # Define the custom loss function\n",
        "# def custom_loss(y_true, y_pred):\n",
        "#     penalty = 10.0  # Penalty factor for overestimation\n",
        "#     squared_difference = tf.square(y_true - y_pred)\n",
        "#     overestimation_penalty = tf.maximum(0.0, y_pred - y_true) * penalty\n",
        "#     return tf.reduce_mean(squared_difference + overestimation_penalty, axis=-1)\n",
        "\n",
        "# Register the custom loss function\n",
        "# keras.utils.get_custom_objects()['custom_loss'] = custom_loss\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up callbacks to save the best model and enable early stopping\n",
        "checkpoint_gender = ModelCheckpoint('best_gender_model_v1_4.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_set,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_gender],\n",
        "    validation_data=validation_set\n",
        ")\n",
        "\n",
        "    \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
